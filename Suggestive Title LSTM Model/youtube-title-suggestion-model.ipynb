{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/garrisonwinter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/garrisonwinter/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/garrisonwinter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow.keras\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "import gc\n",
    "import opendatasets as od\n",
    "import nltk\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import re,string,unicodedata\n",
    "from keras import models    \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Get Our Data\n",
    "\n",
    "* Our data is Trending youtube videos in US, CA and GB\n",
    "* For this kernal title, category and view_count will be used for each video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3C66w5Z0ixs</td>\n",
       "      <td>I ASKED HER TO BE MY GIRLFRIEND...</td>\n",
       "      <td>22</td>\n",
       "      <td>1514614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M9Pmf9AB4Mo</td>\n",
       "      <td>Apex Legends | Stories from the Outlands – “Th...</td>\n",
       "      <td>20</td>\n",
       "      <td>2381688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J78aPJ3VyNs</td>\n",
       "      <td>I left youtube for a month and THIS is what ha...</td>\n",
       "      <td>24</td>\n",
       "      <td>2038853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kXLn3HkpjaA</td>\n",
       "      <td>XXL 2020 Freshman Class Revealed - Official An...</td>\n",
       "      <td>10</td>\n",
       "      <td>496771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VIUo6yapDbc</td>\n",
       "      <td>Ultimate DIY Home Movie Theater for The LaBran...</td>\n",
       "      <td>26</td>\n",
       "      <td>1123889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  categoryId  \\\n",
       "0  3C66w5Z0ixs                 I ASKED HER TO BE MY GIRLFRIEND...          22   \n",
       "1  M9Pmf9AB4Mo  Apex Legends | Stories from the Outlands – “Th...          20   \n",
       "2  J78aPJ3VyNs  I left youtube for a month and THIS is what ha...          24   \n",
       "3  kXLn3HkpjaA  XXL 2020 Freshman Class Revealed - Official An...          10   \n",
       "4  VIUo6yapDbc  Ultimate DIY Home Movie Theater for The LaBran...          26   \n",
       "\n",
       "   view_count  \n",
       "0     1514614  \n",
       "1     2381688  \n",
       "2     2038853  \n",
       "3      496771  \n",
       "4     1123889  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "od.download('https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset', force=True)\n",
    "\n",
    "#Let's read our data \n",
    "\n",
    "columns = ['title','categoryId','view_count','video_id']\n",
    "\n",
    "main_data = pd.read_csv(\"/youtube-trending-video-dataset/US_youtube_trending_data.csv\",usecols=columns)\n",
    "# old_main_data = pd.read_csv(\"/kaggle/input/youtube-new/USvideos.csv\",usecols=['title', 'category_id',\"views\"])\n",
    "# old_main_data = old_main_data.rename({'category_id': 'categoryId', 'views': 'view_count'}, axis=1)\n",
    "ca_main_data = pd.read_csv(\"/youtube-trending-video-dataset/CA_youtube_trending_data.csv\",usecols=columns)\n",
    "gb_main_data = pd.read_csv(\"/youtube-trending-video-dataset/GB_youtube_trending_data.csv\",usecols=columns)\n",
    "main_data = pd.concat([main_data,ca_main_data,gb_main_data],axis=0,ignore_index=True)\n",
    "\n",
    "del gb_main_data #Not using this\n",
    "del ca_main_data #Not using this\n",
    "gc.collect()\n",
    "main_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "**In total we have 503.475 video which all have non-null title and category data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/youtube-trending-video-dataset/US_category_id.json\") as f:\n",
    "    categories = json.load(f)[\"items\"]\n",
    "cat_dict = {}\n",
    "for cat in categories:\n",
    "    cat_dict[int(cat[\"id\"])] = cat[\"snippet\"][\"title\"]\n",
    "main_data['category_name'] = main_data['categoryId'].map(cat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Autos & Vehicles',\n",
       " 'Comedy',\n",
       " 'Education',\n",
       " 'Entertainment',\n",
       " 'Film & Animation',\n",
       " 'Gaming',\n",
       " 'Howto & Style',\n",
       " 'Music',\n",
       " 'News & Politics',\n",
       " 'Nonprofits & Activism',\n",
       " 'People & Blogs',\n",
       " 'Pets & Animals',\n",
       " 'Science & Technology',\n",
       " 'Sports',\n",
       " 'Travel & Events'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(main_data['category_name'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total we have 44 different type of categoriesi but we have in our data there are videos which belongs only one of the 12 different category**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The distrubituon of categories in our data**\n",
    "* **We can see that Gaming, Entertainment and music categories are outnumbered**\n",
    "* **but since the database is big enough to not to let these categories to dominant the other ones.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "* Cleaning the data is one of the most important part of nlp, because no matter how efficient your model is, if you are working with uncleaned data, you should not expect very high efficiency.  \n",
    "\n",
    "* Removing unnecessary spaces.\n",
    "* Cleaning the numbers\n",
    "* Correction of the misspelled words\n",
    "* Correction of rare words\n",
    "* Cleaning bad case words\n",
    "* Cleaning the repeat words\n",
    "* Cleaning the emojies, unneccessary punctiations, characters\n",
    "* Cleaning the StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(578326, 5)\n",
      "(61384, 5)\n"
     ]
    }
   ],
   "source": [
    "print(main_data.shape)\n",
    "main_data = main_data.drop_duplicates(['video_id'], keep='first')\n",
    "test = main_data.copy()\n",
    "print(main_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_case_words = {'nationalpost':'national post','businessinsider':'business insider','jewprofits': 'jew profits', 'QMAS': 'Quality Migrant Admission Scheme', 'casterating': 'castrating',\n",
    "                  'Kashmiristan': 'Kashmir', 'CareOnGo': 'India first and largest Online distributor of medicines',\n",
    "                  'Setya Novanto': 'a former Indonesian politician', 'TestoUltra': 'male sexual enhancement supplement',\n",
    "                  'rammayana': 'ramayana', 'Badaganadu': 'Brahmin community that mainly reside in Karnataka',\n",
    "                  'bitcjes': 'bitches', 'mastubrate': 'masturbate', 'Français': 'France',\n",
    "                  'Adsresses': 'address', 'flemmings': 'flemming', 'intermate': 'inter mating', 'feminisam': 'feminism',\n",
    "                  'cuckholdry': 'cuckold', 'Niggor': 'black hip-hop and electronic artist', 'narcsissist': 'narcissist',\n",
    "                  'Genderfluid': 'Gender fluid', ' Im ': ' I am ', ' dont ': ' do not ', 'Qoura': 'Quora',\n",
    "                  'ethethnicitesnicites': 'ethnicity', 'Namit Bathla': 'Content Writer', 'What sApp': 'WhatsApp',\n",
    "                  'Führer': 'Fuhrer', 'covfefe': 'coverage', 'accedentitly': 'accidentally', 'Cuckerberg': 'Zuckerberg',\n",
    "                  'transtrenders': 'incredibly disrespectful to real transgender people',\n",
    "                  'frozen tamod': 'Pornographic website', 'hindians': 'North Indian', 'hindian': 'North Indian',\n",
    "                  'celibatess': 'celibates', 'Trimp': 'Trump', 'wanket': 'wanker', 'wouldd': 'would',\n",
    "                  'arragent': 'arrogant', 'Ra - apist': 'rapist', 'idoot': 'idiot', 'gangstalkers': 'gangs talkers',\n",
    "                  'toastsexual': 'toast sexual', 'inapropriately': 'inappropriately', 'dumbassess': 'dumbass',\n",
    "                  'germanized': 'become german', 'helisexual': 'sexual', 'regilious': 'religious',\n",
    "                  'timetraveller': 'time traveller', 'darkwebcrawler': 'dark webcrawler', 'routez': 'route',\n",
    "                  'trumpians': 'Trump supporters','Trumpster':'trumpeters', 'irreputable': 'reputation', 'serieusly': 'seriously',\n",
    "                  'anti cipation': 'anticipation', 'microaggression': 'micro aggression', 'Afircans': 'Africans',\n",
    "                  'microapologize': 'micro apologize', 'Vishnus': 'Vishnu', 'excritment': 'excitement',\n",
    "                  'disagreemen': 'disagreement', 'gujratis': 'gujarati', 'gujaratis': 'gujarati',\n",
    "                  'ugggggggllly': 'ugly',\n",
    "                  'Germanity': 'German', 'SoyBoys': 'cuck men lacking masculine characteristics',\n",
    "                  'н': 'h', 'м': 'm', 'ѕ': 's', 'т': 't', 'в': 'b', 'υ': 'u', 'ι': 'i',\n",
    "                  'genetilia': 'genitalia', 'r - apist': 'rapist', 'Borokabama': 'Barack Obama',\n",
    "                  'arectifier': 'rectifier', 'pettypotus': 'petty potus', 'magibabble': 'magi babble',\n",
    "                  'nothinking': 'thinking', 'centimiters': 'centimeters', 'saffronized': 'India, politics, derogatory',\n",
    "                  'saffronize': 'India, politics, derogatory', ' incect ': ' insect ', 'weenus': 'elbow skin',\n",
    "                  'Pakistainies': 'Pakistanis', 'goodspeaks': 'good speaks', 'inpregnated': 'in pregnant',\n",
    "                  'rapefilms': 'rape films', 'rapiest': 'rapist', 'hatrednesss': 'hatred',\n",
    "                  'heightism': 'height discrimination', 'getmy': 'get my', 'onsocial': 'on social',\n",
    "                  'worstplatform': 'worst platform', 'platfrom': 'platform', 'instagate': 'instigate',\n",
    "                  'Loy Machedeo': 'person', ' dsire ': ' desire ', 'iservant': 'servant', 'intelliegent': 'intelligent',\n",
    "                  'WW 1': ' WW1 ', 'WW 2': ' WW2 ', 'ww 1': ' WW1 ', 'ww 2': ' WW2 ',\n",
    "                  'keralapeoples': 'kerala peoples', 'trumpervotes': 'trumper votes', 'fucktrumpet': 'fuck trumpet',\n",
    "                  'likebJaish': 'like bJaish', 'likemy': 'like my', 'Howlikely': 'How likely',\n",
    "                  'disagreementts': 'disagreements', 'disagreementt': 'disagreement',\n",
    "                  'meninist': \"male chauvinism\", 'feminists': 'feminism supporters', 'Ghumendra': 'Bhupendra',\n",
    "                  'emellishments': 'embellishments',\n",
    "                  'settelemen': 'settlement',\n",
    "                  'Richmencupid': 'rich men dating website', 'richmencupid': 'rich men dating website',\n",
    "                  'Gaudry - Schost': '', 'ladymen': 'ladyboy', 'hasserment': 'Harassment',\n",
    "                  'instrumentalizing': 'instrument', 'darskin': 'dark skin', 'balckwemen': 'balck women',\n",
    "                  'recommendor': 'recommender', 'wowmen': 'women', 'expertthink': 'expert think',\n",
    "                  'whitesplaining': 'white splaining', 'Inquoraing': 'inquiring', 'whilemany': 'while many',\n",
    "                  'manyother': 'many other', 'involvedinthe': 'involved in the', 'slavetrade': 'slave trade',\n",
    "                  'aswell': 'as well', 'fewshowanyRemorse': 'few show any Remorse', 'trageting': 'targeting',\n",
    "                  'getile': 'gentile', 'Gujjus': 'derogatory Gujarati', 'judisciously': 'judiciously',\n",
    "                  'Hue Mungus': 'feminist bait', 'Hugh Mungus': 'feminist bait', 'Hindustanis': '',\n",
    "                  'Virushka': 'Great Relationships Couple', 'exclusinary': 'exclusionary', 'himdus': 'hindus',\n",
    "                  'Milo Yianopolous': 'a British polemicist', 'hidusim': 'hinduism',\n",
    "                  'holocaustable': 'holocaust', 'evangilitacal': 'evangelical', 'Busscas': 'Buscas',\n",
    "                  'holocaustal': 'holocaust', 'incestious': 'incestuous', 'Tennesseus': 'Tennessee',\n",
    "                  'GusDur': 'Gus Dur',\n",
    "                  'RPatah - Tan Eng Hwan': 'Silsilah', 'Reinfectus': 'reinfect', 'pharisaistic': 'pharisaism',\n",
    "                  'nuslims': 'Muslims', 'taskus': '', 'musims': 'Muslims',\n",
    "                  'Musevi': 'the independence of Mexico', ' racious ': 'discrimination expression of racism',\n",
    "                  'Muslimophobia': 'Muslim phobia', 'justyfied': 'justified', 'holocause': 'holocaust',\n",
    "                  'musilim': 'Muslim', 'misandrous': 'misandry', 'glrous': 'glorious', 'desemated': 'decimated',\n",
    "                  'votebanks': 'vote banks', 'Parkistan': 'Pakistan', 'Eurooe': 'Europe', 'animlaistic': 'animalistic',\n",
    "                  'Asiasoid': 'Asian', 'Congoid': 'Congolese', 'inheritantly': 'inherently',\n",
    "                  'Asianisation': 'Becoming Asia',\n",
    "                  'Russosphere': 'russia sphere of influence', 'exMuslims': 'Ex-Muslims',\n",
    "                  'discriminatein': 'discrimination', ' hinus ': ' hindus ', 'Nibirus': 'Nibiru',\n",
    "                  'habius - corpus': 'habeas corpus', 'prentious': 'pretentious', 'Sussia': 'ancient Jewish village',\n",
    "                  'moustachess': 'moustaches', 'Russions': 'Russians', 'Yuguslavia': 'Yugoslavia',\n",
    "                  'atrocitties': 'atrocities', 'Muslimophobe': 'Muslim phobic', 'fallicious': 'fallacious',\n",
    "                  'recussed': 'recursed', '@ usafmonitor': '', 'lustfly': 'lustful', 'canMuslims': 'can Muslims',\n",
    "                  'journalust': 'journalist', 'digustingly': 'disgustingly', 'harasing': 'harassing',\n",
    "                  'greatuncle': 'great uncle', 'Drumpf': 'Trump', 'rejectes': 'rejected', 'polyagamous': 'polygamous',\n",
    "                  'Mushlims': 'Muslims', 'accusition': 'accusation', 'geniusses': 'geniuses',\n",
    "                  'moustachesomething': 'moustache something', 'heineous': 'heinous',\n",
    "                  'Sapiosexuals': 'sapiosexual', 'sapiosexuals': 'sapiosexual', 'Sapiosexual': 'sapiosexual',\n",
    "                  'sapiosexual': 'Sexually attracted to intelligence', 'pansexuals': 'pansexual',\n",
    "                  'autosexual': 'auto sexual', 'sexualSlutty': 'sexual Slutty', 'hetorosexuality': 'hetoro sexuality',\n",
    "                  'chinesese': 'chinese', 'pizza gate': 'debunked conspiracy theory',\n",
    "                  'countryless': 'Having no country',\n",
    "                  'muslimare': 'Muslim are', 'iPhoneX': 'iPhone', 'lionese': 'lioness', 'marionettist': 'Marionettes',\n",
    "                  'demonetize': 'demonetized', 'eneyone': 'anyone', 'Karonese': 'Karo people Indonesia',\n",
    "                  'minderheid': 'minder worse', 'mainstreamly': 'mainstream', 'contraproductive': 'contra productive',\n",
    "                  'diffenky': 'differently', 'abandined': 'abandoned', 'p0 rnstars': 'pornstars',\n",
    "                  'overproud': 'over proud',\n",
    "                  'cheekboned': 'cheek boned', 'heriones': 'heroines', 'eventhogh': 'even though',\n",
    "                  'americanmedicalassoc': 'american medical assoc', 'feelwhen': 'feel when', 'Hhhow': 'how',\n",
    "                  'reallySemites': 'really Semites', 'gamergaye': 'gamersgate', 'manspreading': 'man spreading',\n",
    "                  'thammana': 'Tamannaah Bhatia', 'dogmans': 'dogmas', 'managementskills': 'management skills',\n",
    "                  'mangoliod': 'mongoloid', 'geerymandered': 'gerrymandered', 'mandateing': 'man dateing',\n",
    "                  'Romanium': 'Romanum',\n",
    "                  'mailwoman': 'mail woman', 'humancoalition': 'human coalition',\n",
    "                  'manipullate': 'manipulate', 'everyo0 ne': 'everyone', 'takeove': 'takeover',\n",
    "                  'Nonchristians': 'Non Christians', 'goverenments': 'governments', 'govrment': 'government',\n",
    "                  'polygomists': 'polygamists', 'Demogorgan': 'Demogorgon', 'maralago': 'Mar-a-Lago',\n",
    "                  'antibigots': 'anti bigots', 'gouing': 'going', 'muzaffarbad': 'muzaffarabad',\n",
    "                  'suchvstupid': 'such stupid', 'apartheidisrael': 'apartheid israel', \n",
    "                  'personaltiles': 'personal titles', 'lawyergirlfriend': 'lawyer girl friend',\n",
    "                  'northestern': 'northwestern', 'yeardold': 'years old', 'masskiller': 'mass killer',\n",
    "                  'southeners': 'southerners', 'Unitedstatesian': 'United states',\n",
    "                  'peoplekind': 'people kind', 'peoplelike': 'people like', 'countrypeople': 'country people',\n",
    "                  'shitpeople': 'shit people', 'trumpology': 'trump ology', 'trumpites': 'Trump supporters',\n",
    "                  'trumplies': 'trump lies', 'donaldtrumping': 'donald trumping', 'trumpdating': 'trump dating',\n",
    "                  'trumpsters': 'trumpeters','Trumpers':'president trump', 'ciswomen': 'cis women', 'womenizer': 'womanizer',\n",
    "                  'pregnantwomen': 'pregnant women', 'autoliker': 'auto liker', 'smelllike': 'smell like',\n",
    "                  'autolikers': 'auto likers', 'religiouslike': 'religious like', 'likemail': 'like mail',\n",
    "                  'fislike': 'dislike', 'sneakerlike': 'sneaker like', 'like⬇': 'like',\n",
    "                  'likelovequotes': 'like lovequotes', 'likelogo': 'like logo', 'sexlike': 'sex like',\n",
    "                  'Whatwould': 'What would', 'Howwould': 'How would', 'manwould': 'man would',\n",
    "                  'exservicemen': 'ex servicemen', 'femenism': 'feminism', 'devopment': 'development',\n",
    "                  'doccuments': 'documents', 'supplementplatform': 'supplement platform', 'mendatory': 'mandatory',\n",
    "                  'moviments': 'movements', 'Kremenchuh': 'Kremenchug', 'docuements': 'documents',\n",
    "                  'determenism': 'determinism', 'envisionment': 'envision ment',\n",
    "                  'tricompartmental': 'tri compartmental', 'AddMovement': 'Add Movement',\n",
    "                  'mentionong': 'mentioning', 'Whichtreatment': 'Which treatment', 'repyament': 'repayment',\n",
    "                  'insemenated': 'inseminated', 'inverstment': 'investment',\n",
    "                  'managemental': 'manage mental', 'Inviromental': 'Environmental', 'menstrution': 'menstruation',\n",
    "                  'indtrument': 'instrument', 'mentenance': 'maintenance', 'fermentqtion': 'fermentation',\n",
    "                  'achivenment': 'achievement', 'mismanagements': 'mis managements', 'requriment': 'requirement',\n",
    "                  'denomenator': 'denominator', 'drparment': 'department', 'acumens': 'acumen s',\n",
    "                  'celemente': 'Clemente', 'manajement': 'management', 'govermenent': 'government',\n",
    "                  'accomplishmments': 'accomplishments', 'rendementry': 'rendement ry',\n",
    "                  'repariments': 'departments', 'menstrute': 'menstruate', 'determenistic': 'deterministic',\n",
    "                  'resigment': 'resignment', 'selfpayment': 'self payment', 'imrpovement': 'improvement',\n",
    "                  'enivironment': 'environment', 'compartmentley': 'compartment',\n",
    "                  'augumented': 'augmented', 'parmenent': 'permanent', 'dealignment': 'de alignment',\n",
    "                  'develepoments': 'developments', 'menstrated': 'menstruated', 'phnomenon': 'phenomenon',\n",
    "                  'Employmment': 'Employment', 'dimensionalise': 'dimensional ise', 'menigioma': 'meningioma',\n",
    "                  'recrument': 'recrement', 'Promenient': 'Provenient', 'gonverment': 'government',\n",
    "                  'statemment': 'statement', 'recuirement': 'requirement', 'invetsment': 'investment',\n",
    "                  'parilment': 'parchment', 'parmently': 'patiently', 'agreementindia': 'agreement india',\n",
    "                  'menifesto': 'manifesto', 'accomplsihments': 'accomplishments', 'disangagement': 'disengagement',\n",
    "                  'aevelopment': 'development', 'procument': 'procumbent', 'harashment': 'harassment',\n",
    "                  'Tiannanmen': 'Tiananmen', 'commensalisms': 'commensal isms', 'devlelpment': 'development',\n",
    "                  'dimensons': 'dimensions', 'recruitment2017': 'recruitment 2017', 'polishment': 'pol ishment',\n",
    "                  'CommentSafe': 'Comment Safe', 'meausrements': 'measurements', 'geomentrical': 'geometrical',\n",
    "                  'undervelopment': 'undevelopment', 'mensurational': 'mensuration al', 'fanmenow': 'fan menow',\n",
    "                  'permenganate': 'permanganate', 'bussinessmen': 'businessmen',\n",
    "                  'supertournaments': 'super tournaments', 'permanmently': 'permanently',\n",
    "                  'lamenectomy': 'lamnectomy', 'assignmentcanyon': 'assignment canyon', 'adgestment': 'adjustment',\n",
    "                  'mentalized': 'metalized', 'docyments': 'documents', 'requairment': 'requirement',\n",
    "                  'batsmencould': 'batsmen could', 'argumentetc': 'argument etc', 'enjoiment': 'enjoyment',\n",
    "                  'invement': 'movement', 'accompliushments': 'accomplishments', 'regements': 'regiments',\n",
    "                  'departmentHow': 'department How', 'Aremenian': 'Armenian', 'amenclinics': 'amen clinics',\n",
    "                  'nonfermented': 'non fermented', 'Instumentation': 'Instrumentation', 'mentalitiy': 'mentality',\n",
    "                  ' govermen ': 'goverment', 'underdevelopement': 'under developement', 'parlimentry': 'parliamentary',\n",
    "                  'indemenity': 'indemnity', 'Inatrumentation': 'Instrumentation', 'menedatory': 'mandatory',\n",
    "                  'mentiri': 'entire', 'accomploshments': 'accomplishments', 'instrumention': 'instrument ion',\n",
    "                  'afvertisements': 'advertisements', 'parlementarian': 'parlement arian',\n",
    "                  'entitlments': 'entitlements', 'endrosment': 'endorsement', 'improment': 'impriment',\n",
    "                  'archaemenid': 'Achaemenid', 'replecement': 'replacement', 'placdment': 'placement',\n",
    "                  'femenise': 'feminise', 'envinment': 'environment', 'AmenityCompany': 'Amenity Company',\n",
    "                  'increaments': 'increments', 'accomplihsments': 'accomplishments',\n",
    "                  'manygovernment': 'many government', 'panishments': 'punishments', 'elinment': 'eloinment',\n",
    "                  'mendalin': 'mend alin', 'farmention': 'farm ention', 'preincrement': 'pre increment',\n",
    "                  'postincrement': 'post increment', 'achviements': 'achievements', 'menditory': 'mandatory',\n",
    "                  'Emouluments': 'Emoluments', 'Stonemen': 'Stone men', 'menmium': 'medium',\n",
    "                  'entaglement': 'entanglement', 'integumen': 'integument', 'harassument': 'harassment',\n",
    "                  'retairment': 'retainment', 'enviorement': 'environment', 'tormentous': 'torment ous',\n",
    "                  'confiment': 'confident', 'Enchroachment': 'Encroachment', 'prelimenary': 'preliminary',\n",
    "                  'fudamental': 'fundamental', 'instrumenot': 'instrument', 'icrement': 'increment',\n",
    "                  'prodimently': 'prominently', 'meniss': 'menise', 'Whoimplemented': 'Who implemented',\n",
    "                  'Representment': 'Rep resentment', 'StartFragment': 'Start Fragment',\n",
    "                  'EndFragment': 'End Fragment', ' documentarie ': ' documentaries ', 'requriments': 'requirements',\n",
    "                  'constitutionaldevelopment': 'constitutional development', 'parlamentarians': 'parliamentarians',\n",
    "                  'Rumenova': 'Rumen ova', 'argruments': 'arguments', 'findamental': 'fundamental',\n",
    "                  'totalinvestment': 'total investment', 'gevernment': 'government', 'recmommend': 'recommend',\n",
    "                  'appsmoment': 'apps moment', 'menstruual': 'menstrual', 'immplemented': 'implemented',\n",
    "                  'engangement': 'engagement', 'invovement': 'involvement', 'returement': 'retirement',\n",
    "                  'simentaneously': 'simultaneously', 'accompishments': 'accomplishments',\n",
    "                  'menstraution': 'menstruation', 'experimently': 'experiment', 'abdimen': 'abdomen',\n",
    "                  'cemenet': 'cement', 'propelment': 'propel ment', 'unamendable': 'un amendable',\n",
    "                  'employmentnews': 'employment news', 'lawforcement': 'law forcement',\n",
    "                  'menstuating': 'menstruating', 'fevelopment': 'development', 'reglamented': 'reg lamented',\n",
    "                  'imrovment': 'improvement', 'recommening': 'recommending', 'sppliment': 'supplement',\n",
    "                  'measument': 'measurement', 'reimbrusement': 'reimbursement', 'Nutrament': 'Nutriment',\n",
    "                  'puniahment': 'punishment', 'subligamentous': 'sub ligamentous', 'comlementry': 'complementary',\n",
    "                  'reteirement': 'retirement', 'envioronments': 'environments', 'haraasment': 'harassment',\n",
    "                  'USAgovernment': 'USA government', 'Apartmentfinder': 'Apartment finder',\n",
    "                  'encironment': 'environment', 'metacompartment': 'meta compartment',\n",
    "                  'augumentation': 'argumentation', 'dsymenorrhoea': 'dysmenorrhoea',\n",
    "                  'nonabandonment': 'non abandonment', 'annoincement': 'announcement',\n",
    "                  'menberships': 'memberships', 'Gamenights': 'Game nights', 'enliightenment': 'enlightenment',\n",
    "                  'supplymentry': 'supplementary', 'parlamentary': 'parliamentary', 'duramen': 'dura men',\n",
    "                  'hotelmanagement': 'hotel management', 'deartment': 'department',\n",
    "                  'treatmentshelp': 'treatments help', 'attirements': 'attire ments',\n",
    "                  'amendmending': 'amend mending', 'pseudomeningocele': 'pseudo meningocele',\n",
    "                  'intrasegmental': 'intra segmental', 'treatmenent': 'treatment', 'infridgement': 'infringement',\n",
    "                  'infringiment': 'infringement', 'recrecommend': 'rec recommend', 'entartaiment': 'entertainment',\n",
    "                  'inplementing': 'implementing', 'indemendent': 'independent', 'tremendeous': 'tremendous',\n",
    "                  'commencial': 'commercial', 'scomplishments': 'accomplishments', 'Emplement': 'Implement',\n",
    "                  'dimensiondimensions': 'dimension dimensions', 'depolyment': 'deployment',\n",
    "                  'conpartment': 'compartment', 'govnments': 'movements', 'menstrat': 'menstruate',\n",
    "                  'accompplishments': 'accomplishments', 'Enchacement': 'Enchancement',\n",
    "                  'developmenent': 'development', 'emmenagogues': 'emmenagogue', 'aggeement': 'agreement',\n",
    "                  'elementsbond': 'elements bond', 'remenant': 'remnant', 'Manamement': 'Management',\n",
    "                  'Augumented': 'Augmented', 'dimensonless': 'dimensionless',\n",
    "                  'ointmentsointments': 'ointments ointments', 'achiements': 'achievements',\n",
    "                  'recurtment': 'recurrent', 'gouverments': 'governments', 'docoment': 'document',\n",
    "                  'programmingassignments': 'programming assignments', 'menifest': 'manifest',\n",
    "                  'investmentguru': 'investment guru', 'deployements': 'deployments', 'Invetsment': 'Investment',\n",
    "                  'plaement': 'placement', 'Perliament': 'Parliament', 'femenists': 'feminists',\n",
    "                  'ecumencial': 'ecumenical', 'advamcements': 'advancements', 'refundment': 'refund ment',\n",
    "                  'settlementtake': 'settlement take', 'mensrooms': 'mens rooms',\n",
    "                  'productManagement': 'product Management', 'armenains': 'armenians',\n",
    "                  'betweenmanagement': 'between management', 'difigurement': 'disfigurement',\n",
    "                  'Armenized': 'Armenize', 'hurrasement': 'hurra sement', 'mamgement': 'management',\n",
    "                  'momuments': 'monuments', 'eauipments': 'equipments', 'managemenet': 'management',\n",
    "                  'treetment': 'treatment', 'webdevelopement': 'web developement', 'supplemenary': 'supplementary',\n",
    "                  'Encironmental': 'Environmental', 'Understandment': 'Understand ment',\n",
    "                  'enrollnment': 'enrollment', 'thinkstrategic': 'think strategic', 'thinkinh': 'thinking',\n",
    "                  'Softthinks': 'Soft thinks', 'underthinking': 'under thinking', 'thinksurvey': 'think survey',\n",
    "                  'whitelash': 'white lash', 'whiteheds': 'whiteheads', 'whitetning': 'whitening',\n",
    "                  'whitegirls': 'white girls', 'whitewalkers': 'white walkers', 'manycountries': 'many countries',\n",
    "                  'accomany': 'accompany', 'fromGermany': 'from Germany', 'manychat': 'many chat',\n",
    "                  'Germanyl': 'Germany l', 'manyness': 'many ness', 'many4': 'many', 'exmuslims': 'ex muslims',\n",
    "                  'digitizeindia': 'digitize india', 'indiarush': 'india rush', 'indiareads': 'india reads',\n",
    "                  'telegraphindia': 'telegraph india', 'Southindia': 'South india', 'Airindia': 'Air india',\n",
    "                  'siliconindia': 'silicon india', 'airindia': 'air india', 'indianleaders': 'indian leaders',\n",
    "                  'fundsindia': 'funds india', 'indianarmy': 'indian army', 'Technoindia': 'Techno india',\n",
    "                  'Betterindia': 'Better india', 'capesindia': 'capes india', 'Rigetti': 'Ligetti',\n",
    "                  'vegetablr': 'vegetable', 'get90': 'get', 'Magetta': 'Maretta', 'nagetive': 'native',\n",
    "                  'isUnforgettable': 'is Unforgettable', 'get630': 'get 630', 'GadgetPack': 'Gadget Pack',\n",
    "                  'Languagetool': 'Language tool', 'bugdget': 'budget', 'africaget': 'africa get',\n",
    "                  'ABnegetive': 'Abnegative', 'orangetheory': 'orange theory', 'getsmuggled': 'get smuggled',\n",
    "                  'avegeta': 'ave geta', 'gettubg': 'getting', 'gadgetsnow': 'gadgets now',\n",
    "                  'surgetank': 'surge tank', 'gadagets': 'gadgets', 'getallparts': 'get allparts',\n",
    "                  'messenget': 'messenger', 'vegetarean': 'vegetarian', 'get1000': 'get 1000',\n",
    "                  'getfinancing': 'get financing', 'getdrip': 'get drip', 'AdsTargets': 'Ads Targets',\n",
    "                  'tgethr': 'together', 'vegetaries': 'vegetables', 'forgetfulnes': 'forgetfulness',\n",
    "                  'fisgeting': 'fidgeting', 'BudgetAir': 'Budget Air',\n",
    "                  'getDepersonalization': 'get Depersonalization', 'negetively': 'negatively',\n",
    "                  'gettibg': 'getting', 'nauget': 'naught', 'Bugetti': 'Bugatti', 'plagetum': 'plage tum',\n",
    "                  'vegetabale': 'vegetable', 'changetip': 'change tip', 'blackwashing': 'black washing',\n",
    "                  'blackpink': 'black pink', 'blackmoney': 'black money',\n",
    "                  'blackmarks': 'black marks', 'blackbeauty': 'black beauty', 'unblacklisted': 'un blacklisted',\n",
    "                  'blackdotes': 'black dotes', 'blackboxing': 'black boxing', 'blackpaper': 'black paper',\n",
    "                  'blackpower': 'black power', 'Latinamericans': 'Latin americans', 'musigma': 'mu sigma',\n",
    "                  'Indominus': 'In dominus', 'usict': 'USSCt', 'indominus': 'in dominus', 'Musigma': 'Mu sigma',\n",
    "                  'plus5': 'plus', 'Russiagate': 'Russia gate', 'russophobic': 'Russophobiac',\n",
    "                  'Marcusean': 'Marcuse an', 'Radijus': 'Radius', 'cobustion': 'combustion',\n",
    "                  'Austrialians': 'Australians', 'mylogenous': 'myogenous', 'Raddus': 'Radius',\n",
    "                  'hetrogenous': 'heterogenous', 'greenhouseeffect': 'greenhouse effect', 'aquous': 'aqueous',\n",
    "                  'Taharrush': 'Tahar rush', 'Senousa': 'Venous', 'diplococcus': 'diplo coccus',\n",
    "                  'CityAirbus': 'City Airbus', 'sponteneously': 'spontaneously', 'trustless': 't rustless',\n",
    "                  'Pushkaram': 'Pushkara m', 'Fusanosuke': 'Fu sanosuke', 'isthmuses': 'isthmus es',\n",
    "                  'lucideus': 'lucidum', 'overjustification': 'over justification', 'Bindusar': 'Bind usar',\n",
    "                  'cousera': 'couler', 'musturbation': 'masturbation', 'infustry': 'industry',\n",
    "                  'Huswifery': 'Huswife ry', 'rombous': 'bombous', 'disengenuously': 'disingenuously',\n",
    "                  'sllybus': 'syllabus', 'celcious': 'delicious', 'cellsius': 'celsius',\n",
    "                  'lethocerus': 'Lethocerus', 'monogmous': 'monogamous', 'Ballyrumpus': 'Bally rumpus',\n",
    "                  'Koushika': 'Koushik a', 'vivipoarous': 'viviparous', 'ludiculous': 'ridiculous',\n",
    "                  'sychronous': 'synchronous', 'industiry': 'industry', 'scuduse': 'scud use',\n",
    "                  'babymust': 'baby must', 'simultqneously': 'simultaneously', 'exust': 'ex ust',\n",
    "                  'notmusing': 'not musing', 'Zamusu': 'Amuse', 'tusaki': 'tu saki', 'Marrakush': 'Marrakesh',\n",
    "                  'justcheaptickets': 'just cheaptickets', 'Ayahusca': 'Ayahausca', 'samousa': 'samosa',\n",
    "                  'Gusenberg': 'Gutenberg', 'illustratuons': 'illustrations', 'extemporeneous': 'extemporaneous',\n",
    "                  'Mathusla': 'Mathusala', 'Confundus': 'Con fundus', 'tusts': 'trusts', 'poisenious': 'poisonous',\n",
    "                  'Mevius': 'Medius', 'inuslating': 'insulating', 'aroused21000': 'aroused 21000',\n",
    "                  'Wenzeslaus': 'Wenceslaus', 'JustinKase': 'Justin Kase', 'purushottampur': 'purushottam pur',\n",
    "                  'citruspay': 'citrus pay', 'secutus': 'sects', 'austentic': 'austenitic',\n",
    "                  'FacePlusPlus': 'Face PlusPlus', 'aysnchronous': 'asynchronous',\n",
    "                  'teamtreehouse': 'team treehouse', 'uncouncious': 'unconscious', 'Priebuss': 'Prie buss',\n",
    "                  'consciousuness': 'consciousness', 'susubsoil': 'su subsoil', 'trimegistus': 'Trismegistus',\n",
    "                  'protopeterous': 'protopterous', 'trustworhty': 'trustworthy', 'ushually': 'usually',\n",
    "                  'industris': 'industries', 'instantneous': 'instantaneous', 'superplus': 'super plus',\n",
    "                  'shrusti': 'shruti', 'hindhus': 'hindus', 'outonomous': 'autonomous', 'reliegious': 'religious',\n",
    "                  'Kousakis': 'Kou sakis', 'reusult': 'result', 'JanusGraph': 'Janus Graph',\n",
    "                  'palusami': 'palus ami', 'mussraff': 'muss raff', 'hukous': 'humous',\n",
    "                  'photoacoustics': 'photo acoustics', 'kushanas': 'kusha nas', 'justdile': 'justice',\n",
    "                  'Massahusetts': 'Massachusetts', 'uspset': 'upset', 'sustinet': 'sustinent',\n",
    "                  'consicious': 'conscious', 'Sadhgurus': 'Sadh gurus', 'hystericus': 'hysteric us',\n",
    "                  'visahouse': 'visa house', 'supersynchronous': 'super synchronous', 'posinous': 'rosinous',\n",
    "                  'Fernbus': 'Fern bus', 'Tiltbrush': 'Tilt brush', 'glueteus': 'gluteus', 'posionus': 'poisons',\n",
    "                  'Freus': 'Frees', 'Zhuchengtyrannus': 'Zhucheng tyrannus', 'savonious': 'sanious',\n",
    "                  'CusJo': 'Cusco', 'congusion': 'confusion', 'dejavus': 'dejavu s', 'uncosious': 'uncopious',\n",
    "                  'previius': 'previous', 'counciousness': 'conciousness', 'lustorus': 'lustrous',\n",
    "                  'sllyabus': 'syllabus', 'mousquitoes': 'mosquitoes', 'Savvius': 'Savvies', 'arceius': 'Arcesius',\n",
    "                  'prejusticed': 'prejudiced', 'requsitioned': 'requisitioned',\n",
    "                  'deindustralization': 'deindustrialization', 'muscleblaze': 'muscle blaze',\n",
    "                  'ConsciousX5': 'conscious', 'nitrogenious': 'nitrogenous', 'mauritious': 'mauritius',\n",
    "                  'rigrously': 'rigorously', 'Yutyrannus': 'Yu tyrannus', 'muscualr': 'muscular',\n",
    "                  'conscoiusness': 'consciousness', 'Causians': 'Crusians', 'WorkFusion': 'Work Fusion',\n",
    "                  'puspak': 'pu spak', 'Inspirus': 'Inspires', 'illiustrations': 'illustrations',\n",
    "                  'Nobushi': 'No bushi', 'theuseof': 'thereof', 'suspicius': 'suspicious', 'Intuous': 'Virtuous',\n",
    "                  'gaushalas': 'gaus halas', 'campusthrough': 'campus through', 'seriousity': 'seriosity',\n",
    "                  'resustence': 'resistence', 'geminatus': 'geminates', 'disquss': 'discuss',\n",
    "                  'nicholus': 'nicholas', 'Husnai': 'Hussar', 'diiscuss': 'discuss', 'diffussion': 'diffusion',\n",
    "                  'phusicist': 'physicist', 'ernomous': 'enormous', 'Khushali': 'Khushal i', 'heitus': 'Leitus',\n",
    "                  'cracksbecause': 'cracks because', 'Nautlius': 'Nautilus', 'trausted': 'trusted',\n",
    "                  'Dardandus': 'Dardanus', 'Megatapirus': 'Mega tapirus', 'clusture': 'culture',\n",
    "                  'vairamuthus': 'vairamuthu s', 'disclousre': 'disclosure',\n",
    "                  'industrilaization': 'industrialization', 'musilms': 'muslims', 'Australia9': 'Australian',\n",
    "                  'causinng': 'causing', 'ibdustries': 'industries', 'searious': 'serious',\n",
    "                  'Coolmuster': 'Cool muster', 'sissyphus': 'sisyphus', ' justificatio ': 'justification',\n",
    "                  'antihindus': 'anti hindus', 'Moduslink': 'Modus link', 'zymogenous': 'zymogen ous',\n",
    "                  'prospeorus': 'prosperous', 'Retrocausality': 'Retro causality', 'FusionGPS': 'Fusion GPS',\n",
    "                  'Mouseflow': 'Mouse flow', 'bootyplus': 'booty plus', 'Itylus': 'I tylus',\n",
    "                  'Olnhausen': 'Olshausen', 'suspeect': 'suspect', 'entusiasta': 'enthusiast',\n",
    "                  'fecetious': 'facetious', 'bussiest': 'fussiest', 'Draconius': 'Draconis',\n",
    "                  'requsite': 'requisite', 'nauseatic': 'nausea tic', 'Brusssels': 'Brussels',\n",
    "                  'repurcussion': 'repercussion', 'Jeisus': 'Jesus', 'philanderous': 'philander ous',\n",
    "                  'muslisms': 'muslims', 'august2017': 'august 2017', 'calccalculus': 'calc calculus',\n",
    "                  'unanonymously': 'un anonymously', 'Imaprtus': 'Impetus', 'carnivorus': 'carnivorous',\n",
    "                  'Corypheus': 'Coryphees', 'austronauts': 'astronauts', 'neucleus': 'nucleus',\n",
    "                  'housepoor': 'house poor', 'rescouses': 'responses', 'Tagushi': 'Tagus hi',\n",
    "                  'hyperfocusing': 'hyper focusing', 'nutriteous': 'nutritious', 'chylus': 'chylous',\n",
    "                  'preussure': 'pressure', 'outfocus': 'out focus', 'Hanfus': 'Hannus', 'Rustyrose': 'Rusty rose',\n",
    "                  'vibhushant': 'vibhushan t', 'conciousnes': 'conciousness', 'Venus25': 'Venus',\n",
    "                  'Sedataious': 'Seditious', 'promuslim': 'pro muslim', 'statusGuru': 'status Guru',\n",
    "                  'yousician': 'musician', 'transgenus': 'trans genus', 'Pushbullet': 'Push bullet',\n",
    "                  'jeesyllabus': 'jee syllabus', 'complusary': 'compulsory', 'Holocoust': 'Holocaust',\n",
    "                  'careerplus': 'career plus', 'Lllustrate': 'Illustrate', 'Musino': 'Musion',\n",
    "                  'Phinneus': 'Phineus', 'usedtoo': 'used too', 'JustBasic': 'Just Basic', 'webmusic': 'web music',\n",
    "                  'TrustKit': 'Trust Kit', 'industrZgies': 'industries', 'rubustness': 'robustness',\n",
    "                  'Missuses': 'Miss uses', 'Musturbation': 'Masturbation', 'bustees': 'bus tees',\n",
    "                  'justyfy': 'justify', 'pegusus': 'pegasus', 'industrybuying': 'industry buying',\n",
    "                  'advantegeous': 'advantageous', 'kotatsus': 'kotatsu s', 'justcreated': 'just created',\n",
    "                  'simultameously': 'simultaneously', 'husoone': 'huso one', 'twiceusing': 'twice using',\n",
    "                  'cetusplay': 'cetus play', 'sqamous': 'squamous', 'claustophobic': 'claustrophobic',\n",
    "                  'Kaushika': 'Kaushik a', 'dioestrus': 'di oestrus', 'Degenerous': 'De generous',\n",
    "                  'neculeus': 'nucleus', 'cutaneously': 'cu taneously', 'Alamotyrannus': 'Alamo tyrannus',\n",
    "                  'Ivanious': 'Avanious', 'arceous': 'araceous', 'Flixbus': 'Flix bus', 'caausing': 'causing',\n",
    "                  'publious': 'Publius', 'Juilus': 'Julius', 'Australianism': 'Australian ism',\n",
    "                  'vetronus': 'verrons', 'nonspontaneous': 'non spontaneous', 'calcalus': 'calculus',\n",
    "                  'commudus': 'Commodus', 'Rheusus': 'Rhesus', 'syallubus': 'syllabus', 'Yousician': 'Musician',\n",
    "                  'qurush': 'qu rush', 'athiust': 'athirst', 'conclusionless': 'conclusion less',\n",
    "                  'usertesting': 'user testing', 'redius': 'radius', 'Austrolia': 'Australia',\n",
    "                  'sllaybus': 'syllabus', 'toponymous': 'top onymous', 'businiss': 'business',\n",
    "                  'hyperthalamus': 'hyper thalamus', 'clause55': 'clause', 'cosicous': 'conscious',\n",
    "                  'Sushena': 'Saphena', 'Luscinus': 'Luscious', 'Prussophile': 'Russophile', 'jeaslous': 'jealous',\n",
    "                  'Austrelia': 'Australia', 'contiguious': 'contiguous',\n",
    "                  'subconsciousnesses': 'sub consciousnesses', ' jusification ': 'justification',\n",
    "                  'dilusion': 'delusion', 'anticoncussive': 'anti concussive', 'disngush': 'disgust',\n",
    "                  'constiously': 'consciously', 'filabustering': 'filibustering', 'GAPbuster': 'GAP buster',\n",
    "                  'insectivourous': 'insectivorous', 'glocuse': 'louse', 'Antritrust': 'Antitrust',\n",
    "                  'thisAustralian': 'this Australian', 'FusionDrive': 'Fusion Drive', 'nuclus': 'nucleus',\n",
    "                  'abussive': 'abusive', 'mustang1': 'mustangs', 'inradius': 'in radius', 'polonious': 'polonius',\n",
    "                  'ofKulbhushan': 'of Kulbhushan', 'homosporous': 'homos porous', 'circumradius': 'circum radius',\n",
    "                  'atlous': 'atrous', 'insustry': 'industry', 'campuswith': 'campus with', 'beacsuse': 'because',\n",
    "                  'concuous': 'conscious', 'nonHindus': 'non Hindus', 'carnivourous': 'carnivorous',\n",
    "                  'tradeplus': 'trade plus', 'Jeruselam': 'Jerusalem',\n",
    "                  'musuclar': 'muscular', 'deangerous': 'dangerous', 'disscused': 'discussed',\n",
    "                  'industdial': 'industrial', 'sallatious': 'fallacious', 'rohmbus': 'rhombus',\n",
    "                  'golusu': 'gol usu', 'Minangkabaus': 'Minangkabau s', 'Mustansiriyah': 'Mustansiriya h',\n",
    "                  'anomymously': 'anonymously', 'abonymously': 'anonymously', 'indrustry': 'industry',\n",
    "                  'Musharrf': 'Musharraf', 'workouses': 'workhouses', 'sponataneously': 'spontaneously',\n",
    "                  'anmuslim': 'an muslim', 'syallbus': 'syllabus', 'presumptuousnes': 'presumptuousness',\n",
    "                  'Thaedus': 'Thaddus', 'industey': 'industry', 'hkust': 'hust', 'Kousseri': 'Kousser i',\n",
    "                  'mousestats': 'mouses tats', 'russiagate': 'russia gate', 'simantaneously': 'simultaneously',\n",
    "                  'Austertana': 'Auster tana', 'infussions': 'infusions', 'coclusion': 'conclusion',\n",
    "                  'sustainabke': 'sustainable', 'tusami': 'tu sami', 'anonimously': 'anonymously',\n",
    "                  'usebase': 'use base', 'balanoglossus': 'Balanoglossus', 'Unglaus': 'Ung laus',\n",
    "                  'ignoramouses': 'ignoramuses', 'snuus': 'snugs', 'reusibility': 'reusability',\n",
    "                  'Straussianism': 'Straussian ism', 'simoultaneously': 'simultaneously',\n",
    "                  'realbonus': 'real bonus', 'nuchakus': 'nunchakus', 'annonimous': 'anonymous',\n",
    "                  'Incestious': 'Incestuous', 'Manuscriptology': 'Manuscript ology', 'difusse': 'diffuse',\n",
    "                  'Pliosaurus': 'Pliosaur us', 'cushelle': 'cush elle', 'Catallus': 'Catullus',\n",
    "                  'MuscleBlaze': 'Muscle Blaze', 'confousing': 'confusing', 'enthusiasmless': 'enthusiasm less',\n",
    "                  'Tetherusd': 'Tethered', 'Josephius': 'Josephus', 'jusrlt': 'just',\n",
    "                  'simutaneusly': 'simultaneously', 'mountaneous': 'mountainous', 'Badonicus': 'Sardonicus',\n",
    "                  'muccus': 'mucous', 'nicus': 'nidus', 'austinlizards': 'austin lizards',\n",
    "                  'errounously': 'erroneously', 'Australua': 'Australia', 'sylaabus': 'syllabus',\n",
    "                  'dusyant': 'distant', 'javadiscussion': 'java discussion', 'megabuses': 'mega buses',\n",
    "                  'danergous': 'dangerous', 'contestious': 'contentious', 'exause': 'excuse',\n",
    "                  'muscluar': 'muscular', 'avacous': 'vacuous', 'Ingenhousz': 'Ingenious',\n",
    "                  'holocausting': 'holocaust ing', 'Pakustan': 'Pakistan', 'purusharthas': 'purushartha',\n",
    "                  'bapus': 'bapu s', 'useul': 'useful', 'pretenious': 'pretentious', 'homogeneus': 'homogeneous',\n",
    "                  'bhlushes': 'blushes', 'Saggittarius': 'Sagittarius', 'sportsusa': 'sports usa',\n",
    "                  'kerataconus': 'keratoconus', 'infrctuous': 'infectuous', 'Anonoymous': 'Anonymous',\n",
    "                  'triphosphorus': 'tri phosphorus', 'ridicjlously': 'ridiculously',\n",
    "                  'worldbusiness': 'world business', 'hollcaust': 'holocaust', 'Dusra': 'Dura',\n",
    "                  'meritious': 'meritorious', 'Sauskes': 'Causes', 'inudustry': 'industry',\n",
    "                  'frustratd': 'frustrate', 'hypotenous': 'hypogenous', 'Dushasana': 'Dush asana',\n",
    "                  'saadus': 'status', 'keratokonus': 'keratoconus', 'Jarrus': 'Harrus', 'neuseous': 'nauseous',\n",
    "                  'simutanously': 'simultaneously', 'diphosphorus': 'di phosphorus', 'sulprus': 'surplus',\n",
    "                  'Hasidus': 'Hasid us', 'suspenive': 'suspensive', 'illlustrator': 'illustrator',\n",
    "                  'userflows': 'user flows', 'intrusivethoughts': 'intrusive thoughts', 'countinous': 'continuous',\n",
    "                  'gpusli': 'gusli', 'Calculus1': 'Calculus', 'bushiri': 'Bushire',\n",
    "                  'torvosaurus': 'Torosaurus', 'chestbusters': 'chest busters', 'Satannus': 'Sat annus',\n",
    "                  'falaxious': 'fallacious', 'obnxious': 'obnoxious', 'tranfusions': 'transfusions',\n",
    "                  'PlayMagnus': 'Play Magnus', 'Epicodus': 'Episodes', 'Hypercubus': 'Hypercubes',\n",
    "                  'Musickers': 'Musick ers', 'programmebecause': 'programme because', 'indiginious': 'indigenous',\n",
    "                  'housban': 'Housman', 'iusso': 'kusso', 'annilingus': 'anilingus', 'Nennus': 'Genius',\n",
    "                  'pussboy': 'puss boy', 'Photoacoustics': 'Photo acoustics', 'Hindusthanis': 'Hindustanis',\n",
    "                  'lndustrial': 'industrial', 'tyrannously': 'tyrannous', 'Susanoomon': 'Susanoo mon',\n",
    "                  'colmbus': 'columbus', 'sussessful': 'successful', 'ousmania': 'ous mania',\n",
    "                  'ilustrating': 'illustrating', 'famousbirthdays': 'famous birthdays',\n",
    "                  'suspectance': 'suspect ance', 'extroneous': 'extraneous', 'teethbrush': 'teeth brush',\n",
    "                  'abcmouse': 'abc mouse', 'degenerous': 'de generous', 'doesGauss': 'does Gauss',\n",
    "                  'insipudus': 'insipidus', 'movielush': 'movie lush', 'Rustichello': 'Rustic hello',\n",
    "                  'Firdausiya': 'Firdausi ya', 'checkusers': 'check users', 'householdware': 'household ware',\n",
    "                  'prosporously': 'prosperously', 'SteLouse': 'Ste Louse', 'obfuscaton': 'obfuscation',\n",
    "                  'amorphus': 'amorph us', 'trustworhy': 'trustworthy', 'celsious': 'cesious',\n",
    "                  'dangorous': 'dangerous', 'anticancerous': 'anti cancerous', 'cousi ': 'cousin ',\n",
    "                  'austroloid': 'australoid', 'fergussion': 'percussion', 'andKyokushin': 'and Kyokushin',\n",
    "                  'cousan': 'cousin', 'Huskystar': 'Hu skystar', 'retrovisus': 'retrovirus', 'becausr': 'because',\n",
    "                  'Jerusalsem': 'Jerusalem', 'motorious': 'notorious', 'industrilised': 'industrialised',\n",
    "                  'powerballsusa': 'powerballs usa', 'monoceious': 'monoecious', 'batteriesplus': 'batteries plus',\n",
    "                  'nonviscuous': 'nonviscous', 'industion': 'induction', 'bussinss': 'bussings',\n",
    "                  'userbags': 'user bags', 'Jlius': 'Julius', 'thausand': 'thousand', 'plustwo': 'plus two',\n",
    "                  'defpush': 'def push', 'subconcussive': 'sub concussive', 'muslium': 'muslim',\n",
    "                  'industrilization': 'industrialization', 'Maurititus': 'Mauritius', 'uslme': 'some',\n",
    "                  'Susgaon': 'Surgeon', 'Pantherous': 'Panther ous', 'antivirius': 'antivirus',\n",
    "                  'Trustclix': 'Trust clix', 'silumtaneously': 'simultaneously', 'Icompus': 'Corpus',\n",
    "                  'atonomous': 'autonomous', 'Reveuse': 'Reve use', 'legumnous': 'leguminous',\n",
    "                  'syllaybus': 'syllabus', 'louspeaker': 'loudspeaker', 'susbtraction': 'substraction',\n",
    "                  'virituous': 'virtuous', 'disastrius': 'disastrous', 'jerussalem': 'jerusalem',\n",
    "                  'Industrailzed': 'Industrialized', 'recusion': 'recushion',\n",
    "                  'simultenously': 'simultaneously',\n",
    "                  'Pulphus': 'Pulpous', 'harbaceous': 'herbaceous', 'phlegmonous': 'phlegmon ous', 'use38': 'use',\n",
    "                  'jusify': 'justify', 'instatanously': 'instantaneously', 'tetramerous': 'tetramer ous',\n",
    "                  'usedvin': 'used vin', 'sagittarious': 'sagittarius', 'mausturbate': 'masturbate',\n",
    "                  'subcautaneous': 'subcutaneous', 'dangergrous': 'dangerous', 'sylabbus': 'syllabus',\n",
    "                  'hetorozygous': 'heterozygous', 'Ignasius': 'Ignacius', 'businessbor': 'business bor',\n",
    "                  'Bhushi': 'Thushi', 'Moussolini': 'Mussolini', 'usucaption': 'usu caption',\n",
    "                  'Customzation': 'Customization', 'cretinously': 'cretinous', 'genuiuses': 'geniuses',\n",
    "                  'Moushmee': 'Mousmee', 'neigous': 'nervous',\n",
    "                  'infrustructre': 'infrastructure', 'Ilusha': 'Ilesha', 'suconciously': 'unconciously',\n",
    "                  'stusy': 'study', 'mustectomy': 'mastectomy', 'Farmhousebistro': 'Farmhouse bistro',\n",
    "                  'instantanous': 'instantaneous', 'JustForex': 'Just Forex', 'Indusyry': 'Industry',\n",
    "                  'mustabating': 'must abating', 'uninstrusive': 'unintrusive', 'customshoes': 'customs hoes',\n",
    "                  'homageneous': 'homogeneous', 'Empericus': 'Imperious', 'demisexuality': 'demi sexuality',\n",
    "                  'transexualism': 'transsexualism', 'sexualises': 'sexualise', 'demisexuals': 'demisexual',\n",
    "                  'sexuly': 'sexily', 'Pornosexuality': 'Porno sexuality', 'sexond': 'second', 'sexxual': 'sexual',\n",
    "                  'asexaul': 'asexual', 'sextactic': 'sex tactic', 'sexualityism': 'sexuality ism',\n",
    "                  'monosexuality': 'mono sexuality', 'intwrsex': 'intersex', 'hypersexualize': 'hyper sexualize',\n",
    "                  'homosexualtiy': 'homosexuality', 'examsexams': 'exams exams', 'sexmates': 'sex mates',\n",
    "                  'sexyjobs': 'sexy jobs', 'sexitest': 'sexiest', 'fraysexual': 'fray sexual',\n",
    "                  'sexsurrogates': 'sex surrogates', 'sexuallly': 'sexually', 'gamersexual': 'gamer sexual',\n",
    "                  'greysexual': 'grey sexual', 'omnisexuality': 'omni sexuality', 'hetereosexual': 'heterosexual',\n",
    "                  'productsexamples': 'products examples', 'sexgods': 'sex gods', 'semisexual': 'semi sexual',\n",
    "                  'homosexulity': 'homosexuality', 'sexeverytime': 'sex everytime', 'neurosexist': 'neuro sexist',\n",
    "                  'worldquant': 'world quant', 'Freshersworld': 'Freshers world', 'smartworld': 'sm artworld',\n",
    "                  'Mistworlds': 'Mist worlds', 'boothworld': 'booth world', 'ecoworld': 'eco world',\n",
    "                  'Ecoworld': 'Eco world', 'underworldly': 'under worldly', 'worldrank': 'world rank',\n",
    "                  'Clearworld': 'Clear world', 'Boothworld': 'Booth world', 'Rimworld': 'Rim world',\n",
    "                  'cryptoworld': 'crypto world', 'machineworld': 'machine world', 'worldwideley': 'worldwide ley',\n",
    "                  'capuletwant': 'capulet want', 'Bhagwanti': 'Bhagwant i', 'Unwanted72': 'Unwanted 72',\n",
    "                  'wantrank': 'want rank',\n",
    "                  'willhappen': 'will happen', 'thateasily': 'that easily',\n",
    "                  'Whatevidence': 'What evidence', 'metaphosphates': 'meta phosphates',\n",
    "                  'exilarchate': 'exilarch ate', 'aulphate': 'sulphate', 'Whateducation': 'What education',\n",
    "                  'persulphates': 'per sulphates', 'disulphate': 'di sulphate', 'picosulphate': 'pico sulphate',\n",
    "                  'tetraosulphate': 'tetrao sulphate', 'prechinese': 'pre chinese',\n",
    "                  'Hellochinese': 'Hello chinese', 'muchdeveloped': 'much developed', 'stomuch': 'stomach',\n",
    "                  'Whatmakes': 'What makes', 'Lensmaker': 'Lens maker', 'eyemake': 'eye make',\n",
    "                  'Techmakers': 'Tech makers', 'cakemaker': 'cake maker', 'makeup411': 'makeup 411',\n",
    "                  'objectmake': 'object make', 'crazymaker': 'crazy maker', 'techmakers': 'tech makers',\n",
    "                  'makedonian': 'macedonian', 'makeschool': 'make school', 'anxietymake': 'anxiety make',\n",
    "                  'makeshifter': 'make shifter', 'countryball': 'country ball', 'Whichcountry': 'Which country',\n",
    "                  'countryHow': 'country How', 'Zenfone': 'Zen fone', 'Electroneum': 'Electro neum',\n",
    "                  'electroneum': 'electro neum', 'Demonetisation': 'demonetization', 'zenfone': 'zen fone',\n",
    "                  'ZenFone': 'Zen Fone', 'onecoin': 'one coin', 'demonetizing': 'demonetized',\n",
    "                  'iphone7': 'iPhone', 'iPhone6': 'iPhone', 'microneedling': 'micro needling', 'iphone6': 'iPhone',\n",
    "                  'Monegasques': 'Monegasque s', 'demonetised': 'demonetized',\n",
    "                  'EveryoneDiesTM': 'EveryoneDies TM', 'teststerone': 'testosterone', 'DoneDone': 'Done Done',\n",
    "                  'papermoney': 'paper money', 'Sasabone': 'Sasa bone', 'Blackphone': 'Black phone',\n",
    "                  'Bonechiller': 'Bone chiller', 'Moneyfront': 'Money front', 'workdone': 'work done',\n",
    "                  'iphoneX': 'iPhone', 'roxycodone': 'r oxycodone',\n",
    "                  'moneycard': 'money card', 'Fantocone': 'Fantocine', 'eletronegativity': 'electronegativity',\n",
    "                  'mellophones': 'mellophone s', 'isotones': 'iso tones', 'donesnt': 'doesnt',\n",
    "                  'thereanyone': 'there anyone', 'electronegativty': 'electronegativity',\n",
    "                  'commissiioned': 'commissioned', 'earvphone': 'earphone', 'condtioners': 'conditioners',\n",
    "                  'demonetistaion': 'demonetization', 'ballonets': 'ballo nets', 'DoneClaim': 'Done Claim',\n",
    "                  'alimoney': 'alimony', 'iodopovidone': 'iodo povidone', 'bonesetters': 'bone setters',\n",
    "                  'componendo': 'compon endo', 'probationees': 'probationers', 'one300': 'one 300',\n",
    "                  'nonelectrolyte': 'non electrolyte', 'ozonedepletion': 'ozone depletion',\n",
    "                  'Stonehart': 'Stone hart', 'Vodafone2': 'Vodafones', 'chaparone': 'chaperone',\n",
    "                  'Noonein': 'Noo nein', 'Frosione': 'Erosion', 'IPhone7': 'Iphone', 'pentanone': 'penta none',\n",
    "                  'poneglyphs': 'pone glyphs', 'cyclohexenone': 'cyclohexanone', 'marlstone': 'marls tone',\n",
    "                  'androneda': 'andromeda', 'iphone8': 'iPhone', 'acidtone': 'acid tone',\n",
    "                  'noneconomically': 'non economically', 'Honeyfund': 'Honey fund', 'germanophone': 'Germanophobe',\n",
    "                  'Democratizationed': 'Democratization ed', 'haoneymoon': 'honeymoon', 'iPhone7': 'iPhone 7',\n",
    "                  'someonewith': 'some onewith', 'Hexanone': 'Hexa none', 'bonespur': 'bones pur',\n",
    "                  'sisterzoned': 'sister zoned', 'HasAnyone': 'Has Anyone',\n",
    "                  'stonepelters': 'stone pelters', 'Chronexia': 'Chronaxia', 'brotherzone': 'brother zone',\n",
    "                  'brotherzoned': 'brother zoned', 'fonecare': 'f onecare', 'nonexsistence': 'nonexistence',\n",
    "                  'conents': 'contents', 'phonecases': 'phone cases', 'Commissionerates': 'Commissioner ates',\n",
    "                  'activemoney': 'active money', 'dingtone': 'ding tone', 'wheatestone': 'wheatstone',\n",
    "                  'chiropractorone': 'chiropractor one', 'heeadphones': 'headphones', 'Maimonedes': 'Maimonides',\n",
    "                  'onepiecedeals': 'onepiece deals', 'oneblade': 'one blade', 'venetioned': 'Venetianed',\n",
    "                  'sunnyleone': 'sunny leone', 'prendisone': 'prednisone', 'Anglosaxophone': 'Anglo saxophone',\n",
    "                  'Blackphones': 'Black phones', 'jionee': 'jinnee', 'chromonema': 'chromo nema',\n",
    "                  'iodoketones': 'iodo ketones', 'demonetizations': 'demonetization', 'aomeone': 'someone',\n",
    "                  'trillonere': 'trillones', 'abandonee': 'abandon',\n",
    "                  'MasterColonel': 'Master Colonel', 'fronend': 'friend', 'Wildstone': 'Wilds tone',\n",
    "                  'patitioned': 'petitioned', 'lonewolfs': 'lone wolfs', 'Spectrastone': 'Spectra stone',\n",
    "                  'dishonerable': 'dishonorable', 'poisiones': 'poisons',\n",
    "                  'condioner': 'conditioner', 'unpermissioned': 'unper missioned', 'friedzone': 'fried zone',\n",
    "                  'umumoney': 'umu money', 'anyonestudied': 'anyone studied', 'dictioneries': 'dictionaries',\n",
    "                  'nosebone': 'nose bone', 'ofVodafone': 'of Vodafone',\n",
    "                  'Yumstone': 'Yum stone', 'oxandrolonesteroid': 'oxandrolone steroid',\n",
    "                  'Mifeprostone': 'Mifepristone', 'pheramones': 'pheromones',\n",
    "                  'sinophone': 'Sinophobe', 'peloponesian': 'peloponnesian', 'michrophone': 'microphone',\n",
    "                  'commissionets': 'commissioners', 'methedone': 'methadone', 'cobditioners': 'conditioners',\n",
    "                  'urotone': 'protone', 'smarthpone': 'smartphone', 'conecTU': 'connect you', 'beloney': 'boloney',\n",
    "                  'comfortzone': 'comfort zone', 'testostersone': 'testosterone', 'camponente': 'component',\n",
    "                  'Idonesia': 'Indonesia', 'dolostones': 'dolostone', 'psiphone': 'psi phone',\n",
    "                  'ceftriazone': 'ceftriaxone', 'feelonely': 'feel onely', 'monetation': 'moderation',\n",
    "                  'activationenergy': 'activation energy', 'moneydriven': 'money driven',\n",
    "                  'staionery': 'stationery', 'zoneflex': 'zone flex', 'moneycash': 'money cash',\n",
    "                  'conectiin': 'connection', 'Wannaone': 'Wanna one',\n",
    "                  'Pictones': 'Pict ones', 'demonentization': 'demonetization',\n",
    "                  'phenonenon': 'phenomenon', 'evenafter': 'even after', 'Sevenfriday': 'Seven friday',\n",
    "                  'Devendale': 'Evendale', 'theeventchronicle': 'the event chronicle',\n",
    "                  'seventysomething': 'seventy something', 'sevenpointed': 'seven pointed',\n",
    "                  'richfeel': 'rich feel', 'overfeel': 'over feel', 'feelingstupid': 'feeling stupid',\n",
    "                  'Photofeeler': 'Photo feeler', 'feelomgs': 'feelings', 'feelinfs': 'feelings',\n",
    "                  'PlayerUnknown': 'Player Unknown', 'Playerunknown': 'Player unknown', 'knowlefge': 'knowledge',\n",
    "                  'knowledgd': 'knowledge', 'knowledeg': 'knowledge', 'knowble': 'Knowle', 'Howknow': 'Howk now',\n",
    "                  'knowledgeWoods': 'knowledge Woods', 'knownprogramming': 'known programming',\n",
    "                  'selfknowledge': 'self knowledge', 'knowldage': 'knowledge', 'knowyouve': 'know youve',\n",
    "                  'aknowlege': 'knowledge', 'Audetteknown': 'Audette known', 'knowlegdeable': 'knowledgeable',\n",
    "                  'trueoutside': 'true outside', 'saynthesize': 'synthesize', 'EssayTyper': 'Essay Typer',\n",
    "                  'meesaya': 'mee saya', 'Rasayanam': 'Rasayan am', 'fanessay': 'fan essay', 'momsays': 'moms ays',\n",
    "                  'sayying': 'saying', 'saydaw': 'say daw', 'Fanessay': 'Fan essay', 'theyreally': 'they really',\n",
    "                  'gayifying': 'gayed up with homosexual love', 'gayke': 'gay Online retailers',\n",
    "                  'Lingayatism': 'Lingayat',\n",
    "                  'macapugay': 'Macaulay', 'jewsplain': 'jews plain',\n",
    "                  'banggood': 'bang good', 'goodfriends': 'good friends',\n",
    "                  'goodfirms': 'good firms', 'Banggood': 'Bang good', 'dogooder': 'do gooder',\n",
    "                  'stillshots': 'stills hots', 'stillsuits': 'still suits', 'panromantic': 'pan romantic',\n",
    "                  'paracommando': 'para commando', 'romantize': 'romanize', 'manupulative': 'manipulative',\n",
    "                  'manjha': 'mania', 'mankrit': 'mank rit',\n",
    "                  'heteroromantic': 'hetero romantic', 'pulmanery': 'pulmonary', 'manpads': 'man pads',\n",
    "                  'supermaneuverable': 'super maneuverable', 'mandatkry': 'mandatory', 'armanents': 'armaments',\n",
    "                  'manipative': 'mancipative', 'himanity': 'humanity', 'maneuever': 'maneuver',\n",
    "                  'Kumarmangalam': 'Kumar mangalam', 'Brahmanwadi': 'Brahman wadi',\n",
    "                  'exserviceman': 'ex serviceman',\n",
    "                  'managewp': 'managed', 'manies': 'many', 'recordermans': 'recorder mans',\n",
    "                  'Feymann': 'Heymann', 'salemmango': 'salem mango', 'manufraturing': 'manufacturing',\n",
    "                  'sreeman': 'freeman', 'tamanaa': 'Tamanac', 'chlamydomanas': 'chlamydomonas',\n",
    "                  'comandant': 'commandant', 'huemanity': 'humanity', 'manaagerial': 'managerial',\n",
    "                  'lithromantics': 'lith romantics',\n",
    "                  'geramans': 'germans', 'Nagamandala': 'Naga mandala', 'humanitariarism': 'humanitarianism',\n",
    "                  'wattman': 'watt man', 'salesmanago': 'salesman ago', 'Washwoman': 'Wash woman',\n",
    "                  'rammandir': 'ram mandir', 'nomanclature': 'nomenclature', 'Haufman': 'Kaufman',\n",
    "                  'prefomance': 'performance', 'ramanunjan': 'Ramanujan', 'Freemansonry': 'Freemasonry',\n",
    "                  'supermaneuverability': 'super maneuverability', 'manstruate': 'menstruate',\n",
    "                  'Tarumanagara': 'Taruma nagara', 'RomanceTale': 'Romance Tale', 'heteromantic': 'hete romantic',\n",
    "                  'terimanals': 'terminals', 'womansplaining': 'wo mansplaining',\n",
    "                  'performancelearning': 'performance learning', 'sociomantic': 'sciomantic',\n",
    "                  'batmanvoice': 'batman voice', 'PerformanceTesting': 'Performance Testing',\n",
    "                  'manorialism': 'manorial ism', 'newscommando': 'news commando',\n",
    "                  'Entwicklungsroman': 'Entwicklungs roman',\n",
    "                  'Kunstlerroman': 'Kunstler roman', 'bodhidharman': 'Bodhidharma', 'Howmaney': 'How maney',\n",
    "                  'manufucturing': 'manufacturing', 'remmaning': 'remaining', 'rangeman': 'range man',\n",
    "                  'mythomaniac': 'mythomania', 'katgmandu': 'katmandu',\n",
    "                  'Superowoman': 'Superwoman', 'Rahmanland': 'Rahman land', 'Dormmanu': 'Dormant',\n",
    "                  'Geftman': 'Gentman', 'manufacturig': 'manufacturing', 'bramanistic': 'Brahmanistic',\n",
    "                  'padmanabhanagar': 'padmanabhan agar', 'homoromantic': 'homo romantic', 'femanists': 'feminists',\n",
    "                  'demihuman': 'demi human', 'manrega': 'Manresa', 'Pasmanda': 'Pas manda',\n",
    "                  'manufacctured': 'manufactured', 'remaninder': 'remainder', 'Marimanga': 'Mari manga',\n",
    "                  'Sloatman': 'Sloat man', 'manlet': 'man let', 'perfoemance': 'performance',\n",
    "                  'mangolian': 'mongolian', 'mangekyu': 'mange kyu', 'mansatory': 'mandatory',\n",
    "                  'managemebt': 'management', 'manufctures': 'manufactures', 'Bramanical': 'Brahmanical',\n",
    "                  'manaufacturing': 'manufacturing', 'Lakhsman': 'Lakhs man', 'Sarumans': 'Sarum ans',\n",
    "                  'mangalasutra': 'mangalsutra', 'Germanised': 'German ised',\n",
    "                  'managersworking': 'managers working', 'cammando': 'commando', 'mandrillaris': 'mandrill aris',\n",
    "                  'Emmanvel': 'Emmarvel', 'manupalation': 'manipulation', 'welcomeromanian': 'welcome romanian',\n",
    "                  'humanfemale': 'human female', 'mankirt': 'mankind', 'Haffmann': 'Hoffmann',\n",
    "                  'Panromantic': 'Pan romantic', 'demantion': 'detention', 'Suparwoman': 'Superwoman',\n",
    "                  'parasuramans': 'parasuram ans', 'sulmann': 'Suilmann', 'Shubman': 'Subman',\n",
    "                  'manspread': 'man spread', 'mandingan': 'Mandingan', 'mandalikalu': 'mandalika lu',\n",
    "                  'manufraturer': 'manufacturer', 'Wedgieman': 'Wedgie man', 'manwues': 'manages',\n",
    "                  'humanzees': 'human zees', 'Steymann': 'Stedmann', 'Jobberman': 'Jobber man',\n",
    "                  'maniquins': 'mani quins', 'biromantical': 'bi romantical', 'Rovman': 'Roman',\n",
    "                  'pyromantic': 'pyro mantic', 'Tastaman': 'Rastaman', 'Spoolman': 'Spool man',\n",
    "                  'Subramaniyan': 'Subramani yan', 'abhimana': 'abhiman a', 'manholding': 'man holding',\n",
    "                  'seviceman': 'serviceman', 'womansplained': 'womans plained', 'manniya': 'mania',\n",
    "                  'Bhraman': 'Braman', 'Laakman': 'Layman', 'mansturbate': 'masturbate',\n",
    "                  'Sulamaniya': 'Sulamani ya', 'demanters': 'decanters', 'postmanare': 'postman are',\n",
    "                  'mannualy': 'annual', 'rstman': 'Rotman', 'permanentjobs': 'permanent jobs',\n",
    "                  'Allmang': 'All mang', 'TradeCommander': 'Trade Commander', 'BasedStickman': 'Based Stickman',\n",
    "                  'Deshabhimani': 'Desha bhimani', 'manslamming': 'mans lamming', 'Brahmanwad': 'Brahman wad',\n",
    "                  'fundemantally': 'fundamentally', 'supplemantary': 'supplementary', 'egomanias': 'ego manias',\n",
    "                  'manvantar': 'Manvantara', 'spymania': 'spy mania', 'mangonada': 'mango nada',\n",
    "                  'manthras': 'mantras', 'Humanpark': 'Human park', 'manhuas': 'mahuas',\n",
    "                  'manterrupting': 'interrupting', 'dermatillomaniac': 'dermatillomania',\n",
    "                  'performancies': 'performances', 'manipulant': 'manipulate',\n",
    "                  'painterman': 'painter man', 'mangalik': 'manglik',\n",
    "                  'Neurosemantics': 'Neuro semantics', 'discrimantion': 'discrimination',\n",
    "                  'Womansplaining': 'feminist', 'mongodump': 'mongo dump', 'roadgods': 'road gods',\n",
    "                  'Oligodendraglioma': 'Oligodendroglioma', 'unrightly': 'un rightly', 'Janewright': 'Jane wright',\n",
    "                  ' righten ': ' tighten ', 'brightiest': 'brightest',\n",
    "                  'frighter': 'fighter', 'righteouness': 'righteousness', 'triangleright': 'triangle right',\n",
    "                  'Brightspace': 'Brights pace', 'techinacal': 'technical', 'chinawares': 'china wares',\n",
    "                  'Vancouever': 'Vancouver', 'cheverlet': 'cheveret', 'deverstion': 'diversion',\n",
    "                  'everbodys': 'everybody', 'Dramafever': 'Drama fever', 'reverificaton': 'reverification',\n",
    "                  'canterlever': 'canter lever', 'keywordseverywhere': 'keywords everywhere',\n",
    "                  'neverunlearned': 'never unlearned', 'everyfirst': 'every first',\n",
    "                  'neverhteless': 'nevertheless', 'clevercoyote': 'clever coyote', 'irrevershible': 'irreversible',\n",
    "                  'achievership': 'achievers hip', 'easedeverything': 'eased everything', 'youbever': 'you bever',\n",
    "                  'everperson': 'ever person', 'everydsy': 'everyday', 'whemever': 'whenever',\n",
    "                  'everyonr': 'everyone', 'severiity': 'severity', 'narracist': 'nar racist',\n",
    "                  'racistly': 'racist', 'takesuch': 'take such', 'mystakenly': 'mistakenly',\n",
    "                  'shouldntake': 'shouldnt take', 'Kalitake': 'Kali take', 'msitake': 'mistake',\n",
    "                  'straitstimes': 'straits times', 'timefram': 'timeframe', 'watchtime': 'watch time',\n",
    "                  'timetraveling': 'timet raveling', 'peactime': 'peacetime', 'timetabe': 'timetable',\n",
    "                  'cooktime': 'cook time', 'blocktime': 'block time', 'timesjobs': 'times jobs',\n",
    "                  'timesence': 'times ence', 'Touchtime': 'Touch time', 'timeloop': 'time loop',\n",
    "                  'subcentimeter': 'sub centimeter', 'timejobs': 'time jobs', 'Guardtime': 'Guard time',\n",
    "                  'realtimepolitics': 'realtime politics', 'loadingtimes': 'loading times',\n",
    "                  'timesnow': '24-hour English news channel in India', 'timesspark': 'times spark',\n",
    "                  'timetravelling': 'timet ravelling',\n",
    "                  'antimeter': 'anti meter', 'timewaste': 'time waste', 'cryptochristians': 'crypto christians',\n",
    "                  'Whatcould': 'What could', 'becomesdouble': 'becomes double', 'deathbecomes': 'death becomes',\n",
    "                  'youbecome': 'you become', 'greenseer': 'people who possess the magical ability',\n",
    "                  'rseearch': 'research', 'homeseek': 'home seek',\n",
    "                  'Greenseer': 'people who possess the magical ability', 'starseeders': 'star seeders',\n",
    "                  'seekingmillionaire': 'seeking millionaire', 'see\\u202c': 'see',\n",
    "                  'seeies': 'series', 'CodeAgon': 'Code Agon',\n",
    "                  'royago': 'royal', 'Dragonkeeper': 'Dragon keeper', 'mcgreggor': 'McGregor',\n",
    "                  'catrgory': 'category', 'Dragonknight': 'Dragon knight', 'Antergos': 'Anteros',\n",
    "                  'togofogo': 'togo fogo', 'mongorestore': 'mongo restore', 'gorgops': 'gorgons',\n",
    "                  'withgoogle': 'with google', 'goundar': 'Gondar', 'algorthmic': 'algorithmic',\n",
    "                  'goatnuts': 'goat nuts', 'vitilgo': 'vitiligo', 'polygony': 'poly gony',\n",
    "                  'digonals': 'diagonals', 'Luxemgourg': 'Luxembourg', 'UCSanDiego': 'UC SanDiego',\n",
    "                  'Ringostat': 'Ringo stat', 'takingoff': 'taking off', 'MongoImport': 'Mongo Import',\n",
    "                  'alggorithms': 'algorithms', 'dragonknight': 'dragon knight', 'negotiatior': 'negotiation',\n",
    "                  'gomovies': 'go movies', 'Withgott': 'Without',\n",
    "                  'categoried': 'categories', 'Stocklogos': 'Stock logos', 'Pedogogical': 'Pedological',\n",
    "                  'Wedugo': 'Wedge', 'golddig': 'gold dig', 'goldengroup': 'golden group',\n",
    "                  'merrigo': 'merligo', 'googlemapsAPI': 'googlemaps API', 'goldmedal': 'gold medal',\n",
    "                  'golemized': 'polemized', 'Caligornia': 'California', 'unergonomic': 'un ergonomic',\n",
    "                  'fAegon': 'wagon', 'vertigos': 'vertigo s', 'trigonomatry': 'trigonometry',\n",
    "                  'hypogonadic': 'hypogonadia', 'Mogolia': 'Mongolia', 'governmaent': 'government',\n",
    "                  'ergotherapy': 'ergo therapy', 'Bogosort': 'Bogo sort', 'goalwise': 'goal wise',\n",
    "                  'alogorithms': 'algorithms', 'MercadoPago': 'Mercado Pago', 'rivigo': 'rivi go',\n",
    "                  'govshutdown': 'gov shutdown', 'gorlfriend': 'girlfriend',\n",
    "                  'stategovt': 'state govt', 'Chickengonia': 'Chicken gonia', 'Yegorovich': 'Yegorov ich',\n",
    "                  'regognitions': 'recognitions', 'gorichen': 'Gori Chen Mountain',\n",
    "                  'goegraphies': 'geographies', 'gothras': 'goth ras', 'belagola': 'bela gola',\n",
    "                  'snapragon': 'snapdragon', 'oogonial': 'oogonia l', 'Amigofoods': 'Amigo foods',\n",
    "                  'Sigorn': 'son of Styr', 'algorithimic': 'algorithmic',\n",
    "                  'innermongolians': 'inner mongolians', 'ArangoDB': 'Arango DB', 'zigolo': 'gigolo',\n",
    "                  'regognized': 'recognized', 'Moongot': 'Moong ot', 'goldquest': 'gold quest',\n",
    "                  'catagorey': 'category', 'got7': 'got', 'jetbingo': 'jet bingo', 'Dragonchain': 'Dragon chain',\n",
    "                  'catwgorized': 'categorized', 'gogoro': 'gogo ro', 'Tobagoans': 'Tobago ans',\n",
    "                  'digonal': 'di gonal', 'algoritmic': 'algorismic', 'dragonflag': 'dragon flag',\n",
    "                  'Indigoflight': 'Indigo flight',\n",
    "                  'governening': 'governing', 'ergosphere': 'ergo sphere',\n",
    "                  'pingo5': 'pingo', 'Montogo': 'montego', 'Rivigo': 'technology-enabled logistics company',\n",
    "                  'Jigolo': 'Gigolo', 'phythagoras': 'pythagoras', 'Mangolian': 'Mongolian',\n",
    "                  'forgottenfaster': 'forgotten faster', 'stargold': 'a Hindi movie channel',\n",
    "                  'googolplexain': 'googolplexian', 'corpgov': 'corp gov',\n",
    "                  'govtribe': 'provides real-time federal contracting market intel',\n",
    "                  'dragonglass': 'dragon glass', 'gorakpur': 'Gorakhpur', 'MangoPay': 'Mango Pay',\n",
    "                  'chigoe': 'sub-tropical climates', 'BingoBox': 'an investment company', '走go': 'go',\n",
    "                  'followingorder': 'following order', 'pangolinminer': 'pangolin miner',\n",
    "                  'negosiation': 'negotiation', 'lexigographers': 'lexicographers', 'algorithom': 'algorithm',\n",
    "                  'unforgottable': 'unforgettable', 'wellsfargoemail': 'wellsfargo email',\n",
    "                  'daigonal': 'diagonal', 'Pangoro': 'cantankerous Pokemon', 'negotiotions': 'negotiations',\n",
    "                  'Swissgolden': 'Swiss golden', 'google4': 'google', 'Agoraki': 'Ago raki',\n",
    "                  'Garthago': 'Carthago', 'Stegosauri': 'stegosaurus', 'ergophobia': 'ergo phobia',\n",
    "                  'bigolive': 'big olive', 'bittergoat': 'bitter goat', 'naggots': 'faggots',\n",
    "                  'googology': 'online encyclopedia', 'algortihms': 'algorithms', 'bengolis': 'Bengalis',\n",
    "                  'fingols': 'Finnish people are supposedly descended from Mongols',\n",
    "                  'savethechildren': 'save thechildren',\n",
    "                  'stopings': 'stoping', 'stopsits': 'stop sits', 'stopsigns': 'stop signs',\n",
    "                  'Galastop': 'Galas top', 'pokestops': 'pokes tops', 'forcestop': 'forces top',\n",
    "                  'Hopstop': 'Hops top', 'stoppingexercises': 'stopping exercises', 'coinstop': 'coins top',\n",
    "                  'stoppef': 'stopped', 'workaway': 'work away', 'snazzyway': 'snazzy way',\n",
    "                  'Rewardingways': 'Rewarding ways', 'cloudways': 'cloud ways', 'Cloudways': 'Cloud ways',\n",
    "                  'Brainsway': 'Brains way', 'nesraway': 'nearaway',\n",
    "                  'AlwaysHired': 'Always Hired', 'expessway': 'expressway', 'Syncway': 'Sync way',\n",
    "                  'LeewayHertz': 'Blockchain Company', 'towayrds': 'towards', 'swayable': 'sway able',\n",
    "                  'Telloway': 'Tello way', 'palsmodium': 'plasmodium', 'Gobackmodi': 'Goback modi',\n",
    "                  'comodies': 'corodies', 'islamphobic': 'islam phobic', 'islamphobia': 'islam phobia',\n",
    "                  'citiesbetter': 'cities better', 'betterv3': 'better', 'betterDtu': 'better Dtu',\n",
    "                  'Babadook': 'a horror drama film', 'Ahemadabad': 'Ahmadabad', 'faidabad': 'Faizabad',\n",
    "                  'Amedabad': 'Ahmedabad', 'kabadii': 'kabaddi', 'badmothing': 'badmouthing',\n",
    "                  'badminaton': 'badminton', 'badtameezdil': 'badtameez dil', 'badeffects': 'bad effects',\n",
    "                  '∠bad': 'bad', 'ahemadabad': 'Ahmadabad', 'embaded': 'embased', 'Isdhanbad': 'Is dhanbad',\n",
    "                  'badgermoles': 'enormous, blind mammal', 'allhabad': 'Allahabad', 'ghazibad': 'ghazi bad',\n",
    "                  'htderabad': 'Hyderabad', 'Auragabad': 'Aurangabad', 'ahmedbad': 'Ahmedabad',\n",
    "                  'ahmdabad': 'Ahmadabad', 'alahabad': 'Allahabad',\n",
    "                  'Hydeabad': 'Hyderabad', 'Gyroglove': 'wearable technology', 'foodlovee': 'food lovee',\n",
    "                  'slovenised': 'slovenia', 'handgloves': 'hand gloves', 'lovestep': 'love step',\n",
    "                  'lovejihad': 'love jihad', 'RolloverBox': 'Rollover Box', 'stupidedt': 'stupidest',\n",
    "                  'toostupid': 'too stupid',\n",
    "                  'pakistanisbeautiful': 'pakistanis beautiful', 'ispakistan': 'is pakistan',\n",
    "                  'inpersonations': 'impersonations', 'medicalperson': 'medical person',\n",
    "                  'interpersonation': 'inter personation', 'workperson': 'work person',\n",
    "                  'personlich': 'person lich', 'persoenlich': 'person lich',\n",
    "                  'middleperson': 'middle person', 'personslized': 'personalized',\n",
    "                  'personifaction': 'personification', 'welcomemarriage': 'welcome marriage',\n",
    "                  'come2': 'come to', 'upcomedians': 'up comedians', 'overvcome': 'overcome',\n",
    "                  'talecome': 'tale come', 'cometitive': 'competitive', 'arencome': 'aren come',\n",
    "                  'achecomes': 'ache comes', '」come': 'come',\n",
    "                  'comepleted': 'completed', 'overcomeanxieties': 'overcome anxieties',\n",
    "                  'demigirl': 'demi girl', 'gridgirl': 'female models of the race', 'halfgirlfriend': 'half girlfriend',\n",
    "                  'girlriend': 'girlfriend', 'fitgirl': 'fit girl', 'girlfrnd': 'girlfriend', 'awrong': 'aw rong',\n",
    "                  'northcap': 'north cap', 'productionsupport': 'production support',\n",
    "                  'Designbold': 'Online Photo Editor Design Studio',\n",
    "                  'skyhold': 'sky hold', 'shuoldnt': 'shouldnt', 'anarold': 'Android', 'yaerold': 'year old',\n",
    "                  'soldiders': 'soldiers', 'indrold': 'Android', 'blindfoldedly': 'blindfolded',\n",
    "                  'overcold': 'over cold', 'Goldmont': 'microarchitecture in Intel', 'boldspot': 'bolds pot',\n",
    "                  'Rankholders': 'Rank holders', 'cooldrink': 'cool drink', 'beltholders': 'belt holders',\n",
    "                  'GoldenDict': 'open-source dictionary program', 'softskill': 'softs kill',\n",
    "                  'Cooldige': 'the 30th president of the United States',\n",
    "                  'newkiller': 'new killer', 'skillselect': 'skills elect', 'nonskilled': 'non skilled',\n",
    "                  'killyou': 'kill you', 'Skillport': 'Army e-Learning Program', 'unkilled': 'un killed',\n",
    "                  'killikng': 'killing', 'killograms': 'kilograms',\n",
    "                  'Worldkillers': 'World killers', 'reskilled': 'skilled',\n",
    "                  'killedshivaji': 'killed shivaji', 'honorkillings': 'honor killings',\n",
    "                  'skillclasses': 'skill classes', 'microskills': 'micros kills',\n",
    "                  'Skillselect': 'Skills elect', 'ratkill': 'rat kill',\n",
    "                  'pleasegive': 'please give', 'flashgive': 'flash give',\n",
    "                  'southerntelescope': 'southern telescope', 'westsouth': 'west south',\n",
    "                  'southAfricans': 'south Africans', 'Joboutlooks': 'Job outlooks', 'joboutlook': 'job outlook',\n",
    "                  'Outlook365': 'Outlook 365', 'Neulife': 'Neu life', 'qualifeid': 'qualified',\n",
    "                  'nullifed': 'nullified', 'lifeaffect': 'life affect', 'lifestly': 'lifestyle',\n",
    "                  'aristocracylifestyle': 'aristocracy lifestyle', 'antilife': 'anti life',\n",
    "                  'afterafterlife': 'after afterlife', 'lifestylye': 'lifestyle', 'prelife': 'pre life',\n",
    "                  'lifeute': 'life ute', 'liferature': 'literature',\n",
    "                  'securedlife': 'secured life', 'doublelife': 'double life', 'antireligion': 'anti religion',\n",
    "                  'coreligionist': 'co religionist', 'petrostates': 'petro states', 'otherstates': 'others tates',\n",
    "                  'spacewithout': 'space without', 'withoutyou': 'without you',\n",
    "                  'withoutregistered': 'without registered', 'weightwithout': 'weight without',\n",
    "                  'withoutcheck': 'without check', 'milkwithout': 'milk without',\n",
    "                  'Highschoold': 'High school', 'memoney': 'money', 'moneyof': 'mony of', 'Oneplus': 'OnePlus',\n",
    "                  'OnePlus': 'Chinese smartphone manufacturer', 'Beerus': 'the God of Destruction',\n",
    "                  'takeoverr': 'takeover', 'demonetizedd': 'demonetized', 'polyhouse': 'Polytunnel',\n",
    "                  'Elitmus': 'eLitmus', 'eLitmus': 'Indian company that helps companies in hiring employees',\n",
    "                  'becone': 'become', 'nestaway': 'nest away', 'takeoverrs': 'takeovers', 'Istop': 'I stop',\n",
    "                  'Austira': 'Australia', 'germeny': 'Germany', 'mansoon': 'man soon',\n",
    "                  'worldmax': 'wholesaler of drum parts',\n",
    "                  'ammusement': 'amusement', 'manyare': 'many are', 'supplymentary': 'supply mentary',\n",
    "                  'timesup': 'times up', 'homologus': 'homologous', 'uimovement': 'ui movement', 'spause': 'spouse',\n",
    "                  'aesexual': 'asexual', 'Iovercome': 'I overcome', 'developmeny': 'development',\n",
    "                  'hindusm': 'hinduism', 'sexpat': 'sex tourism', 'sunstop': 'sun stop', 'polyhouses': 'Polytunnel',\n",
    "                  'usefl': 'useful', 'Fundamantal': 'fundamental', 'environmentai': 'environmental',\n",
    "                  'Redmi': 'Xiaomi Mobile', 'Loy Machedo': ' Motivational Speaker ', 'unacademy': 'Unacademy',\n",
    "                  'Boruto': 'Naruto Next Generations', 'Upwork': 'Up work',\n",
    "                  'Unacademy': 'educational technology company',\n",
    "                  'HackerRank': 'Hacker Rank', 'upwork': 'up work', 'Chromecast': 'Chrome cast',\n",
    "                  'microservices': 'micro services', 'Undertale': 'video game', 'undergraduation': 'under graduation',\n",
    "                  'chapterwise': 'chapter wise', 'twinflame': 'twin flame', 'Hotstar': 'Hot star',\n",
    "                  'blockchains': 'blockchain',\n",
    "                  'darkweb': 'dark web', 'Microservices': 'Micro services', 'Nearbuy': 'Nearby',\n",
    "                  ' Padmaavat ': ' Padmavati ', ' padmavat ': ' Padmavati ', ' Padmaavati ': ' Padmavati ',\n",
    "                  ' Padmavat ': ' Padmavati ', ' internshala ': ' internship and online training platform in India ',\n",
    "                  'dream11': ' fantasy sports platform in India ', 'conciousnesss': 'consciousnesses',\n",
    "                  'Dream11': ' fantasy sports platform in India ', 'cointry': 'country', ' coinvest ': ' invest ',\n",
    "                  '23 andme': 'privately held personal genomics and biotechnology company in California',\n",
    "                  'Trumpism': 'philosophy and politics espoused by Donald Trump',\n",
    "                  'Trumpian': 'viewpoints of President Donald Trump', 'Trumpists': 'admirer of Donald Trump',\n",
    "                  'coincidents': 'coincidence', 'coinsized': 'coin sized', 'coincedences': 'coincidences',\n",
    "                  'cointries': 'countries', 'coinsidered': 'considered', 'coinfirm': 'confirm',\n",
    "                  'humilates':'humiliates', 'vicevice':'vice vice', 'politicak':'political', 'Sumaterans':'Sumatrans',\n",
    "                  'Kamikazis':'Kamikazes', 'unmoraled':'unmoral', 'eduacated':'educated', 'moraled':'morale',\n",
    "                  'Amharc':'Amarc', 'where Burkhas':'wear Burqas', 'Baloochistan':'Balochistan', 'durgahs':'durgans',\n",
    "                  'illigitmate':'illegitimate', 'hillum':'helium','treatens':'threatens','mutiliating':'mutilating',\n",
    "                  'speakingly':'speaking', 'pretex':'pretext', 'menstruateion':'menstruation', \n",
    "                  'genocidizing':'genociding', 'maratis':'Maratism','Parkistinian':'Pakistani', 'SPEICIAL':'SPECIAL',\n",
    "                  'REFERNECE':'REFERENCE', 'provocates':'provokes', 'FAMINAZIS':'FEMINAZIS', 'repugicans':'republicans',\n",
    "                  'tonogenesis':'tone', 'winor':'win', 'redicules':'ridiculous', 'Beluchistan':'Balochistan', \n",
    "                  'volime':'volume', 'namaj':'namaz', 'CONgressi':'Congress', 'Ashifa':'Asifa', 'queffing':'queefing',\n",
    "                  'montheistic':'nontheistic', 'Rajsthan':'Rajasthan', 'Rajsthanis':'Rajasthanis', 'specrum':'spectrum',\n",
    "                  'brophytes':'bryophytes', 'adhaar':'Adhara', 'slogun':'slogan', 'harassd':'harassed',\n",
    "                  'transness':'trans gender', 'Insdians':'Indians', 'Trampaphobia':'Trump aphobia', 'attrected':'attracted',\n",
    "                  'Yahtzees':'Yahtzee', 'thiests':'atheists', 'thrir':'their', 'extraterestrial':'extraterrestrial',\n",
    "                  'silghtest':'slightest', 'primarty':'primary','brlieve':'believe', 'fondels':'fondles',\n",
    "                  'loundly':'loudly', 'bootythongs':'booty thongs', 'understamding':'understanding', 'degenarate':'degenerate',\n",
    "                  'narsistic':'narcistic', 'innerskin':'inner skin','spectulated':'speculated', 'hippocratical':'Hippocratical',\n",
    "                  'itstead':'instead', 'parralels':'parallels', 'sloppers':'slippers'\n",
    "                  }\n",
    "contraction_mapping = {\n",
    "    \"Trump's\" : 'trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n",
    "    'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not',\n",
    "    'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n",
    "    'can;t': 'cannot','can;t;ve': 'cannot have',\n",
    "    'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n",
    "    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n",
    "    'couldn;t;ve': 'could not have','couldn´t': 'could not',\n",
    "    'couldn´t´ve': 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could´ve': 'could have',\n",
    "    'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n",
    "    'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n",
    "    'doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n",
    "    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n",
    "    'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not','hasn’t': 'has not',\n",
    "    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not','haven’t': 'have not',\"he'd\": 'he would',\n",
    "    \"he'd've\": 'he would have',\"he'll\": 'he will',\n",
    "    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n",
    "    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have','he´ll': 'he will',\n",
    "    'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n",
    "    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n",
    "    'how;s': 'how is','how´d': 'how did','how´ll': 'how will','how´s': 'how is','how’d': 'how did','how’ll': 'how will',\n",
    "    'how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n",
    "    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n",
    "    'isn,t': 'is not','isn;t': 'is not','isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n",
    "    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it´d': 'it would','it´ll': 'it will','it´s': 'it is',\n",
    "    'it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n",
    "    'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n",
    "    'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us',\n",
    "    'let’s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n",
    "    'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn´t': 'might not',\n",
    "    'mightn’t': 'might not','might´ve': 'might have','might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n",
    "    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not','must´ve': 'must have',\n",
    "    'must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n",
    "    'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n",
    "    'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not','sha´n´t': 'shall not','sha’n’t': 'shall not',\n",
    "    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n",
    "    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she´d': 'she would','she´ll': 'she will',\n",
    "    'she´s': 'she is','she’d': 'she would','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n",
    "    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have',\n",
    "    'should’ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n",
    "    'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\"there'd\": 'there had',\n",
    "    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n",
    "    'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n",
    "    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n",
    "    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n",
    "    'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are','they´ve': 'they have','they’d': 'they would','they’ll': 'they will',\n",
    "    'they’re': 'they are','they’ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not',\n",
    "    'wasn’t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n",
    "    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n",
    "    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not','we´d': 'we would','we´ll': 'we will',\n",
    "    'we´re': 'we are','we´ve': 'we have','we’d': 'we would','we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n",
    "    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n",
    "    'what;s': 'what is','what;ve': 'what have','what´ll': 'what will',\n",
    "    'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will','what’re': 'what are','what’s': 'what is',\n",
    "    'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n",
    "    'where;s': 'where is','where´d': 'where did','where´s': 'where is','where’d': 'where did','where’s': 'where is',\n",
    "    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n",
    "    'who´ll': 'who will','who´s': 'who is','who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n",
    "    'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn´t': 'would not',\n",
    "    'wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n",
    "    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n",
    "    'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would','you’ll': 'you will','you’re': 'you are',\n",
    "    '´cause': 'because','’cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n",
    "    \"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n",
    "    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n",
    "    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n",
    "    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n",
    "    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I',\n",
    "    'ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at','…and':'and','civilbeat':'civil beat',\\\n",
    "    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\\\n",
    "    'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','Sᴛᴀʀᴛ':'start','gubmit':'submit','CO₂':'carbon dioxide','ғɪʀsᴛ':'first',\\\n",
    "    'ᴇɴᴅ':'end','ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra',\\\n",
    "    'Gʀᴇᴀᴛ':'great','sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\\\n",
    "    'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','Mᴀᴋᴇ':'make',\\\n",
    "    'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','financialpost':'financial post', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'Maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', 'ɴᴇᴇᴅ':' need ',\n",
    "    'ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'Sᴛaʀᴛ':' start', 'SHOPO':'shop',\n",
    "    }\n",
    "mispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','Gʀᴇat':'great','ʙᴏᴛtoᴍ':'bottom',\n",
    "                'washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver',\n",
    "                'SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yᴏᴜ':'you','Trumpsters':'trump','Trumpian':'trump',\n",
    "                'bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit',\n",
    "                'utilitas':'utilities','ᴀ':'a', '😉':'wink','😂':'joy','😀':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable',\n",
    "                'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine',\n",
    "                'fentayal':'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers',\n",
    "                'negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments',\n",
    "                'envirnmetalists': 'environmentalists',}\n",
    "\n",
    "special_punc_mappings = {\"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '”': '\"', \"″\": '\"', '“': '\"', '•': '.', '−': '-',\n",
    "                         \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','،':'','„':'',\n",
    "                         '…': ' ... ', '\\ufeff': ''}\n",
    "\n",
    "blacklist = [\"lil\",\"ft\",\"got\",\"get\",\"mv\",\"first\",\"vs\",\"highlights\",\"channel\",\"new\",\"official\",\"best\",\"check\",\"latest\",\"also\",\"thanks\",\"join\",\"»\",\"new\",\"video\",\"content\",\"thanks\",\"»\",\"tiktok\",\"s\",\"’\",\"–\",'“',\"im\",'”',\"v\",\"—\",\"w\",\"g\",\"‘\",\"u\",\"►\",\"m\",\"i\",\"t\",\"de\",\"us\",\"instagram\",\"twitter\",\"videos\",\"subscribe\",\"go\",\"la\",\"every\",\"facebook\",\"watch\",\"youtube\",\"follow\",\"like\"]\n",
    "\n",
    "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "\n",
    "rare_words_mapping = {' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA','u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA',\n",
    "                      ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',' U.s ': 'USA', ' u.S ': ' USA ', 'fu.k': 'fuck', 'U.K.': 'UK', ' u.k ': ' UK ',\n",
    "                      ' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n",
    "                      'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n",
    "                      'Ra apist': 'Rapist', '2fifth': 'twenty fifth', '2third': 'twenty third','2nineth': 'twenty nineth', '2fourth': 'twenty fourth','#metoo': 'MeToo',\n",
    "                      'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n",
    "                      'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin','fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n",
    "                      'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n",
    "                      'culturr': 'culture','weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n",
    "                      'indans': 'indians', 'mastuburate': 'masturbate', 'f**k': 'fuck', 'F**k': 'fuck', 'F**K': 'fuck',\n",
    "                      ' u r ': ' you are ', ' u ': ' you ', '操你妈': 'fuck your mother', 'e.g.': 'for example',\n",
    "                      'i.e.': 'in other words', '...': '.', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n",
    "                      'f***': 'fuck', 'f**': 'fuc', 'F***': 'fuck', 'F**': 'fuc','a****': 'assho', 'a**': 'ass', 'h***': 'hole', 'A****': 'assho', 'A**': 'ass', 'H***': 'hole',\n",
    "                      's***': 'shit', 's**': 'shi', 'S***': 'shit', 'S**': 'shi', 'Sh**': 'shit',\n",
    "                      'p****': 'pussy', 'p*ssy': 'pussy', 'P****': 'pussy','p***': 'porn', 'p*rn': 'porn', 'P***': 'porn',\n",
    "                      'st*up*id': 'stupid','d***': 'dick', 'di**': 'dick', 'h*ck': 'hack',\n",
    "                      'b*tch': 'bitch', 'bi*ch': 'bitch', 'bit*h': 'bitch', 'bitc*': 'bitch', 'b****': 'bitch',\n",
    "                      'b***': 'bitc', 'b**': 'bit', 'b*ll': 'bull'\n",
    "                      }\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    \"\"\"\n",
    "    remove extra spaces and ending space if any\n",
    "    \"\"\"\n",
    "    for space in spaces:\n",
    "        text = text.replace(space, ' ')\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "def clean_special_punctuations(text):\n",
    "    for punc in special_punc_mappings:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, special_punc_mappings[punc])\n",
    "    # remove_diacritics don´t' ->  'don t'\n",
    "    #text = remove_diacritics(text)\n",
    "    return text\n",
    "def clean_number(text):\n",
    "    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n",
    "    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
    "    text = re.sub(r'(\\d+)(e)(\\d+)','\\g<1> \\g<3>', text)\n",
    "    \n",
    "    return text\n",
    "def pre_clean_rare_words(text):\n",
    "    for rare_word in rare_words_mapping:\n",
    "        if rare_word in text:\n",
    "            text = text.replace(rare_word, rare_words_mapping[rare_word])\n",
    "\n",
    "    return text\n",
    "def clean_misspell(text):\n",
    "    for bad_word in mispell_dict:\n",
    "        if bad_word in text:\n",
    "            text = text.replace(bad_word, mispell_dict[bad_word])\n",
    "    return text\n",
    "\n",
    "import string\n",
    "regular_punct = list(string.punctuation)\n",
    "all_punct = list(set(regular_punct + extra_punct))\n",
    "# do not spacing - and .\n",
    "all_punct.remove('-')\n",
    "all_punct.remove('.')\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text\n",
    "def clean_repeat_words(text):\n",
    "    \n",
    "    text = re.sub(r\"\\b(I|i)(I|i)+ng\\b\", \"ing\", text) #this one is causing few issues(fixed via monkey patching in other dicts for now), need to check it..\n",
    "    text = re.sub(r\"(-+|\\.+)\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_bad_case_words(text):\n",
    "    for bad_word in bad_case_words:\n",
    "        if bad_word in text:\n",
    "            text = text.replace(bad_word, bad_case_words[bad_word])\n",
    "    return text\n",
    "\n",
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def tokenise(text):\n",
    "    words = word_tokenize(text) \n",
    "    return words\n",
    "\n",
    "def retokenise(word_list):\n",
    "    sentence = \"\"\n",
    "    for word in word_list:\n",
    "        sentence = sentence + \" \" + word\n",
    "    return sentence\n",
    "\n",
    "# def newFunc(text):\n",
    "#     list=[]\n",
    "#     for i in text:\n",
    "#         if i not in blacklist:\n",
    "#             list.append(i)\n",
    "#     return list\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+',\"\", text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('\\'','', text)\n",
    "    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n",
    "    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
    "    text = re.sub(r'(\\d+)(e)(\\d+)','\\g<1> \\g<3>', text)\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" america \", text)\n",
    "    text = re.sub(r\" USA \", \" america \", text)\n",
    "    text = re.sub(r\" u s \", \" america \", text)\n",
    "    text = re.sub(r\" uk \", \" england \", text)\n",
    "    text = re.sub(r\" UK \", \" england \", text)\n",
    "    text = re.sub(r\"india\", \"india\", text)\n",
    "    text = re.sub(r\"switzerland\", \"switzerland\", text)\n",
    "    text = re.sub(r\"china\", \"china\", text)\n",
    "    text = re.sub(r\"chinese\", \"chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"america\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    text = re.sub(r\" th \", \" the \", text)\n",
    "    # text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n",
    "    # text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n",
    "    # text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n",
    "    # text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n",
    "    # text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n",
    "    # text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n",
    "    # text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n",
    "    # text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n",
    "    # text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n",
    "    # text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n",
    "    # text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n",
    "    text = text.replace(\"What sApp\", ' WhatsApp ')\n",
    "    text = remove_space(text)    \n",
    "    text = re.sub(r\"minut\", \"Banglominutere\", text)\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('\\'','', text)\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    \n",
    "    \n",
    "    text = str(text).replace(' s ','').replace('…', ' ').replace('—','-').replace('•°•°•','') #should be broken down to regexs (lazy to do it haha)\n",
    "    for punct in \"/-'\":\n",
    "        if punct in text:\n",
    "            text = text.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        if punct in text:\n",
    "            text = text.replace(punct, f' {punct} ')\n",
    "    for punct in '?!-,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~–—✰«»§✈➤›☭✔½☺éïà😏🤣😢😁🙄😃😄😊😜😎😆💙👍🤔😅😡▀▄·―═►♥▬' + '“”’': \n",
    "        #if we add . here then all the WEBPAGE LINKS WILL VANISH WE DON'T WANT THAT\n",
    "        if punct in text: #can be used a FE for emojis but here we are just removing them..\n",
    "            text = text.replace(punct, '')\n",
    "    for punct in '.•': #hence here it is\n",
    "        if punct in text:\n",
    "            text = text.replace(punct, f' ')\n",
    "        \n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f\\xad]', '', text)\n",
    "    text = re.sub(r'(\\d+)(e)(\\d+)',r'\\g<1> \\g<3>', text) #is a dup from above cell...\n",
    "    text = re.sub(r\"(-+|\\.+)\\s?\", \"  \", text)\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "    text = re.sub(r'ᴵ+', '', text)\n",
    "    \n",
    "    # text = re.sub(r'(can|by|been|and|are|for|it|TV|already|justhow|some|had|is|will|would|should|shall|must|can|his|here|there|them|these|their|has|have|the|be|that|not|was|he|just|they|who)(how)', '\\g<1> \\g<2>', text) \n",
    "    return text\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61384/61384 [00:13<00:00, 4568.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>view_count</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3C66w5Z0ixs</td>\n",
       "      <td>i asked her to be my girlfriend</td>\n",
       "      <td>22</td>\n",
       "      <td>1514614</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M9Pmf9AB4Mo</td>\n",
       "      <td>apex legends stories from the outlands the end...</td>\n",
       "      <td>20</td>\n",
       "      <td>2381688</td>\n",
       "      <td>Gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J78aPJ3VyNs</td>\n",
       "      <td>i left youtube for a month and this is what ha...</td>\n",
       "      <td>24</td>\n",
       "      <td>2038853</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kXLn3HkpjaA</td>\n",
       "      <td>xxl freshman class revealed official announcement</td>\n",
       "      <td>10</td>\n",
       "      <td>496771</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VIUo6yapDbc</td>\n",
       "      <td>ultimate diy home movie theater for the labran...</td>\n",
       "      <td>26</td>\n",
       "      <td>1123889</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578173</th>\n",
       "      <td>FIGP0KFwRbQ</td>\n",
       "      <td>car vs outback daniel ricciardos great aussie ...</td>\n",
       "      <td>17</td>\n",
       "      <td>1180037</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578174</th>\n",
       "      <td>TQklmcsCRUc</td>\n",
       "      <td>operating system is out whats new</td>\n",
       "      <td>28</td>\n",
       "      <td>222059</td>\n",
       "      <td>Science &amp; Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578175</th>\n",
       "      <td>bH7WapVQ8po</td>\n",
       "      <td>i cruised through rough seas to find the north...</td>\n",
       "      <td>24</td>\n",
       "      <td>100237</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578183</th>\n",
       "      <td>aJIAA7Xt24U</td>\n",
       "      <td>proteas vs west indies i highlights march supe...</td>\n",
       "      <td>17</td>\n",
       "      <td>818208</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578188</th>\n",
       "      <td>0YqsjNsmkr4</td>\n",
       "      <td>the new rolex releases in switzerland</td>\n",
       "      <td>26</td>\n",
       "      <td>203488</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61384 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           video_id                                              title  \\\n",
       "0       3C66w5Z0ixs                    i asked her to be my girlfriend   \n",
       "1       M9Pmf9AB4Mo  apex legends stories from the outlands the end...   \n",
       "2       J78aPJ3VyNs  i left youtube for a month and this is what ha...   \n",
       "3       kXLn3HkpjaA  xxl freshman class revealed official announcement   \n",
       "4       VIUo6yapDbc  ultimate diy home movie theater for the labran...   \n",
       "...             ...                                                ...   \n",
       "578173  FIGP0KFwRbQ  car vs outback daniel ricciardos great aussie ...   \n",
       "578174  TQklmcsCRUc                  operating system is out whats new   \n",
       "578175  bH7WapVQ8po  i cruised through rough seas to find the north...   \n",
       "578183  aJIAA7Xt24U  proteas vs west indies i highlights march supe...   \n",
       "578188  0YqsjNsmkr4              the new rolex releases in switzerland   \n",
       "\n",
       "        categoryId  view_count         category_name  \n",
       "0               22     1514614        People & Blogs  \n",
       "1               20     2381688                Gaming  \n",
       "2               24     2038853         Entertainment  \n",
       "3               10      496771                 Music  \n",
       "4               26     1123889         Howto & Style  \n",
       "...            ...         ...                   ...  \n",
       "578173          17     1180037                Sports  \n",
       "578174          28      222059  Science & Technology  \n",
       "578175          24      100237         Entertainment  \n",
       "578183          17      818208                Sports  \n",
       "578188          26      203488         Howto & Style  \n",
       "\n",
       "[61384 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the part that we call all the functions that we write for cleaning process.\n",
    "# Let's start and see what the difference will be.\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_space(text)\n",
    "    text = clean_special_punctuations(text)\n",
    "    text = clean_number(text)\n",
    "    # text = pre_clean_rare_words(text)\n",
    "    # text = clean_misspell(text)\n",
    "    # text = spacing_punctuation(text)\n",
    "    # text = clean_bad_case_words(text)\n",
    "    # text = clean_repeat_words(text)\n",
    "    text = remove_space(text)\n",
    "    text = clean_text(text)\n",
    "    return text\n",
    "\n",
    "main_data['title'] = main_data['title'].progress_apply(lambda x:preprocess(x))\n",
    "# main_data['title'] = main_data['title'].progress_apply(lambda x : tokenise(x)).progress_apply(lambda x: newFunc(x)).progress_apply(lambda x: retokenise(x))\n",
    "\n",
    "gc.collect()\n",
    "main_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Split out to several dataframes for several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Education\n",
      "Total input sequences:  8292\n",
      "Education Max sequence length is: \n",
      "16\n",
      "Epoch 1/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 7.4904 - accuracy: 0.0385\n",
      "Epoch 1: loss improved from inf to 7.49036, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 45s 19ms/step - loss: 7.4904 - accuracy: 0.0385 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 7.0525 - accuracy: 0.0410\n",
      "Epoch 2: loss improved from 7.49036 to 7.05145, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 49s 24ms/step - loss: 7.0514 - accuracy: 0.0410 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 6.8656 - accuracy: 0.0420\n",
      "Epoch 3: loss improved from 7.05145 to 6.86575, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 43s 21ms/step - loss: 6.8658 - accuracy: 0.0420 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 6.7163 - accuracy: 0.0415\n",
      "Epoch 4: loss improved from 6.86575 to 6.71556, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 39s 19ms/step - loss: 6.7156 - accuracy: 0.0415 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2070/2073 [============================>.] - ETA: 0s - loss: 6.5611 - accuracy: 0.0472\n",
      "Epoch 5: loss improved from 6.71556 to 6.56046, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 40s 19ms/step - loss: 6.5605 - accuracy: 0.0472 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 6.4015 - accuracy: 0.0565\n",
      "Epoch 6: loss improved from 6.56046 to 6.40243, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 6.4024 - accuracy: 0.0564 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 6.2520 - accuracy: 0.0645\n",
      "Epoch 7: loss improved from 6.40243 to 6.25272, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 34s 17ms/step - loss: 6.2527 - accuracy: 0.0644 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 6.0572 - accuracy: 0.0707\n",
      "Epoch 8: loss improved from 6.25272 to 6.05655, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 42s 20ms/step - loss: 6.0566 - accuracy: 0.0708 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 5.8936 - accuracy: 0.0755\n",
      "Epoch 9: loss improved from 6.05655 to 5.89345, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 40s 19ms/step - loss: 5.8935 - accuracy: 0.0755 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 5.7492 - accuracy: 0.0802\n",
      "Epoch 10: loss improved from 5.89345 to 5.74997, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 37s 18ms/step - loss: 5.7500 - accuracy: 0.0801 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 5.6223 - accuracy: 0.0890\n",
      "Epoch 11: loss improved from 5.74997 to 5.62249, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 39s 19ms/step - loss: 5.6225 - accuracy: 0.0890 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 5.4911 - accuracy: 0.0922\n",
      "Epoch 12: loss improved from 5.62249 to 5.49111, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 40s 19ms/step - loss: 5.4911 - accuracy: 0.0923 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 5.3559 - accuracy: 0.0977\n",
      "Epoch 13: loss improved from 5.49111 to 5.35519, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 38s 18ms/step - loss: 5.3552 - accuracy: 0.0978 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 5.2115 - accuracy: 0.0999\n",
      "Epoch 14: loss improved from 5.35519 to 5.21227, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 38s 18ms/step - loss: 5.2123 - accuracy: 0.0999 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 5.0506 - accuracy: 0.1048\n",
      "Epoch 15: loss improved from 5.21227 to 5.05064, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 39s 19ms/step - loss: 5.0506 - accuracy: 0.1048 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 4.8901 - accuracy: 0.1106\n",
      "Epoch 16: loss improved from 5.05064 to 4.89021, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 39s 19ms/step - loss: 4.8902 - accuracy: 0.1107 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 4.7271 - accuracy: 0.1187\n",
      "Epoch 17: loss improved from 4.89021 to 4.72715, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 39s 19ms/step - loss: 4.7271 - accuracy: 0.1187 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 4.5732 - accuracy: 0.1272\n",
      "Epoch 18: loss improved from 4.72715 to 4.57282, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 39s 19ms/step - loss: 4.5728 - accuracy: 0.1272 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "2070/2073 [============================>.] - ETA: 0s - loss: 4.4079 - accuracy: 0.1329\n",
      "Epoch 19: loss improved from 4.57282 to 4.40799, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 37s 18ms/step - loss: 4.4080 - accuracy: 0.1329 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 4.2487 - accuracy: 0.1439\n",
      "Epoch 20: loss improved from 4.40799 to 4.24917, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 37s 18ms/step - loss: 4.2492 - accuracy: 0.1438 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 4.0893 - accuracy: 0.1517\n",
      "Epoch 21: loss improved from 4.24917 to 4.09008, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 37s 18ms/step - loss: 4.0901 - accuracy: 0.1516 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 3.9461 - accuracy: 0.1649\n",
      "Epoch 22: loss improved from 4.09008 to 3.94582, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 35s 17ms/step - loss: 3.9458 - accuracy: 0.1650 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 3.7800 - accuracy: 0.1813\n",
      "Epoch 23: loss improved from 3.94582 to 3.77996, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 35s 17ms/step - loss: 3.7800 - accuracy: 0.1813 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 3.6544 - accuracy: 0.1952\n",
      "Epoch 24: loss improved from 3.77996 to 3.65452, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 35s 17ms/step - loss: 3.6545 - accuracy: 0.1950 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "2070/2073 [============================>.] - ETA: 0s - loss: 3.5205 - accuracy: 0.2063\n",
      "Epoch 25: loss improved from 3.65452 to 3.52104, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 3.5210 - accuracy: 0.2061 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "2070/2073 [============================>.] - ETA: 0s - loss: 3.4131 - accuracy: 0.2163\n",
      "Epoch 26: loss improved from 3.52104 to 3.41347, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 3.4135 - accuracy: 0.2164 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 3.3020 - accuracy: 0.2279\n",
      "Epoch 27: loss improved from 3.41347 to 3.30187, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 35s 17ms/step - loss: 3.3019 - accuracy: 0.2281 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "2070/2073 [============================>.] - ETA: 0s - loss: 3.2065 - accuracy: 0.2415\n",
      "Epoch 28: loss improved from 3.30187 to 3.20679, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 37s 18ms/step - loss: 3.2068 - accuracy: 0.2414 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 3.1036 - accuracy: 0.2536\n",
      "Epoch 29: loss improved from 3.20679 to 3.10363, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 35s 17ms/step - loss: 3.1036 - accuracy: 0.2536 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 3.0122 - accuracy: 0.2688\n",
      "Epoch 30: loss improved from 3.10363 to 3.01225, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 3.0122 - accuracy: 0.2688 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 2.9279 - accuracy: 0.2795\n",
      "Epoch 31: loss improved from 3.01225 to 2.92788, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 35s 17ms/step - loss: 2.9279 - accuracy: 0.2795 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 2.8343 - accuracy: 0.2932\n",
      "Epoch 32: loss improved from 2.92788 to 2.83484, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 37s 18ms/step - loss: 2.8348 - accuracy: 0.2931 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 2.7586 - accuracy: 0.3085\n",
      "Epoch 33: loss improved from 2.83484 to 2.75858, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 35s 17ms/step - loss: 2.7586 - accuracy: 0.3085 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 2.6852 - accuracy: 0.3244\n",
      "Epoch 34: loss improved from 2.75858 to 2.68550, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 34s 17ms/step - loss: 2.6855 - accuracy: 0.3240 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 2.6086 - accuracy: 0.3378\n",
      "Epoch 35: loss improved from 2.68550 to 2.60864, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 38s 19ms/step - loss: 2.6086 - accuracy: 0.3378 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 2.5274 - accuracy: 0.3582\n",
      "Epoch 36: loss improved from 2.60864 to 2.52739, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 2.5274 - accuracy: 0.3582 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 2.4597 - accuracy: 0.3683\n",
      "Epoch 37: loss improved from 2.52739 to 2.45971, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 2.4597 - accuracy: 0.3683 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 2.3747 - accuracy: 0.3866\n",
      "Epoch 38: loss improved from 2.45971 to 2.37467, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 2.3747 - accuracy: 0.3866 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.4012\n",
      "Epoch 39: loss improved from 2.37467 to 2.30319, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 35s 17ms/step - loss: 2.3032 - accuracy: 0.4011 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "2072/2073 [============================>.] - ETA: 0s - loss: 2.2295 - accuracy: 0.4188\n",
      "Epoch 40: loss improved from 2.30319 to 2.22985, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 2.2299 - accuracy: 0.4187 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 2.1653 - accuracy: 0.4301\n",
      "Epoch 41: loss improved from 2.22985 to 2.16508, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 2.1651 - accuracy: 0.4301 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 2.1033 - accuracy: 0.4409\n",
      "Epoch 42: loss improved from 2.16508 to 2.10333, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 38s 18ms/step - loss: 2.1033 - accuracy: 0.4409 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 2.0389 - accuracy: 0.4565\n",
      "Epoch 43: loss improved from 2.10333 to 2.03892, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 2.0389 - accuracy: 0.4567 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 1.9708 - accuracy: 0.4736\n",
      "Epoch 44: loss improved from 2.03892 to 1.97105, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 37s 18ms/step - loss: 1.9711 - accuracy: 0.4736 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 1.9081 - accuracy: 0.4947\n",
      "Epoch 45: loss improved from 1.97105 to 1.90808, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 38s 18ms/step - loss: 1.9081 - accuracy: 0.4947 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "2071/2073 [============================>.] - ETA: 0s - loss: 1.8492 - accuracy: 0.5000\n",
      "Epoch 46: loss improved from 1.90808 to 1.84968, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 37s 18ms/step - loss: 1.8497 - accuracy: 0.4998 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "2070/2073 [============================>.] - ETA: 0s - loss: 1.7805 - accuracy: 0.5176\n",
      "Epoch 47: loss improved from 1.84968 to 1.78097, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 1.7810 - accuracy: 0.5175 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 1.7502 - accuracy: 0.5300\n",
      "Epoch 48: loss improved from 1.78097 to 1.75024, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 38s 18ms/step - loss: 1.7502 - accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 1.6731 - accuracy: 0.5444\n",
      "Epoch 49: loss improved from 1.75024 to 1.67308, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 1.6731 - accuracy: 0.5444 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "2073/2073 [==============================] - ETA: 0s - loss: 1.6195 - accuracy: 0.5526\n",
      "Epoch 50: loss improved from 1.67308 to 1.61955, saving model to Educationnextword1.h5\n",
      "2073/2073 [==============================] - 36s 17ms/step - loss: 1.6195 - accuracy: 0.5526 - lr: 0.0010\n",
      "On Autos & Vehicles\n",
      "Total input sequences:  10343\n",
      "Autos & Vehicles Max sequence length is: \n",
      "19\n",
      "Epoch 1/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 6.9840 - accuracy: 0.0380\n",
      "Epoch 1: loss improved from inf to 6.98391, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 65s 23ms/step - loss: 6.9839 - accuracy: 0.0380 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 6.6299 - accuracy: 0.0439\n",
      "Epoch 2: loss improved from 6.98391 to 6.62991, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 56s 22ms/step - loss: 6.6299 - accuracy: 0.0439 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 6.4746 - accuracy: 0.0474\n",
      "Epoch 3: loss improved from 6.62991 to 6.47463, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 6.4746 - accuracy: 0.0474 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 6.3406 - accuracy: 0.0482\n",
      "Epoch 4: loss improved from 6.47463 to 6.34059, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 56s 22ms/step - loss: 6.3406 - accuracy: 0.0482 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 6.2093 - accuracy: 0.0524\n",
      "Epoch 5: loss improved from 6.34059 to 6.20966, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 6.2097 - accuracy: 0.0524 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 6.0621 - accuracy: 0.0585\n",
      "Epoch 6: loss improved from 6.20966 to 6.06254, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 56s 22ms/step - loss: 6.0625 - accuracy: 0.0585 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 5.9070 - accuracy: 0.0647\n",
      "Epoch 7: loss improved from 6.06254 to 5.90746, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 5.9075 - accuracy: 0.0647 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 5.7503 - accuracy: 0.0738\n",
      "Epoch 8: loss improved from 5.90746 to 5.75026, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 56s 22ms/step - loss: 5.7503 - accuracy: 0.0738 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 5.6125 - accuracy: 0.0787\n",
      "Epoch 9: loss improved from 5.75026 to 5.61167, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 5.6117 - accuracy: 0.0788 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 5.4695 - accuracy: 0.0838\n",
      "Epoch 10: loss improved from 5.61167 to 5.46923, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 5.4692 - accuracy: 0.0837 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 5.3272 - accuracy: 0.0889\n",
      "Epoch 11: loss improved from 5.46923 to 5.32722, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 5.3272 - accuracy: 0.0889 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 5.1869 - accuracy: 0.0937\n",
      "Epoch 12: loss improved from 5.32722 to 5.18694, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 5.1869 - accuracy: 0.0937 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 5.0385 - accuracy: 0.1025\n",
      "Epoch 13: loss improved from 5.18694 to 5.03842, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 56s 22ms/step - loss: 5.0384 - accuracy: 0.1025 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 4.8811 - accuracy: 0.1072\n",
      "Epoch 14: loss improved from 5.03842 to 4.88108, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 4.8811 - accuracy: 0.1072 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 4.7297 - accuracy: 0.1176\n",
      "Epoch 15: loss improved from 4.88108 to 4.72969, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 4.7297 - accuracy: 0.1176 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 4.5890 - accuracy: 0.1296\n",
      "Epoch 16: loss improved from 4.72969 to 4.58905, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 23ms/step - loss: 4.5891 - accuracy: 0.1296 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 4.4545 - accuracy: 0.1366\n",
      "Epoch 17: loss improved from 4.58905 to 4.45439, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 4.4544 - accuracy: 0.1366 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 4.3186 - accuracy: 0.1462\n",
      "Epoch 18: loss improved from 4.45439 to 4.31837, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 4.3184 - accuracy: 0.1463 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 4.1870 - accuracy: 0.1559\n",
      "Epoch 19: loss improved from 4.31837 to 4.18703, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 4.1870 - accuracy: 0.1559 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 4.0688 - accuracy: 0.1639\n",
      "Epoch 20: loss improved from 4.18703 to 4.06796, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 4.0680 - accuracy: 0.1642 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 3.9450 - accuracy: 0.1792\n",
      "Epoch 21: loss improved from 4.06796 to 3.94474, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 3.9447 - accuracy: 0.1793 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 3.8279 - accuracy: 0.1857\n",
      "Epoch 22: loss improved from 3.94474 to 3.82773, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 3.8277 - accuracy: 0.1856 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 3.7104 - accuracy: 0.1980\n",
      "Epoch 23: loss improved from 3.82773 to 3.71039, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 3.7104 - accuracy: 0.1980 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 3.5995 - accuracy: 0.2123\n",
      "Epoch 24: loss improved from 3.71039 to 3.60054, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 3.6005 - accuracy: 0.2121 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 3.4884 - accuracy: 0.2211\n",
      "Epoch 25: loss improved from 3.60054 to 3.48841, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 3.4884 - accuracy: 0.2210 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 3.3703 - accuracy: 0.2376\n",
      "Epoch 26: loss improved from 3.48841 to 3.36966, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 3.3697 - accuracy: 0.2376 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 3.2641 - accuracy: 0.2516\n",
      "Epoch 27: loss improved from 3.36966 to 3.26369, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 3.2637 - accuracy: 0.2518 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 3.1522 - accuracy: 0.2656\n",
      "Epoch 28: loss improved from 3.26369 to 3.15213, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 3.1521 - accuracy: 0.2657 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 3.0400 - accuracy: 0.2815\n",
      "Epoch 29: loss improved from 3.15213 to 3.04003, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 3.0400 - accuracy: 0.2815 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 2.9291 - accuracy: 0.2980\n",
      "Epoch 30: loss improved from 3.04003 to 2.92874, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 2.9287 - accuracy: 0.2980 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 2.8208 - accuracy: 0.3198\n",
      "Epoch 31: loss improved from 2.92874 to 2.82027, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 2.8203 - accuracy: 0.3199 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 2.7247 - accuracy: 0.3342\n",
      "Epoch 32: loss improved from 2.82027 to 2.72585, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 2.7258 - accuracy: 0.3341 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 2.6170 - accuracy: 0.3498\n",
      "Epoch 33: loss improved from 2.72585 to 2.61723, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 2.6172 - accuracy: 0.3497 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 2.5140 - accuracy: 0.3744\n",
      "Epoch 34: loss improved from 2.61723 to 2.51400, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 2.5140 - accuracy: 0.3743 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 2.4243 - accuracy: 0.3948\n",
      "Epoch 35: loss improved from 2.51400 to 2.42435, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 2.4243 - accuracy: 0.3948 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 2.3370 - accuracy: 0.4118\n",
      "Epoch 36: loss improved from 2.42435 to 2.33731, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 2.3373 - accuracy: 0.4117 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 2.2432 - accuracy: 0.4260\n",
      "Epoch 37: loss improved from 2.33731 to 2.24326, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 2.2433 - accuracy: 0.4259 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 2.1577 - accuracy: 0.4463\n",
      "Epoch 38: loss improved from 2.24326 to 2.15770, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 2.1577 - accuracy: 0.4463 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 2.0687 - accuracy: 0.4662\n",
      "Epoch 39: loss improved from 2.15770 to 2.06851, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 2.0685 - accuracy: 0.4661 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 1.9924 - accuracy: 0.4838\n",
      "Epoch 40: loss improved from 2.06851 to 1.99305, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 1.9930 - accuracy: 0.4837 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 1.9065 - accuracy: 0.4995\n",
      "Epoch 41: loss improved from 1.99305 to 1.90602, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 23ms/step - loss: 1.9060 - accuracy: 0.4997 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 1.8304 - accuracy: 0.5128\n",
      "Epoch 42: loss improved from 1.90602 to 1.83095, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 57s 22ms/step - loss: 1.8310 - accuracy: 0.5126 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 1.7554 - accuracy: 0.5369\n",
      "Epoch 43: loss improved from 1.83095 to 1.75533, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 1.7553 - accuracy: 0.5368 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 1.6905 - accuracy: 0.5484\n",
      "Epoch 44: loss improved from 1.75533 to 1.69052, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 1.6905 - accuracy: 0.5484 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 1.6209 - accuracy: 0.5670\n",
      "Epoch 45: loss improved from 1.69052 to 1.62089, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 60s 23ms/step - loss: 1.6209 - accuracy: 0.5670 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 1.5558 - accuracy: 0.5821\n",
      "Epoch 46: loss improved from 1.62089 to 1.55603, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 1.5560 - accuracy: 0.5820 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 1.5033 - accuracy: 0.5915\n",
      "Epoch 47: loss improved from 1.55603 to 1.50341, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 60s 23ms/step - loss: 1.5034 - accuracy: 0.5915 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "2584/2586 [============================>.] - ETA: 0s - loss: 1.4250 - accuracy: 0.6160\n",
      "Epoch 48: loss improved from 1.50341 to 1.42525, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 59s 23ms/step - loss: 1.4252 - accuracy: 0.6161 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 1.3765 - accuracy: 0.6206\n",
      "Epoch 49: loss improved from 1.42525 to 1.37653, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 60s 23ms/step - loss: 1.3765 - accuracy: 0.6206 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "2585/2586 [============================>.] - ETA: 0s - loss: 1.3224 - accuracy: 0.6392\n",
      "Epoch 50: loss improved from 1.37653 to 1.32256, saving model to Autos & Vehiclesnextword1.h5\n",
      "2586/2586 [==============================] - 58s 22ms/step - loss: 1.3226 - accuracy: 0.6391 - lr: 0.0010\n",
      "On Science & Technology\n",
      "Total input sequences:  9482\n",
      "Science & Technology Max sequence length is: \n",
      "16\n",
      "Epoch 1/50\n",
      "2369/2371 [============================>.] - ETA: 0s - loss: 7.1974 - accuracy: 0.0301\n",
      "Epoch 1: loss improved from inf to 7.19654, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 56s 22ms/step - loss: 7.1965 - accuracy: 0.0302 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 6.7473 - accuracy: 0.0423\n",
      "Epoch 2: loss improved from 7.19654 to 6.74709, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 6.7471 - accuracy: 0.0423 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 6.5405 - accuracy: 0.0492\n",
      "Epoch 3: loss improved from 6.74709 to 6.54039, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 6.5404 - accuracy: 0.0491 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2369/2371 [============================>.] - ETA: 0s - loss: 6.3581 - accuracy: 0.0566\n",
      "Epoch 4: loss improved from 6.54039 to 6.35727, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 6.3573 - accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 6.1889 - accuracy: 0.0628\n",
      "Epoch 5: loss improved from 6.35727 to 6.18857, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 6.1886 - accuracy: 0.0628 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 6.0412 - accuracy: 0.0685\n",
      "Epoch 6: loss improved from 6.18857 to 6.04091, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 6.0409 - accuracy: 0.0684 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 5.8812 - accuracy: 0.0704\n",
      "Epoch 7: loss improved from 6.04091 to 5.88116, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 5.8812 - accuracy: 0.0703 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 5.7340 - accuracy: 0.0757\n",
      "Epoch 8: loss improved from 5.88116 to 5.73408, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 5.7341 - accuracy: 0.0757 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "2369/2371 [============================>.] - ETA: 0s - loss: 5.5979 - accuracy: 0.0784\n",
      "Epoch 9: loss improved from 5.73408 to 5.59843, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 5.5984 - accuracy: 0.0784 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 5.4733 - accuracy: 0.0820\n",
      "Epoch 10: loss improved from 5.59843 to 5.47361, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 21ms/step - loss: 5.4736 - accuracy: 0.0819 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 5.3558 - accuracy: 0.0874\n",
      "Epoch 11: loss improved from 5.47361 to 5.35581, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 5.3558 - accuracy: 0.0874 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "2369/2371 [============================>.] - ETA: 0s - loss: 5.2331 - accuracy: 0.0923\n",
      "Epoch 12: loss improved from 5.35581 to 5.23221, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 5.2322 - accuracy: 0.0924 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 5.1171 - accuracy: 0.0936\n",
      "Epoch 13: loss improved from 5.23221 to 5.11727, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 21ms/step - loss: 5.1173 - accuracy: 0.0935 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 5.0099 - accuracy: 0.0997\n",
      "Epoch 14: loss improved from 5.11727 to 5.00997, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 5.0100 - accuracy: 0.0997 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 4.8994 - accuracy: 0.1061\n",
      "Epoch 15: loss improved from 5.00997 to 4.89954, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 4.8995 - accuracy: 0.1061 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "2369/2371 [============================>.] - ETA: 0s - loss: 4.7879 - accuracy: 0.1094\n",
      "Epoch 16: loss improved from 4.89954 to 4.78745, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 4.7874 - accuracy: 0.1095 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "2369/2371 [============================>.] - ETA: 0s - loss: 4.6677 - accuracy: 0.1178\n",
      "Epoch 17: loss improved from 4.78745 to 4.66721, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 4.6672 - accuracy: 0.1177 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 4.5551 - accuracy: 0.1195\n",
      "Epoch 18: loss improved from 4.66721 to 4.55512, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 4.5551 - accuracy: 0.1195 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 4.4345 - accuracy: 0.1304\n",
      "Epoch 19: loss improved from 4.55512 to 4.43450, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 4.4345 - accuracy: 0.1304 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 4.3133 - accuracy: 0.1404\n",
      "Epoch 20: loss improved from 4.43450 to 4.31289, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 21ms/step - loss: 4.3129 - accuracy: 0.1405 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 4.2020 - accuracy: 0.1481\n",
      "Epoch 21: loss improved from 4.31289 to 4.20170, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 4.2017 - accuracy: 0.1482 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 4.0852 - accuracy: 0.1590\n",
      "Epoch 22: loss improved from 4.20170 to 4.08521, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 22ms/step - loss: 4.0852 - accuracy: 0.1590 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 3.9654 - accuracy: 0.1694\n",
      "Epoch 23: loss improved from 4.08521 to 3.96539, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 3.9654 - accuracy: 0.1694 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 3.8543 - accuracy: 0.1831\n",
      "Epoch 24: loss improved from 3.96539 to 3.85399, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 3.8540 - accuracy: 0.1831 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 3.7376 - accuracy: 0.1914\n",
      "Epoch 25: loss improved from 3.85399 to 3.73756, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 22ms/step - loss: 3.7376 - accuracy: 0.1914 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 3.6195 - accuracy: 0.2042\n",
      "Epoch 26: loss improved from 3.73756 to 3.61948, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 3.6195 - accuracy: 0.2042 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "2369/2371 [============================>.] - ETA: 0s - loss: 3.5110 - accuracy: 0.2183\n",
      "Epoch 27: loss improved from 3.61948 to 3.51138, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 3.5114 - accuracy: 0.2182 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 3.4075 - accuracy: 0.2287\n",
      "Epoch 28: loss improved from 3.51138 to 3.40754, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 3.4075 - accuracy: 0.2287 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "2369/2371 [============================>.] - ETA: 0s - loss: 3.3037 - accuracy: 0.2444\n",
      "Epoch 29: loss improved from 3.40754 to 3.30336, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 3.3034 - accuracy: 0.2444 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 3.2002 - accuracy: 0.2582\n",
      "Epoch 30: loss improved from 3.30336 to 3.20020, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 3.2002 - accuracy: 0.2582 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 3.1109 - accuracy: 0.2764\n",
      "Epoch 31: loss improved from 3.20020 to 3.11086, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 3.1109 - accuracy: 0.2764 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "2369/2371 [============================>.] - ETA: 0s - loss: 3.0068 - accuracy: 0.2884\n",
      "Epoch 32: loss improved from 3.11086 to 3.00737, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 22ms/step - loss: 3.0074 - accuracy: 0.2882 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 2.9298 - accuracy: 0.2985\n",
      "Epoch 33: loss improved from 3.00737 to 2.92948, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 2.9295 - accuracy: 0.2986 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 2.8265 - accuracy: 0.3188\n",
      "Epoch 34: loss improved from 2.92948 to 2.82648, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 52s 22ms/step - loss: 2.8265 - accuracy: 0.3188 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 2.7465 - accuracy: 0.3309\n",
      "Epoch 35: loss improved from 2.82648 to 2.74631, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 21ms/step - loss: 2.7463 - accuracy: 0.3309 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 2.6616 - accuracy: 0.3532\n",
      "Epoch 36: loss improved from 2.74631 to 2.66159, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 49s 21ms/step - loss: 2.6616 - accuracy: 0.3532 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 2.5789 - accuracy: 0.3662\n",
      "Epoch 37: loss improved from 2.66159 to 2.57889, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 21ms/step - loss: 2.5789 - accuracy: 0.3662 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 2.5052 - accuracy: 0.3800\n",
      "Epoch 38: loss improved from 2.57889 to 2.50512, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 2.5051 - accuracy: 0.3800 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 2.4262 - accuracy: 0.3889\n",
      "Epoch 39: loss improved from 2.50512 to 2.42621, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 22ms/step - loss: 2.4262 - accuracy: 0.3889 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 2.3423 - accuracy: 0.4110\n",
      "Epoch 40: loss improved from 2.42621 to 2.34242, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 2.3424 - accuracy: 0.4110 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 2.2697 - accuracy: 0.4213\n",
      "Epoch 41: loss improved from 2.34242 to 2.26973, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 52s 22ms/step - loss: 2.2697 - accuracy: 0.4213 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 2.1840 - accuracy: 0.4470\n",
      "Epoch 42: loss improved from 2.26973 to 2.18401, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 21ms/step - loss: 2.1840 - accuracy: 0.4470 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "2369/2371 [============================>.] - ETA: 0s - loss: 2.1196 - accuracy: 0.4543\n",
      "Epoch 43: loss improved from 2.18401 to 2.11983, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 2.1198 - accuracy: 0.4543 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 2.0413 - accuracy: 0.4679\n",
      "Epoch 44: loss improved from 2.11983 to 2.04134, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 52s 22ms/step - loss: 2.0413 - accuracy: 0.4679 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 1.9719 - accuracy: 0.4876\n",
      "Epoch 45: loss improved from 2.04134 to 1.97188, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 52s 22ms/step - loss: 1.9719 - accuracy: 0.4876 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 1.8966 - accuracy: 0.5078\n",
      "Epoch 46: loss improved from 1.97188 to 1.89623, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 51s 22ms/step - loss: 1.8962 - accuracy: 0.5079 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 1.8521 - accuracy: 0.5138\n",
      "Epoch 47: loss improved from 1.89623 to 1.85208, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 1.8521 - accuracy: 0.5138 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 1.7675 - accuracy: 0.5341\n",
      "Epoch 48: loss improved from 1.85208 to 1.76753, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 52s 22ms/step - loss: 1.7675 - accuracy: 0.5341 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "2371/2371 [==============================] - ETA: 0s - loss: 1.7100 - accuracy: 0.5502\n",
      "Epoch 49: loss improved from 1.76753 to 1.71003, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 1.7100 - accuracy: 0.5502 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "2370/2371 [============================>.] - ETA: 0s - loss: 1.6610 - accuracy: 0.5675\n",
      "Epoch 50: loss improved from 1.71003 to 1.66129, saving model to Science & Technologynextword1.h5\n",
      "2371/2371 [==============================] - 50s 21ms/step - loss: 1.6613 - accuracy: 0.5675 - lr: 0.0010\n",
      "On Gaming\n",
      "Total input sequences:  59308\n",
      "Gaming Max sequence length is: \n",
      "18\n",
      "Epoch 1/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 7.3397 - accuracy: 0.0384\n",
      "Epoch 1: loss improved from inf to 7.33973, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 484s 32ms/step - loss: 7.3397 - accuracy: 0.0384 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 7.0602 - accuracy: 0.0543\n",
      "Epoch 2: loss improved from 7.33973 to 7.06012, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 485s 33ms/step - loss: 7.0601 - accuracy: 0.0543 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 6.9434 - accuracy: 0.0652\n",
      "Epoch 3: loss improved from 7.06012 to 6.94344, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 470s 32ms/step - loss: 6.9434 - accuracy: 0.0652 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 6.8221 - accuracy: 0.0813\n",
      "Epoch 4: loss improved from 6.94344 to 6.82206, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 473s 32ms/step - loss: 6.8221 - accuracy: 0.0813 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 6.6587 - accuracy: 0.0962\n",
      "Epoch 5: loss improved from 6.82206 to 6.65866, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 475s 32ms/step - loss: 6.6587 - accuracy: 0.0962 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 6.4769 - accuracy: 0.1113\n",
      "Epoch 6: loss improved from 6.65866 to 6.47693, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 473s 32ms/step - loss: 6.4769 - accuracy: 0.1113 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 6.3171 - accuracy: 0.1206\n",
      "Epoch 7: loss improved from 6.47693 to 6.31709, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 478s 32ms/step - loss: 6.3171 - accuracy: 0.1206 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 6.1702 - accuracy: 0.1317\n",
      "Epoch 8: loss improved from 6.31709 to 6.17042, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 474s 32ms/step - loss: 6.1704 - accuracy: 0.1317 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 6.0201 - accuracy: 0.1437\n",
      "Epoch 9: loss improved from 6.17042 to 6.02000, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 482s 32ms/step - loss: 6.0200 - accuracy: 0.1437 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 5.8709 - accuracy: 0.1531\n",
      "Epoch 10: loss improved from 6.02000 to 5.87083, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 481s 32ms/step - loss: 5.8708 - accuracy: 0.1531 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 5.7409 - accuracy: 0.1609\n",
      "Epoch 11: loss improved from 5.87083 to 5.74079, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 482s 33ms/step - loss: 5.7408 - accuracy: 0.1609 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 5.6101 - accuracy: 0.1694\n",
      "Epoch 12: loss improved from 5.74079 to 5.61020, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 482s 33ms/step - loss: 5.6102 - accuracy: 0.1694 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 5.4798 - accuracy: 0.1772\n",
      "Epoch 13: loss improved from 5.61020 to 5.47977, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 485s 33ms/step - loss: 5.4798 - accuracy: 0.1772 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 5.3574 - accuracy: 0.1839\n",
      "Epoch 14: loss improved from 5.47977 to 5.35739, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 484s 33ms/step - loss: 5.3574 - accuracy: 0.1839 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 5.2437 - accuracy: 0.1921\n",
      "Epoch 15: loss improved from 5.35739 to 5.24371, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 482s 33ms/step - loss: 5.2437 - accuracy: 0.1921 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 5.1426 - accuracy: 0.1996\n",
      "Epoch 16: loss improved from 5.24371 to 5.14246, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 486s 33ms/step - loss: 5.1425 - accuracy: 0.1996 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 5.0502 - accuracy: 0.2062\n",
      "Epoch 17: loss improved from 5.14246 to 5.05024, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 485s 33ms/step - loss: 5.0502 - accuracy: 0.2062 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 4.9237 - accuracy: 0.2110\n",
      "Epoch 18: loss improved from 5.05024 to 4.92389, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 477s 32ms/step - loss: 4.9239 - accuracy: 0.2109 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 4.8297 - accuracy: 0.2187\n",
      "Epoch 19: loss improved from 4.92389 to 4.82970, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 483s 33ms/step - loss: 4.8297 - accuracy: 0.2187 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 4.7384 - accuracy: 0.2252\n",
      "Epoch 20: loss improved from 4.82970 to 4.73843, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 486s 33ms/step - loss: 4.7384 - accuracy: 0.2252 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 4.6591 - accuracy: 0.2307\n",
      "Epoch 21: loss improved from 4.73843 to 4.65929, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 483s 33ms/step - loss: 4.6593 - accuracy: 0.2307 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 4.5719 - accuracy: 0.2364\n",
      "Epoch 22: loss improved from 4.65929 to 4.57190, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 486s 33ms/step - loss: 4.5719 - accuracy: 0.2364 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 4.4879 - accuracy: 0.2432\n",
      "Epoch 23: loss improved from 4.57190 to 4.48793, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 492s 33ms/step - loss: 4.4879 - accuracy: 0.2432 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 4.4016 - accuracy: 0.2492\n",
      "Epoch 24: loss improved from 4.48793 to 4.40189, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 496s 33ms/step - loss: 4.4019 - accuracy: 0.2492 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 4.3292 - accuracy: 0.2553\n",
      "Epoch 25: loss improved from 4.40189 to 4.32919, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 489s 33ms/step - loss: 4.3292 - accuracy: 0.2554 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 4.2618 - accuracy: 0.2614\n",
      "Epoch 26: loss improved from 4.32919 to 4.26178, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 482s 33ms/step - loss: 4.2618 - accuracy: 0.2614 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 4.1890 - accuracy: 0.2655\n",
      "Epoch 27: loss improved from 4.26178 to 4.18905, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 487s 33ms/step - loss: 4.1890 - accuracy: 0.2655 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 4.1224 - accuracy: 0.2729\n",
      "Epoch 28: loss improved from 4.18905 to 4.12248, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 491s 33ms/step - loss: 4.1225 - accuracy: 0.2729 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 4.0719 - accuracy: 0.2775\n",
      "Epoch 29: loss improved from 4.12248 to 4.07199, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 490s 33ms/step - loss: 4.0720 - accuracy: 0.2774 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 4.0234 - accuracy: 0.2842\n",
      "Epoch 30: loss improved from 4.07199 to 4.02328, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 490s 33ms/step - loss: 4.0233 - accuracy: 0.2842 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.9627 - accuracy: 0.2879\n",
      "Epoch 31: loss improved from 4.02328 to 3.96271, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 516s 35ms/step - loss: 3.9627 - accuracy: 0.2879 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 3.9041 - accuracy: 0.2922\n",
      "Epoch 32: loss improved from 3.96271 to 3.90399, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 491s 33ms/step - loss: 3.9040 - accuracy: 0.2922 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.8617 - accuracy: 0.2954\n",
      "Epoch 33: loss improved from 3.90399 to 3.86171, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 489s 33ms/step - loss: 3.8617 - accuracy: 0.2954 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.8053 - accuracy: 0.3037\n",
      "Epoch 34: loss improved from 3.86171 to 3.80531, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 495s 33ms/step - loss: 3.8053 - accuracy: 0.3037 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.7731 - accuracy: 0.3075\n",
      "Epoch 35: loss improved from 3.80531 to 3.77310, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 494s 33ms/step - loss: 3.7731 - accuracy: 0.3075 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.7273 - accuracy: 0.3141\n",
      "Epoch 36: loss improved from 3.77310 to 3.72734, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 488s 33ms/step - loss: 3.7273 - accuracy: 0.3141 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.6760 - accuracy: 0.3173\n",
      "Epoch 37: loss improved from 3.72734 to 3.67598, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 492s 33ms/step - loss: 3.6760 - accuracy: 0.3173 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.6462 - accuracy: 0.3215\n",
      "Epoch 38: loss improved from 3.67598 to 3.64616, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 492s 33ms/step - loss: 3.6462 - accuracy: 0.3215 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.6112 - accuracy: 0.3243\n",
      "Epoch 39: loss improved from 3.64616 to 3.61123, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 493s 33ms/step - loss: 3.6112 - accuracy: 0.3243 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 3.5646 - accuracy: 0.3301\n",
      "Epoch 40: loss improved from 3.61123 to 3.56463, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 492s 33ms/step - loss: 3.5646 - accuracy: 0.3301 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.5416 - accuracy: 0.3350\n",
      "Epoch 41: loss improved from 3.56463 to 3.54156, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 496s 33ms/step - loss: 3.5416 - accuracy: 0.3350 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.5113 - accuracy: 0.3359\n",
      "Epoch 42: loss improved from 3.54156 to 3.51130, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 497s 34ms/step - loss: 3.5113 - accuracy: 0.3359 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 3.4998 - accuracy: 0.3390\n",
      "Epoch 43: loss improved from 3.51130 to 3.49964, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 497s 34ms/step - loss: 3.4996 - accuracy: 0.3390 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 3.4812 - accuracy: 0.3457\n",
      "Epoch 44: loss improved from 3.49964 to 3.48129, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 494s 33ms/step - loss: 3.4813 - accuracy: 0.3457 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 3.4379 - accuracy: 0.3476\n",
      "Epoch 45: loss improved from 3.48129 to 3.43789, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 496s 33ms/step - loss: 3.4379 - accuracy: 0.3476 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 3.3964 - accuracy: 0.3512\n",
      "Epoch 46: loss improved from 3.43789 to 3.39637, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 499s 34ms/step - loss: 3.3964 - accuracy: 0.3512 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "14827/14827 [==============================] - ETA: 0s - loss: 3.3538 - accuracy: 0.3542\n",
      "Epoch 47: loss improved from 3.39637 to 3.35382, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 498s 34ms/step - loss: 3.3538 - accuracy: 0.3542 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 3.3264 - accuracy: 0.3587\n",
      "Epoch 48: loss improved from 3.35382 to 3.32656, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 500s 34ms/step - loss: 3.3266 - accuracy: 0.3587 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 3.3069 - accuracy: 0.3624\n",
      "Epoch 49: loss improved from 3.32656 to 3.30678, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 500s 34ms/step - loss: 3.3068 - accuracy: 0.3624 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "14826/14827 [============================>.] - ETA: 0s - loss: 3.2698 - accuracy: 0.3666\n",
      "Epoch 50: loss improved from 3.30678 to 3.26966, saving model to Gamingnextword1.h5\n",
      "14827/14827 [==============================] - 498s 34ms/step - loss: 3.2697 - accuracy: 0.3666 - lr: 0.0010\n",
      "On Comedy\n",
      "Total input sequences:  18824\n",
      "Comedy Max sequence length is: \n",
      "20\n",
      "Epoch 1/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 7.8416 - accuracy: 0.0292\n",
      "Epoch 1: loss improved from inf to 7.84164, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 153s 31ms/step - loss: 7.8416 - accuracy: 0.0292 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 7.4143 - accuracy: 0.0294\n",
      "Epoch 2: loss improved from 7.84164 to 7.41430, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 146s 31ms/step - loss: 7.4143 - accuracy: 0.0294 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 7.2449 - accuracy: 0.0313\n",
      "Epoch 3: loss improved from 7.41430 to 7.24490, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 149s 32ms/step - loss: 7.2449 - accuracy: 0.0313 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 7.0687 - accuracy: 0.0357\n",
      "Epoch 4: loss improved from 7.24490 to 7.06868, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 149s 32ms/step - loss: 7.0687 - accuracy: 0.0357 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 6.9085 - accuracy: 0.0437\n",
      "Epoch 5: loss improved from 7.06868 to 6.90793, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 148s 32ms/step - loss: 6.9079 - accuracy: 0.0437 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 6.7541 - accuracy: 0.0453\n",
      "Epoch 6: loss improved from 6.90793 to 6.75385, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 145s 31ms/step - loss: 6.7538 - accuracy: 0.0453 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 6.5982 - accuracy: 0.0487\n",
      "Epoch 7: loss improved from 6.75385 to 6.59825, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 145s 31ms/step - loss: 6.5982 - accuracy: 0.0487 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 6.4555 - accuracy: 0.0535\n",
      "Epoch 8: loss improved from 6.59825 to 6.45546, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 148s 31ms/step - loss: 6.4555 - accuracy: 0.0535 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 6.3289 - accuracy: 0.0575\n",
      "Epoch 9: loss improved from 6.45546 to 6.32918, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 146s 31ms/step - loss: 6.3292 - accuracy: 0.0575 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 6.2151 - accuracy: 0.0612\n",
      "Epoch 10: loss improved from 6.32918 to 6.21531, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 154s 33ms/step - loss: 6.2153 - accuracy: 0.0611 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 6.0847 - accuracy: 0.0650\n",
      "Epoch 11: loss improved from 6.21531 to 6.08466, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 144s 31ms/step - loss: 6.0847 - accuracy: 0.0650 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 5.9414 - accuracy: 0.0671\n",
      "Epoch 12: loss improved from 6.08466 to 5.94144, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 146s 31ms/step - loss: 5.9414 - accuracy: 0.0671 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 5.8005 - accuracy: 0.0748\n",
      "Epoch 13: loss improved from 5.94144 to 5.80051, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 143s 30ms/step - loss: 5.8005 - accuracy: 0.0748 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 5.6700 - accuracy: 0.0833\n",
      "Epoch 14: loss improved from 5.80051 to 5.66997, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 140s 30ms/step - loss: 5.6700 - accuracy: 0.0833 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 5.5392 - accuracy: 0.0912\n",
      "Epoch 15: loss improved from 5.66997 to 5.53919, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 143s 30ms/step - loss: 5.5392 - accuracy: 0.0912 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 5.4122 - accuracy: 0.0962\n",
      "Epoch 16: loss improved from 5.53919 to 5.41219, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 142s 30ms/step - loss: 5.4122 - accuracy: 0.0962 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 5.2837 - accuracy: 0.1034\n",
      "Epoch 17: loss improved from 5.41219 to 5.28372, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 5.2837 - accuracy: 0.1034 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 5.1565 - accuracy: 0.1071\n",
      "Epoch 18: loss improved from 5.28372 to 5.15668, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 5.1567 - accuracy: 0.1071 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 5.0308 - accuracy: 0.1152\n",
      "Epoch 19: loss improved from 5.15668 to 5.03130, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 140s 30ms/step - loss: 5.0313 - accuracy: 0.1152 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 4.9149 - accuracy: 0.1210\n",
      "Epoch 20: loss improved from 5.03130 to 4.91508, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 4.9151 - accuracy: 0.1210 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 4.8072 - accuracy: 0.1277\n",
      "Epoch 21: loss improved from 4.91508 to 4.80726, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 143s 30ms/step - loss: 4.8073 - accuracy: 0.1278 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 4.6934 - accuracy: 0.1375\n",
      "Epoch 22: loss improved from 4.80726 to 4.69340, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 142s 30ms/step - loss: 4.6934 - accuracy: 0.1375 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 4.5821 - accuracy: 0.1468\n",
      "Epoch 23: loss improved from 4.69340 to 4.58214, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 142s 30ms/step - loss: 4.5821 - accuracy: 0.1468 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 4.4650 - accuracy: 0.1542\n",
      "Epoch 24: loss improved from 4.58214 to 4.46497, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 142s 30ms/step - loss: 4.4650 - accuracy: 0.1542 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 4.3535 - accuracy: 0.1648\n",
      "Epoch 25: loss improved from 4.46497 to 4.35342, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 143s 30ms/step - loss: 4.3534 - accuracy: 0.1648 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 4.2452 - accuracy: 0.1763\n",
      "Epoch 26: loss improved from 4.35342 to 4.24518, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 4.2452 - accuracy: 0.1763 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 4.1356 - accuracy: 0.1830\n",
      "Epoch 27: loss improved from 4.24518 to 4.13555, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 143s 30ms/step - loss: 4.1356 - accuracy: 0.1830 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 4.0300 - accuracy: 0.1946\n",
      "Epoch 28: loss improved from 4.13555 to 4.03029, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 143s 30ms/step - loss: 4.0303 - accuracy: 0.1945 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 3.9160 - accuracy: 0.2052\n",
      "Epoch 29: loss improved from 4.03029 to 3.91600, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 143s 30ms/step - loss: 3.9160 - accuracy: 0.2052 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 3.8163 - accuracy: 0.2157\n",
      "Epoch 30: loss improved from 3.91600 to 3.81631, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 143s 30ms/step - loss: 3.8163 - accuracy: 0.2157 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 3.7151 - accuracy: 0.2248\n",
      "Epoch 31: loss improved from 3.81631 to 3.71480, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 143s 30ms/step - loss: 3.7148 - accuracy: 0.2248 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 3.6221 - accuracy: 0.2403\n",
      "Epoch 32: loss improved from 3.71480 to 3.62213, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 145s 31ms/step - loss: 3.6221 - accuracy: 0.2403 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 3.5221 - accuracy: 0.2488\n",
      "Epoch 33: loss improved from 3.62213 to 3.52228, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 144s 31ms/step - loss: 3.5223 - accuracy: 0.2488 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 3.4315 - accuracy: 0.2624\n",
      "Epoch 34: loss improved from 3.52228 to 3.43169, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 139s 30ms/step - loss: 3.4317 - accuracy: 0.2624 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 3.3305 - accuracy: 0.2742\n",
      "Epoch 35: loss improved from 3.43169 to 3.33051, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 3.3305 - accuracy: 0.2742 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 3.2410 - accuracy: 0.2877\n",
      "Epoch 36: loss improved from 3.33051 to 3.24114, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 3.2411 - accuracy: 0.2876 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 3.1489 - accuracy: 0.2987\n",
      "Epoch 37: loss improved from 3.24114 to 3.14889, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 142s 30ms/step - loss: 3.1489 - accuracy: 0.2987 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 3.0694 - accuracy: 0.3159\n",
      "Epoch 38: loss improved from 3.14889 to 3.06925, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 142s 30ms/step - loss: 3.0692 - accuracy: 0.3160 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 2.9970 - accuracy: 0.3249\n",
      "Epoch 39: loss improved from 3.06925 to 2.99700, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 142s 30ms/step - loss: 2.9970 - accuracy: 0.3249 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 2.9038 - accuracy: 0.3388\n",
      "Epoch 40: loss improved from 2.99700 to 2.90367, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 139s 30ms/step - loss: 2.9037 - accuracy: 0.3388 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "4706/4706 [==============================] - ETA: 0s - loss: 2.8279 - accuracy: 0.3485\n",
      "Epoch 41: loss improved from 2.90367 to 2.82791, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 139s 30ms/step - loss: 2.8279 - accuracy: 0.3485 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 2.7570 - accuracy: 0.3626\n",
      "Epoch 42: loss improved from 2.82791 to 2.75660, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 2.7566 - accuracy: 0.3627 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 2.6714 - accuracy: 0.3798\n",
      "Epoch 43: loss improved from 2.75660 to 2.67159, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 139s 30ms/step - loss: 2.6716 - accuracy: 0.3798 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 2.6066 - accuracy: 0.3913\n",
      "Epoch 44: loss improved from 2.67159 to 2.60626, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 2.6063 - accuracy: 0.3914 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 2.5286 - accuracy: 0.4046\n",
      "Epoch 45: loss improved from 2.60626 to 2.52862, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 142s 30ms/step - loss: 2.5286 - accuracy: 0.4046 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 2.4734 - accuracy: 0.4133\n",
      "Epoch 46: loss improved from 2.52862 to 2.47369, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 2.4737 - accuracy: 0.4132 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 2.3996 - accuracy: 0.4290\n",
      "Epoch 47: loss improved from 2.47369 to 2.39949, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 140s 30ms/step - loss: 2.3995 - accuracy: 0.4290 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 2.3413 - accuracy: 0.4353\n",
      "Epoch 48: loss improved from 2.39949 to 2.34111, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 2.3411 - accuracy: 0.4353 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 2.2785 - accuracy: 0.4528\n",
      "Epoch 49: loss improved from 2.34111 to 2.27858, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 2.2786 - accuracy: 0.4527 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "4705/4706 [============================>.] - ETA: 0s - loss: 2.2194 - accuracy: 0.4611\n",
      "Epoch 50: loss improved from 2.27858 to 2.21936, saving model to Comedynextword1.h5\n",
      "4706/4706 [==============================] - 141s 30ms/step - loss: 2.2194 - accuracy: 0.4611 - lr: 0.0010\n",
      "On Music\n",
      "Total input sequences:  50221\n",
      "Music Max sequence length is: \n",
      "21\n",
      "Epoch 1/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 7.1642 - accuracy: 0.1184\n",
      "Epoch 1: loss improved from inf to 7.16419, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 510s 40ms/step - loss: 7.1642 - accuracy: 0.1184 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 6.7732 - accuracy: 0.1317\n",
      "Epoch 2: loss improved from 7.16419 to 6.77324, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 505s 40ms/step - loss: 6.7732 - accuracy: 0.1317 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 6.5771 - accuracy: 0.1410\n",
      "Epoch 3: loss improved from 6.77324 to 6.57712, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 511s 41ms/step - loss: 6.5771 - accuracy: 0.1410 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 6.3961 - accuracy: 0.1507\n",
      "Epoch 4: loss improved from 6.57712 to 6.39609, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 505s 40ms/step - loss: 6.3961 - accuracy: 0.1507 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 6.2287 - accuracy: 0.1610\n",
      "Epoch 5: loss improved from 6.39609 to 6.22876, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 509s 41ms/step - loss: 6.2288 - accuracy: 0.1610 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 6.0395 - accuracy: 0.1736\n",
      "Epoch 6: loss improved from 6.22876 to 6.03951, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 508s 40ms/step - loss: 6.0395 - accuracy: 0.1736 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 5.8417 - accuracy: 0.1882\n",
      "Epoch 7: loss improved from 6.03951 to 5.84172, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 504s 40ms/step - loss: 5.8417 - accuracy: 0.1881 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 5.6459 - accuracy: 0.2029\n",
      "Epoch 8: loss improved from 5.84172 to 5.64594, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 508s 40ms/step - loss: 5.6459 - accuracy: 0.2029 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 5.4449 - accuracy: 0.2143\n",
      "Epoch 9: loss improved from 5.64594 to 5.44493, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 510s 41ms/step - loss: 5.4449 - accuracy: 0.2143 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 5.2361 - accuracy: 0.2247\n",
      "Epoch 10: loss improved from 5.44493 to 5.23605, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 552s 44ms/step - loss: 5.2361 - accuracy: 0.2247 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 5.0448 - accuracy: 0.2359\n",
      "Epoch 11: loss improved from 5.23605 to 5.04483, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 530s 42ms/step - loss: 5.0448 - accuracy: 0.2359 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 4.8605 - accuracy: 0.2454\n",
      "Epoch 12: loss improved from 5.04483 to 4.86047, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 511s 41ms/step - loss: 4.8605 - accuracy: 0.2454 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 4.6970 - accuracy: 0.2575\n",
      "Epoch 13: loss improved from 4.86047 to 4.69692, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 507s 40ms/step - loss: 4.6969 - accuracy: 0.2576 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 4.5444 - accuracy: 0.2681\n",
      "Epoch 14: loss improved from 4.69692 to 4.54439, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 511s 41ms/step - loss: 4.5444 - accuracy: 0.2681 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 4.3952 - accuracy: 0.2776\n",
      "Epoch 15: loss improved from 4.54439 to 4.39524, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 514s 41ms/step - loss: 4.3952 - accuracy: 0.2776 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 4.2611 - accuracy: 0.2867\n",
      "Epoch 16: loss improved from 4.39524 to 4.26122, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 512s 41ms/step - loss: 4.2612 - accuracy: 0.2867 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 4.1271 - accuracy: 0.2990\n",
      "Epoch 17: loss improved from 4.26122 to 4.12708, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 513s 41ms/step - loss: 4.1271 - accuracy: 0.2990 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 4.0083 - accuracy: 0.3092\n",
      "Epoch 18: loss improved from 4.12708 to 4.00825, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 511s 41ms/step - loss: 4.0083 - accuracy: 0.3092 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 3.8883 - accuracy: 0.3174\n",
      "Epoch 19: loss improved from 4.00825 to 3.88838, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 511s 41ms/step - loss: 3.8884 - accuracy: 0.3174 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 3.7873 - accuracy: 0.3272\n",
      "Epoch 20: loss improved from 3.88838 to 3.78727, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 512s 41ms/step - loss: 3.7873 - accuracy: 0.3272 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 3.6764 - accuracy: 0.3385\n",
      "Epoch 21: loss improved from 3.78727 to 3.67634, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 514s 41ms/step - loss: 3.6763 - accuracy: 0.3385 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 3.5702 - accuracy: 0.3460\n",
      "Epoch 22: loss improved from 3.67634 to 3.57016, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 514s 41ms/step - loss: 3.5702 - accuracy: 0.3460 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 3.4704 - accuracy: 0.3566\n",
      "Epoch 23: loss improved from 3.57016 to 3.47034, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 517s 41ms/step - loss: 3.4703 - accuracy: 0.3566 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 3.3834 - accuracy: 0.3667\n",
      "Epoch 24: loss improved from 3.47034 to 3.38344, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 516s 41ms/step - loss: 3.3834 - accuracy: 0.3667 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 3.3042 - accuracy: 0.3742\n",
      "Epoch 25: loss improved from 3.38344 to 3.30420, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 515s 41ms/step - loss: 3.3042 - accuracy: 0.3742 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 3.2081 - accuracy: 0.3843\n",
      "Epoch 26: loss improved from 3.30420 to 3.20802, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 516s 41ms/step - loss: 3.2080 - accuracy: 0.3843 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 3.1329 - accuracy: 0.3931\n",
      "Epoch 27: loss improved from 3.20802 to 3.13293, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 515s 41ms/step - loss: 3.1329 - accuracy: 0.3931 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 3.0554 - accuracy: 0.4044\n",
      "Epoch 28: loss improved from 3.13293 to 3.05534, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 516s 41ms/step - loss: 3.0553 - accuracy: 0.4045 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.9747 - accuracy: 0.4126\n",
      "Epoch 29: loss improved from 3.05534 to 2.97474, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 514s 41ms/step - loss: 2.9747 - accuracy: 0.4126 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.8990 - accuracy: 0.4218\n",
      "Epoch 30: loss improved from 2.97474 to 2.89896, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 518s 41ms/step - loss: 2.8990 - accuracy: 0.4218 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 2.8289 - accuracy: 0.4316\n",
      "Epoch 31: loss improved from 2.89896 to 2.82880, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 518s 41ms/step - loss: 2.8288 - accuracy: 0.4316 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 2.7618 - accuracy: 0.4406\n",
      "Epoch 32: loss improved from 2.82880 to 2.76184, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 518s 41ms/step - loss: 2.7618 - accuracy: 0.4406 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.7001 - accuracy: 0.4465\n",
      "Epoch 33: loss improved from 2.76184 to 2.70014, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 519s 41ms/step - loss: 2.7001 - accuracy: 0.4465 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.6357 - accuracy: 0.4557\n",
      "Epoch 34: loss improved from 2.70014 to 2.63572, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 522s 42ms/step - loss: 2.6357 - accuracy: 0.4557 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 2.5816 - accuracy: 0.4624\n",
      "Epoch 35: loss improved from 2.63572 to 2.58166, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 525s 42ms/step - loss: 2.5817 - accuracy: 0.4624 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.5219 - accuracy: 0.4722\n",
      "Epoch 36: loss improved from 2.58166 to 2.52187, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 523s 42ms/step - loss: 2.5219 - accuracy: 0.4722 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 2.4843 - accuracy: 0.4765\n",
      "Epoch 37: loss improved from 2.52187 to 2.48429, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 525s 42ms/step - loss: 2.4843 - accuracy: 0.4765 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 2.4236 - accuracy: 0.4866\n",
      "Epoch 38: loss improved from 2.48429 to 2.42362, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 530s 42ms/step - loss: 2.4236 - accuracy: 0.4865 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 2.3703 - accuracy: 0.4934\n",
      "Epoch 39: loss improved from 2.42362 to 2.37022, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 525s 42ms/step - loss: 2.3702 - accuracy: 0.4934 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.3351 - accuracy: 0.5003\n",
      "Epoch 40: loss improved from 2.37022 to 2.33510, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 523s 42ms/step - loss: 2.3351 - accuracy: 0.5003 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.2807 - accuracy: 0.5078\n",
      "Epoch 41: loss improved from 2.33510 to 2.28071, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 522s 42ms/step - loss: 2.2807 - accuracy: 0.5078 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 2.2459 - accuracy: 0.5105\n",
      "Epoch 42: loss improved from 2.28071 to 2.24597, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 524s 42ms/step - loss: 2.2460 - accuracy: 0.5105 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 2.1979 - accuracy: 0.5218\n",
      "Epoch 43: loss improved from 2.24597 to 2.19794, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 525s 42ms/step - loss: 2.1979 - accuracy: 0.5218 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.1678 - accuracy: 0.5242\n",
      "Epoch 44: loss improved from 2.19794 to 2.16776, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 528s 42ms/step - loss: 2.1678 - accuracy: 0.5242 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.1239 - accuracy: 0.5333\n",
      "Epoch 45: loss improved from 2.16776 to 2.12391, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 528s 42ms/step - loss: 2.1239 - accuracy: 0.5333 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.0868 - accuracy: 0.5386\n",
      "Epoch 46: loss improved from 2.12391 to 2.08680, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 526s 42ms/step - loss: 2.0868 - accuracy: 0.5386 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.0633 - accuracy: 0.5433\n",
      "Epoch 47: loss improved from 2.08680 to 2.06329, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 525s 42ms/step - loss: 2.0633 - accuracy: 0.5433 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 2.0282 - accuracy: 0.5492\n",
      "Epoch 48: loss improved from 2.06329 to 2.02817, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 525s 42ms/step - loss: 2.0282 - accuracy: 0.5492 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "12556/12556 [==============================] - ETA: 0s - loss: 1.9884 - accuracy: 0.5572\n",
      "Epoch 49: loss improved from 2.02817 to 1.98843, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 524s 42ms/step - loss: 1.9884 - accuracy: 0.5572 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "12555/12556 [============================>.] - ETA: 0s - loss: 1.9667 - accuracy: 0.5586\n",
      "Epoch 50: loss improved from 1.98843 to 1.96662, saving model to Musicnextword1.h5\n",
      "12556/12556 [==============================] - 528s 42ms/step - loss: 1.9666 - accuracy: 0.5586 - lr: 0.0010\n",
      "On Travel & Events\n",
      "Total input sequences:  3176\n",
      "Travel & Events Max sequence length is: \n",
      "17\n",
      "Epoch 1/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 6.6699 - accuracy: 0.0305\n",
      "Epoch 1: loss improved from inf to 6.66993, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 25s 24ms/step - loss: 6.6699 - accuracy: 0.0305 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 6.2297 - accuracy: 0.0365\n",
      "Epoch 2: loss improved from 6.66993 to 6.22970, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 22ms/step - loss: 6.2297 - accuracy: 0.0365 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 6.0998 - accuracy: 0.0349\n",
      "Epoch 3: loss improved from 6.22970 to 6.09978, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 6.0998 - accuracy: 0.0349 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 5.9551 - accuracy: 0.0451\n",
      "Epoch 4: loss improved from 6.09978 to 5.95820, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 16s 21ms/step - loss: 5.9582 - accuracy: 0.0450 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 5.8268 - accuracy: 0.0559\n",
      "Epoch 5: loss improved from 5.95820 to 5.82606, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 5.8261 - accuracy: 0.0557 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 5.5782 - accuracy: 0.0656\n",
      "Epoch 6: loss improved from 5.82606 to 5.57866, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 23ms/step - loss: 5.5787 - accuracy: 0.0655 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 5.3380 - accuracy: 0.0892\n",
      "Epoch 7: loss improved from 5.57866 to 5.33823, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 5.3382 - accuracy: 0.0891 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 5.1455 - accuracy: 0.1003\n",
      "Epoch 8: loss improved from 5.33823 to 5.14508, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 5.1451 - accuracy: 0.1004 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 4.9833 - accuracy: 0.1053\n",
      "Epoch 9: loss improved from 5.14508 to 4.98402, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 4.9840 - accuracy: 0.1052 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 4.8332 - accuracy: 0.1020\n",
      "Epoch 10: loss improved from 4.98402 to 4.83194, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 4.8319 - accuracy: 0.1020 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 4.6886 - accuracy: 0.1136\n",
      "Epoch 11: loss improved from 4.83194 to 4.68847, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 4.6885 - accuracy: 0.1134 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 4.5292 - accuracy: 0.1196\n",
      "Epoch 12: loss improved from 4.68847 to 4.52902, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 4.5290 - accuracy: 0.1196 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 4.3821 - accuracy: 0.1293\n",
      "Epoch 13: loss improved from 4.52902 to 4.38221, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 19s 24ms/step - loss: 4.3822 - accuracy: 0.1291 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 4.2247 - accuracy: 0.1338\n",
      "Epoch 14: loss improved from 4.38221 to 4.22780, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 4.2278 - accuracy: 0.1335 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 4.0655 - accuracy: 0.1463\n",
      "Epoch 15: loss improved from 4.22780 to 4.06566, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 22ms/step - loss: 4.0657 - accuracy: 0.1464 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 3.9256 - accuracy: 0.1529\n",
      "Epoch 16: loss improved from 4.06566 to 3.92602, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 3.9260 - accuracy: 0.1527 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 3.7716 - accuracy: 0.1599\n",
      "Epoch 17: loss improved from 3.92602 to 3.77161, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 3.7716 - accuracy: 0.1599 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 3.6255 - accuracy: 0.1766\n",
      "Epoch 18: loss improved from 3.77161 to 3.62552, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 3.6255 - accuracy: 0.1766 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 3.4718 - accuracy: 0.1889\n",
      "Epoch 19: loss improved from 3.62552 to 3.47178, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 3.4718 - accuracy: 0.1889 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 3.3213 - accuracy: 0.2040\n",
      "Epoch 20: loss improved from 3.47178 to 3.32344, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 22ms/step - loss: 3.3234 - accuracy: 0.2037 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 3.1775 - accuracy: 0.2251\n",
      "Epoch 21: loss improved from 3.32344 to 3.17647, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 22ms/step - loss: 3.1765 - accuracy: 0.2258 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 3.0323 - accuracy: 0.2428\n",
      "Epoch 22: loss improved from 3.17647 to 3.03233, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 3.0323 - accuracy: 0.2428 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 2.8698 - accuracy: 0.2670\n",
      "Epoch 23: loss improved from 3.03233 to 2.87098, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 2.8710 - accuracy: 0.2670 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 2.7558 - accuracy: 0.2780\n",
      "Epoch 24: loss improved from 2.87098 to 2.75577, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 2.7558 - accuracy: 0.2780 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 2.6173 - accuracy: 0.3186\n",
      "Epoch 25: loss improved from 2.75577 to 2.61733, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 2.6173 - accuracy: 0.3186 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 2.4900 - accuracy: 0.3320\n",
      "Epoch 26: loss improved from 2.61733 to 2.48961, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 2.4896 - accuracy: 0.3319 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 2.3672 - accuracy: 0.3742\n",
      "Epoch 27: loss improved from 2.48961 to 2.36737, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 22ms/step - loss: 2.3674 - accuracy: 0.3741 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 2.2574 - accuracy: 0.3914\n",
      "Epoch 28: loss improved from 2.36737 to 2.25849, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 2.2585 - accuracy: 0.3911 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 2.1512 - accuracy: 0.4179\n",
      "Epoch 29: loss improved from 2.25849 to 2.15318, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 2.1532 - accuracy: 0.4175 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 2.0570 - accuracy: 0.4370\n",
      "Epoch 30: loss improved from 2.15318 to 2.05697, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 2.0570 - accuracy: 0.4370 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 1.9823 - accuracy: 0.4508\n",
      "Epoch 31: loss improved from 2.05697 to 1.98343, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 1.9834 - accuracy: 0.4506 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 1.8666 - accuracy: 0.4852\n",
      "Epoch 32: loss improved from 1.98343 to 1.86783, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 1.8678 - accuracy: 0.4849 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 1.7770 - accuracy: 0.5079\n",
      "Epoch 33: loss improved from 1.86783 to 1.77675, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 23ms/step - loss: 1.7768 - accuracy: 0.5082 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 1.6983 - accuracy: 0.5287\n",
      "Epoch 34: loss improved from 1.77675 to 1.70072, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 19s 23ms/step - loss: 1.7007 - accuracy: 0.5283 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 1.6341 - accuracy: 0.5397\n",
      "Epoch 35: loss improved from 1.70072 to 1.63389, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 1.6339 - accuracy: 0.5400 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 1.5853 - accuracy: 0.5514\n",
      "Epoch 36: loss improved from 1.63389 to 1.58523, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 22ms/step - loss: 1.5852 - accuracy: 0.5513 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 1.4727 - accuracy: 0.5847\n",
      "Epoch 37: loss improved from 1.58523 to 1.47273, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 1.4727 - accuracy: 0.5847 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 1.4175 - accuracy: 0.6007\n",
      "Epoch 38: loss improved from 1.47273 to 1.41918, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 1.4192 - accuracy: 0.6008 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 1.3653 - accuracy: 0.6165\n",
      "Epoch 39: loss improved from 1.41918 to 1.36529, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 1.3653 - accuracy: 0.6165 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 1.3142 - accuracy: 0.6280\n",
      "Epoch 40: loss improved from 1.36529 to 1.31434, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 1.3143 - accuracy: 0.6281 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 1.2530 - accuracy: 0.6364\n",
      "Epoch 41: loss improved from 1.31434 to 1.25619, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 22ms/step - loss: 1.2562 - accuracy: 0.6357 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 1.2392 - accuracy: 0.6360\n",
      "Epoch 42: loss improved from 1.25619 to 1.23922, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 1.2392 - accuracy: 0.6360 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 1.1726 - accuracy: 0.6570\n",
      "Epoch 43: loss improved from 1.23922 to 1.17174, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 1.1717 - accuracy: 0.6574 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 1.1296 - accuracy: 0.6665\n",
      "Epoch 44: loss improved from 1.17174 to 1.12883, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 1.1288 - accuracy: 0.6669 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "792/794 [============================>.] - ETA: 0s - loss: 1.0853 - accuracy: 0.6916\n",
      "Epoch 45: loss improved from 1.12883 to 1.08679, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 1.0868 - accuracy: 0.6914 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "793/794 [============================>.] - ETA: 0s - loss: 1.0518 - accuracy: 0.6920\n",
      "Epoch 46: loss improved from 1.08679 to 1.05136, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 1.0514 - accuracy: 0.6921 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 1.0079 - accuracy: 0.6999\n",
      "Epoch 47: loss improved from 1.05136 to 1.00786, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 1.0079 - accuracy: 0.6999 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 0.9913 - accuracy: 0.7081\n",
      "Epoch 48: loss improved from 1.00786 to 0.99130, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 23ms/step - loss: 0.9913 - accuracy: 0.7081 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 0.9448 - accuracy: 0.7166\n",
      "Epoch 49: loss improved from 0.99130 to 0.94477, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 0.9448 - accuracy: 0.7166 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "794/794 [==============================] - ETA: 0s - loss: 0.9302 - accuracy: 0.7298\n",
      "Epoch 50: loss improved from 0.94477 to 0.93025, saving model to Travel & Eventsnextword1.h5\n",
      "794/794 [==============================] - 18s 22ms/step - loss: 0.9302 - accuracy: 0.7298 - lr: 0.0010\n",
      "On Sports\n",
      "Total input sequences:  102637\n",
      "Sports Max sequence length is: \n",
      "21\n",
      "Epoch 1/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 7.4157 - accuracy: 0.0611\n",
      "Epoch 1: loss improved from inf to 7.41567, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1167s 45ms/step - loss: 7.4157 - accuracy: 0.0611 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 7.1413 - accuracy: 0.1002\n",
      "Epoch 2: loss improved from 7.41567 to 7.14127, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1160s 45ms/step - loss: 7.1413 - accuracy: 0.1002 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 6.8920 - accuracy: 0.1313\n",
      "Epoch 3: loss improved from 7.14127 to 6.89195, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1167s 45ms/step - loss: 6.8920 - accuracy: 0.1313 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 6.6086 - accuracy: 0.1518\n",
      "Epoch 4: loss improved from 6.89195 to 6.60859, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1167s 45ms/step - loss: 6.6086 - accuracy: 0.1518 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 6.3363 - accuracy: 0.1724\n",
      "Epoch 5: loss improved from 6.60859 to 6.33628, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1180s 46ms/step - loss: 6.3363 - accuracy: 0.1724 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 6.0876 - accuracy: 0.1906\n",
      "Epoch 6: loss improved from 6.33628 to 6.08771, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1198s 47ms/step - loss: 6.0877 - accuracy: 0.1906 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 5.9074 - accuracy: 0.2050\n",
      "Epoch 7: loss improved from 6.08771 to 5.90742, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1183s 46ms/step - loss: 5.9074 - accuracy: 0.2050 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 5.6966 - accuracy: 0.2176\n",
      "Epoch 8: loss improved from 5.90742 to 5.69659, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1156s 45ms/step - loss: 5.6966 - accuracy: 0.2176 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 5.5136 - accuracy: 0.2281\n",
      "Epoch 9: loss improved from 5.69659 to 5.51352, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1136s 44ms/step - loss: 5.5135 - accuracy: 0.2281 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 5.3697 - accuracy: 0.2394\n",
      "Epoch 10: loss improved from 5.51352 to 5.36969, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1154s 45ms/step - loss: 5.3697 - accuracy: 0.2394 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 5.2604 - accuracy: 0.2483\n",
      "Epoch 11: loss improved from 5.36969 to 5.26039, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1160s 45ms/step - loss: 5.2604 - accuracy: 0.2483 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 5.1729 - accuracy: 0.2578\n",
      "Epoch 12: loss improved from 5.26039 to 5.17293, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1159s 45ms/step - loss: 5.1729 - accuracy: 0.2578 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 5.0896 - accuracy: 0.2652\n",
      "Epoch 13: loss improved from 5.17293 to 5.08954, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1170s 46ms/step - loss: 5.0895 - accuracy: 0.2652 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 4.9933 - accuracy: 0.2710\n",
      "Epoch 14: loss improved from 5.08954 to 4.99325, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1174s 46ms/step - loss: 4.9933 - accuracy: 0.2710 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 4.9077 - accuracy: 0.2769\n",
      "Epoch 15: loss improved from 4.99325 to 4.90771, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1172s 46ms/step - loss: 4.9077 - accuracy: 0.2769 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 4.8408 - accuracy: 0.2808\n",
      "Epoch 16: loss improved from 4.90771 to 4.84085, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1192s 46ms/step - loss: 4.8408 - accuracy: 0.2808 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 4.7513 - accuracy: 0.2866\n",
      "Epoch 17: loss improved from 4.84085 to 4.75130, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1163s 45ms/step - loss: 4.7513 - accuracy: 0.2866 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 4.6992 - accuracy: 0.2915\n",
      "Epoch 18: loss improved from 4.75130 to 4.69925, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1173s 46ms/step - loss: 4.6992 - accuracy: 0.2915 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 4.6236 - accuracy: 0.2950\n",
      "Epoch 19: loss improved from 4.69925 to 4.62360, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1175s 46ms/step - loss: 4.6236 - accuracy: 0.2950 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 4.5057 - accuracy: 0.3007\n",
      "Epoch 20: loss improved from 4.62360 to 4.50568, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1166s 45ms/step - loss: 4.5057 - accuracy: 0.3007 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 4.4328 - accuracy: 0.3054\n",
      "Epoch 21: loss improved from 4.50568 to 4.43284, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1171s 46ms/step - loss: 4.4328 - accuracy: 0.3054 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 4.3695 - accuracy: 0.3122\n",
      "Epoch 22: loss improved from 4.43284 to 4.36948, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1172s 46ms/step - loss: 4.3695 - accuracy: 0.3122 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 4.3148 - accuracy: 0.3158\n",
      "Epoch 23: loss improved from 4.36948 to 4.31495, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1162s 45ms/step - loss: 4.3150 - accuracy: 0.3158 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 4.2635 - accuracy: 0.3178\n",
      "Epoch 24: loss improved from 4.31495 to 4.26350, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1172s 46ms/step - loss: 4.2635 - accuracy: 0.3178 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 4.1831 - accuracy: 0.3223\n",
      "Epoch 25: loss improved from 4.26350 to 4.18307, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1140s 44ms/step - loss: 4.1831 - accuracy: 0.3223 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 4.1376 - accuracy: 0.3252\n",
      "Epoch 26: loss improved from 4.18307 to 4.13753, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1144s 45ms/step - loss: 4.1375 - accuracy: 0.3252 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 4.1003 - accuracy: 0.3297\n",
      "Epoch 27: loss improved from 4.13753 to 4.10030, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1149s 45ms/step - loss: 4.1003 - accuracy: 0.3297 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 4.0671 - accuracy: 0.3334\n",
      "Epoch 28: loss improved from 4.10030 to 4.06726, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1156s 45ms/step - loss: 4.0673 - accuracy: 0.3334 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 4.0297 - accuracy: 0.3343\n",
      "Epoch 29: loss improved from 4.06726 to 4.02969, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1160s 45ms/step - loss: 4.0297 - accuracy: 0.3343 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 4.0014 - accuracy: 0.3384\n",
      "Epoch 30: loss improved from 4.02969 to 4.00146, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1160s 45ms/step - loss: 4.0015 - accuracy: 0.3384 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 3.9947 - accuracy: 0.3418\n",
      "Epoch 31: loss improved from 4.00146 to 3.99470, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1190s 46ms/step - loss: 3.9947 - accuracy: 0.3418 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 3.9553 - accuracy: 0.3421\n",
      "Epoch 32: loss improved from 3.99470 to 3.95536, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1145s 45ms/step - loss: 3.9554 - accuracy: 0.3421 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 3.9560 - accuracy: 0.3432\n",
      "Epoch 33: loss did not improve from 3.95536\n",
      "25660/25660 [==============================] - 1147s 45ms/step - loss: 3.9560 - accuracy: 0.3432 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 3.9339 - accuracy: 0.3465\n",
      "Epoch 34: loss improved from 3.95536 to 3.93386, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1147s 45ms/step - loss: 3.9339 - accuracy: 0.3465 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 3.9110 - accuracy: 0.3480\n",
      "Epoch 35: loss improved from 3.93386 to 3.91099, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1140s 44ms/step - loss: 3.9110 - accuracy: 0.3480 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 3.9075 - accuracy: 0.3494\n",
      "Epoch 36: loss improved from 3.91099 to 3.90757, saving model to Sportsnextword1.h5\n",
      "25660/25660 [==============================] - 1192s 46ms/step - loss: 3.9076 - accuracy: 0.3493 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 3.9398 - accuracy: 0.3498\n",
      "Epoch 37: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1161s 45ms/step - loss: 3.9398 - accuracy: 0.3498 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 3.9127 - accuracy: 0.3505\n",
      "Epoch 38: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1154s 45ms/step - loss: 3.9127 - accuracy: 0.3505 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 8.8575 - accuracy: 0.1874\n",
      "Epoch 39: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1137s 44ms/step - loss: 8.8575 - accuracy: 0.1874 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 7.2266 - accuracy: 0.1207\n",
      "Epoch 40: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1135s 44ms/step - loss: 7.2265 - accuracy: 0.1207 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 6.8937 - accuracy: 0.1505\n",
      "Epoch 41: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1135s 44ms/step - loss: 6.8938 - accuracy: 0.1505 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 6.7319 - accuracy: 0.1640\n",
      "Epoch 42: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1153s 45ms/step - loss: 6.7319 - accuracy: 0.1640 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 6.6318 - accuracy: 0.1723\n",
      "Epoch 43: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1150s 45ms/step - loss: 6.6318 - accuracy: 0.1723 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 6.5425 - accuracy: 0.1811\n",
      "Epoch 44: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1159s 45ms/step - loss: 6.5425 - accuracy: 0.1811 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 6.4770 - accuracy: 0.1861\n",
      "Epoch 45: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1157s 45ms/step - loss: 6.4769 - accuracy: 0.1861 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 6.4510 - accuracy: 0.1903\n",
      "Epoch 46: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1158s 45ms/step - loss: 6.4510 - accuracy: 0.1903 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "25659/25660 [============================>.] - ETA: 0s - loss: 6.4271 - accuracy: 0.1947\n",
      "Epoch 47: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1158s 45ms/step - loss: 6.4271 - accuracy: 0.1947 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 6.4368 - accuracy: 0.1959\n",
      "Epoch 48: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1155s 45ms/step - loss: 6.4368 - accuracy: 0.1959 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 6.4025 - accuracy: 0.2013\n",
      "Epoch 49: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1155s 45ms/step - loss: 6.4025 - accuracy: 0.2013 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "25660/25660 [==============================] - ETA: 0s - loss: 6.3972 - accuracy: 0.2032\n",
      "Epoch 50: loss did not improve from 3.90757\n",
      "25660/25660 [==============================] - 1162s 45ms/step - loss: 6.3972 - accuracy: 0.2032 - lr: 0.0010\n",
      "On News & Politics\n",
      "Total input sequences:  21112\n",
      "News & Politics Max sequence length is: \n",
      "18\n",
      "Epoch 1/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 7.7395 - accuracy: 0.0304\n",
      "Epoch 1: loss improved from inf to 7.73953, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 161s 29ms/step - loss: 7.7395 - accuracy: 0.0304 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 7.3457 - accuracy: 0.0381\n",
      "Epoch 2: loss improved from 7.73953 to 7.34570, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 156s 29ms/step - loss: 7.3457 - accuracy: 0.0381 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 7.1804 - accuracy: 0.0473\n",
      "Epoch 3: loss improved from 7.34570 to 7.18029, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 7.1803 - accuracy: 0.0473 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 7.0628 - accuracy: 0.0518\n",
      "Epoch 4: loss improved from 7.18029 to 7.06243, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 7.0624 - accuracy: 0.0518 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 6.9344 - accuracy: 0.0553\n",
      "Epoch 5: loss improved from 7.06243 to 6.93435, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 6.9344 - accuracy: 0.0553 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 6.7935 - accuracy: 0.0582\n",
      "Epoch 6: loss improved from 6.93435 to 6.79361, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 6.7936 - accuracy: 0.0582 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 6.6673 - accuracy: 0.0609\n",
      "Epoch 7: loss improved from 6.79361 to 6.66726, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 6.6673 - accuracy: 0.0609 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 6.5498 - accuracy: 0.0628\n",
      "Epoch 8: loss improved from 6.66726 to 6.54983, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 6.5498 - accuracy: 0.0628 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 6.4343 - accuracy: 0.0637\n",
      "Epoch 9: loss improved from 6.54983 to 6.43441, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 6.4344 - accuracy: 0.0637 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 6.3209 - accuracy: 0.0657\n",
      "Epoch 10: loss improved from 6.43441 to 6.32090, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 6.3209 - accuracy: 0.0657 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 6.2192 - accuracy: 0.0678\n",
      "Epoch 11: loss improved from 6.32090 to 6.21920, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 151s 29ms/step - loss: 6.2192 - accuracy: 0.0678 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 6.1169 - accuracy: 0.0702\n",
      "Epoch 12: loss improved from 6.21920 to 6.11684, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 6.1168 - accuracy: 0.0701 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 6.0256 - accuracy: 0.0708\n",
      "Epoch 13: loss improved from 6.11684 to 6.02589, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 6.0259 - accuracy: 0.0708 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 5.9391 - accuracy: 0.0743\n",
      "Epoch 14: loss improved from 6.02589 to 5.93910, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 5.9391 - accuracy: 0.0743 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 5.8548 - accuracy: 0.0745\n",
      "Epoch 15: loss improved from 5.93910 to 5.85467, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 5.8547 - accuracy: 0.0745 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 5.7660 - accuracy: 0.0773\n",
      "Epoch 16: loss improved from 5.85467 to 5.76624, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 5.7662 - accuracy: 0.0773 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 5.6744 - accuracy: 0.0780\n",
      "Epoch 17: loss improved from 5.76624 to 5.67447, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 5.6745 - accuracy: 0.0780 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 5.5692 - accuracy: 0.0814\n",
      "Epoch 18: loss improved from 5.67447 to 5.56923, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 5.5692 - accuracy: 0.0814 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 5.4574 - accuracy: 0.0852\n",
      "Epoch 19: loss improved from 5.56923 to 5.45738, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 5.4574 - accuracy: 0.0852 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 5.3494 - accuracy: 0.0883\n",
      "Epoch 20: loss improved from 5.45738 to 5.34939, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 5.3494 - accuracy: 0.0883 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 5.2314 - accuracy: 0.0905\n",
      "Epoch 21: loss improved from 5.34939 to 5.23172, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 151s 29ms/step - loss: 5.2317 - accuracy: 0.0905 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 5.1128 - accuracy: 0.0961\n",
      "Epoch 22: loss improved from 5.23172 to 5.11284, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 5.1128 - accuracy: 0.0961 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 5.0004 - accuracy: 0.1016\n",
      "Epoch 23: loss improved from 5.11284 to 5.00039, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 151s 29ms/step - loss: 5.0004 - accuracy: 0.1016 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 4.8797 - accuracy: 0.1044\n",
      "Epoch 24: loss improved from 5.00039 to 4.87972, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 4.8797 - accuracy: 0.1044 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 4.7691 - accuracy: 0.1088\n",
      "Epoch 25: loss improved from 4.87972 to 4.76913, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 4.7691 - accuracy: 0.1088 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 4.6682 - accuracy: 0.1162\n",
      "Epoch 26: loss improved from 4.76913 to 4.66824, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 4.6682 - accuracy: 0.1162 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 4.5663 - accuracy: 0.1197\n",
      "Epoch 27: loss improved from 4.66824 to 4.56628, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 4.5663 - accuracy: 0.1197 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 4.4766 - accuracy: 0.1261\n",
      "Epoch 28: loss improved from 4.56628 to 4.47660, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 4.4766 - accuracy: 0.1261 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 4.3929 - accuracy: 0.1355\n",
      "Epoch 29: loss improved from 4.47660 to 4.39268, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 4.3927 - accuracy: 0.1356 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 4.3130 - accuracy: 0.1426\n",
      "Epoch 30: loss improved from 4.39268 to 4.31304, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 151s 29ms/step - loss: 4.3130 - accuracy: 0.1426 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 4.2233 - accuracy: 0.1502\n",
      "Epoch 31: loss improved from 4.31304 to 4.22330, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 154s 29ms/step - loss: 4.2233 - accuracy: 0.1502 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 4.1587 - accuracy: 0.1575\n",
      "Epoch 32: loss improved from 4.22330 to 4.15869, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 155s 29ms/step - loss: 4.1587 - accuracy: 0.1575 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 4.0738 - accuracy: 0.1591\n",
      "Epoch 33: loss improved from 4.15869 to 4.07384, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 4.0738 - accuracy: 0.1591 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 4.0093 - accuracy: 0.1703\n",
      "Epoch 34: loss improved from 4.07384 to 4.00932, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 156s 29ms/step - loss: 4.0093 - accuracy: 0.1703 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 3.9462 - accuracy: 0.1790\n",
      "Epoch 35: loss improved from 4.00932 to 3.94621, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 154s 29ms/step - loss: 3.9462 - accuracy: 0.1790 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 3.8724 - accuracy: 0.1876\n",
      "Epoch 36: loss improved from 3.94621 to 3.87224, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 3.8722 - accuracy: 0.1877 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 3.8048 - accuracy: 0.1921\n",
      "Epoch 37: loss improved from 3.87224 to 3.80479, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 154s 29ms/step - loss: 3.8048 - accuracy: 0.1921 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 3.7443 - accuracy: 0.2003\n",
      "Epoch 38: loss improved from 3.80479 to 3.74427, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 150s 28ms/step - loss: 3.7443 - accuracy: 0.2003 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 3.6810 - accuracy: 0.2109\n",
      "Epoch 39: loss improved from 3.74427 to 3.68112, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 3.6811 - accuracy: 0.2108 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 3.6046 - accuracy: 0.2189\n",
      "Epoch 40: loss improved from 3.68112 to 3.60464, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 154s 29ms/step - loss: 3.6046 - accuracy: 0.2189 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 3.5554 - accuracy: 0.2276\n",
      "Epoch 41: loss improved from 3.60464 to 3.55528, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 3.5553 - accuracy: 0.2276 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 3.4820 - accuracy: 0.2356\n",
      "Epoch 42: loss improved from 3.55528 to 3.48213, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 151s 29ms/step - loss: 3.4821 - accuracy: 0.2356 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 3.4161 - accuracy: 0.2450\n",
      "Epoch 43: loss improved from 3.48213 to 3.41606, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 154s 29ms/step - loss: 3.4161 - accuracy: 0.2450 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 3.3624 - accuracy: 0.2531\n",
      "Epoch 44: loss improved from 3.41606 to 3.36233, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 156s 30ms/step - loss: 3.3623 - accuracy: 0.2531 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 3.2955 - accuracy: 0.2653\n",
      "Epoch 45: loss improved from 3.36233 to 3.29551, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 156s 30ms/step - loss: 3.2955 - accuracy: 0.2653 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 3.2274 - accuracy: 0.2734\n",
      "Epoch 46: loss improved from 3.29551 to 3.22736, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 152s 29ms/step - loss: 3.2274 - accuracy: 0.2734 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "5278/5278 [==============================] - ETA: 0s - loss: 3.1722 - accuracy: 0.2843\n",
      "Epoch 47: loss improved from 3.22736 to 3.17225, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 3.1722 - accuracy: 0.2843 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 3.1078 - accuracy: 0.2971\n",
      "Epoch 48: loss improved from 3.17225 to 3.10778, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 153s 29ms/step - loss: 3.1078 - accuracy: 0.2971 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 3.0626 - accuracy: 0.3006\n",
      "Epoch 49: loss improved from 3.10778 to 3.06242, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 154s 29ms/step - loss: 3.0624 - accuracy: 0.3006 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "5277/5278 [============================>.] - ETA: 0s - loss: 2.9953 - accuracy: 0.3124\n",
      "Epoch 50: loss improved from 3.06242 to 2.99517, saving model to News & Politicsnextword1.h5\n",
      "5278/5278 [==============================] - 151s 29ms/step - loss: 2.9952 - accuracy: 0.3124 - lr: 0.0010\n",
      "On Entertainment\n",
      "Total input sequences:  89547\n",
      "Entertainment Max sequence length is: \n",
      "23\n",
      "Epoch 1/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 8.2293 - accuracy: 0.0314\n",
      "Epoch 1: loss improved from inf to 8.22944, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1200s 53ms/step - loss: 8.2294 - accuracy: 0.0314 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 8.1839 - accuracy: 0.0361\n",
      "Epoch 2: loss improved from 8.22944 to 8.18407, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1204s 54ms/step - loss: 8.1841 - accuracy: 0.0361 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 8.1740 - accuracy: 0.0404\n",
      "Epoch 3: loss improved from 8.18407 to 8.17405, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1203s 54ms/step - loss: 8.1740 - accuracy: 0.0404 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 8.1335 - accuracy: 0.0453\n",
      "Epoch 4: loss improved from 8.17405 to 8.13348, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1214s 54ms/step - loss: 8.1335 - accuracy: 0.0453 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 8.0162 - accuracy: 0.0521\n",
      "Epoch 5: loss improved from 8.13348 to 8.01619, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1213s 54ms/step - loss: 8.0162 - accuracy: 0.0521 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 7.8584 - accuracy: 0.0595\n",
      "Epoch 6: loss improved from 8.01619 to 7.85844, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1209s 54ms/step - loss: 7.8584 - accuracy: 0.0595 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 7.6784 - accuracy: 0.0659\n",
      "Epoch 7: loss improved from 7.85844 to 7.67837, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1212s 54ms/step - loss: 7.6784 - accuracy: 0.0659 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 7.4954 - accuracy: 0.0733\n",
      "Epoch 8: loss improved from 7.67837 to 7.49543, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1212s 54ms/step - loss: 7.4954 - accuracy: 0.0733 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 7.3308 - accuracy: 0.0821\n",
      "Epoch 9: loss improved from 7.49543 to 7.33080, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1212s 54ms/step - loss: 7.3308 - accuracy: 0.0821 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 7.1969 - accuracy: 0.0920\n",
      "Epoch 10: loss improved from 7.33080 to 7.19694, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1206s 54ms/step - loss: 7.1969 - accuracy: 0.0920 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 7.0462 - accuracy: 0.0996\n",
      "Epoch 11: loss improved from 7.19694 to 7.04619, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1206s 54ms/step - loss: 7.0462 - accuracy: 0.0996 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 6.9333 - accuracy: 0.1066\n",
      "Epoch 12: loss improved from 7.04619 to 6.93330, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1205s 54ms/step - loss: 6.9333 - accuracy: 0.1066 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 6.8119 - accuracy: 0.1145\n",
      "Epoch 13: loss improved from 6.93330 to 6.81190, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1191s 53ms/step - loss: 6.8119 - accuracy: 0.1145 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 6.6496 - accuracy: 0.1222\n",
      "Epoch 14: loss improved from 6.81190 to 6.64971, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1183s 53ms/step - loss: 6.6497 - accuracy: 0.1222 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 6.5254 - accuracy: 0.1297\n",
      "Epoch 15: loss improved from 6.64971 to 6.52538, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1188s 53ms/step - loss: 6.5254 - accuracy: 0.1297 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 6.4357 - accuracy: 0.1369\n",
      "Epoch 16: loss improved from 6.52538 to 6.43568, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1190s 53ms/step - loss: 6.4357 - accuracy: 0.1369 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 6.3539 - accuracy: 0.1456\n",
      "Epoch 17: loss improved from 6.43568 to 6.35387, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1187s 53ms/step - loss: 6.3539 - accuracy: 0.1456 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 6.2717 - accuracy: 0.1508\n",
      "Epoch 18: loss improved from 6.35387 to 6.27163, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1184s 53ms/step - loss: 6.2716 - accuracy: 0.1508 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 6.1748 - accuracy: 0.1569\n",
      "Epoch 19: loss improved from 6.27163 to 6.17485, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1189s 53ms/step - loss: 6.1748 - accuracy: 0.1569 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 6.1213 - accuracy: 0.1618\n",
      "Epoch 20: loss improved from 6.17485 to 6.12134, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1186s 53ms/step - loss: 6.1213 - accuracy: 0.1618 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 6.0433 - accuracy: 0.1654\n",
      "Epoch 21: loss improved from 6.12134 to 6.04333, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1189s 53ms/step - loss: 6.0433 - accuracy: 0.1654 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 5.9701 - accuracy: 0.1718\n",
      "Epoch 22: loss improved from 6.04333 to 5.97006, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1188s 53ms/step - loss: 5.9701 - accuracy: 0.1718 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 5.9093 - accuracy: 0.1752\n",
      "Epoch 23: loss improved from 5.97006 to 5.90927, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1181s 53ms/step - loss: 5.9093 - accuracy: 0.1752 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 5.8482 - accuracy: 0.1802\n",
      "Epoch 24: loss improved from 5.90927 to 5.84824, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1176s 53ms/step - loss: 5.8482 - accuracy: 0.1802 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 5.8025 - accuracy: 0.1819\n",
      "Epoch 25: loss improved from 5.84824 to 5.80250, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1171s 52ms/step - loss: 5.8025 - accuracy: 0.1819 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 5.7140 - accuracy: 0.1864\n",
      "Epoch 26: loss improved from 5.80250 to 5.71384, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1166s 52ms/step - loss: 5.7138 - accuracy: 0.1864 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 5.6495 - accuracy: 0.1904\n",
      "Epoch 27: loss improved from 5.71384 to 5.64953, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1176s 53ms/step - loss: 5.6495 - accuracy: 0.1904 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 5.5867 - accuracy: 0.1947\n",
      "Epoch 28: loss improved from 5.64953 to 5.58666, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1275s 57ms/step - loss: 5.5867 - accuracy: 0.1947 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 5.5188 - accuracy: 0.1994\n",
      "Epoch 29: loss improved from 5.58666 to 5.51881, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1542s 69ms/step - loss: 5.5188 - accuracy: 0.1994 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 5.4769 - accuracy: 0.2016\n",
      "Epoch 30: loss improved from 5.51881 to 5.47693, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1542s 69ms/step - loss: 5.4769 - accuracy: 0.2016 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 5.4257 - accuracy: 0.2058\n",
      "Epoch 31: loss improved from 5.47693 to 5.42574, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1017s 45ms/step - loss: 5.4257 - accuracy: 0.2058 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 5.3789 - accuracy: 0.2109\n",
      "Epoch 32: loss improved from 5.42574 to 5.37897, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 923s 41ms/step - loss: 5.3790 - accuracy: 0.2109 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 5.3309 - accuracy: 0.2139\n",
      "Epoch 33: loss improved from 5.37897 to 5.33081, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 894s 40ms/step - loss: 5.3308 - accuracy: 0.2139 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 5.2842 - accuracy: 0.2160\n",
      "Epoch 34: loss improved from 5.33081 to 5.28424, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 886s 40ms/step - loss: 5.2842 - accuracy: 0.2160 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 5.2338 - accuracy: 0.2185\n",
      "Epoch 35: loss improved from 5.28424 to 5.23384, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1112s 50ms/step - loss: 5.2338 - accuracy: 0.2185 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 5.1801 - accuracy: 0.2219\n",
      "Epoch 36: loss improved from 5.23384 to 5.18019, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1168s 52ms/step - loss: 5.1802 - accuracy: 0.2220 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 5.1349 - accuracy: 0.2252\n",
      "Epoch 37: loss improved from 5.18019 to 5.13510, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1158s 52ms/step - loss: 5.1351 - accuracy: 0.2252 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 5.0842 - accuracy: 0.2284\n",
      "Epoch 38: loss improved from 5.13510 to 5.08419, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1110s 50ms/step - loss: 5.0842 - accuracy: 0.2284 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 5.0429 - accuracy: 0.2316\n",
      "Epoch 39: loss improved from 5.08419 to 5.04295, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 942s 42ms/step - loss: 5.0429 - accuracy: 0.2316 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 4.9749 - accuracy: 0.2351\n",
      "Epoch 40: loss improved from 5.04295 to 4.97483, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 944s 42ms/step - loss: 4.9748 - accuracy: 0.2351 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 4.9099 - accuracy: 0.2375\n",
      "Epoch 41: loss improved from 4.97483 to 4.90999, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 952s 43ms/step - loss: 4.9100 - accuracy: 0.2375 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 4.8683 - accuracy: 0.2417\n",
      "Epoch 42: loss improved from 4.90999 to 4.86825, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 947s 42ms/step - loss: 4.8683 - accuracy: 0.2418 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 4.8183 - accuracy: 0.2455\n",
      "Epoch 43: loss improved from 4.86825 to 4.81825, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 944s 42ms/step - loss: 4.8183 - accuracy: 0.2455 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "22386/22387 [============================>.] - ETA: 0s - loss: 4.7915 - accuracy: 0.2479\n",
      "Epoch 44: loss improved from 4.81825 to 4.79148, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 943s 42ms/step - loss: 4.7915 - accuracy: 0.2479 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 4.7451 - accuracy: 0.2514\n",
      "Epoch 45: loss improved from 4.79148 to 4.74509, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 942s 42ms/step - loss: 4.7451 - accuracy: 0.2514 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 4.7194 - accuracy: 0.2517\n",
      "Epoch 46: loss improved from 4.74509 to 4.71940, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 952s 43ms/step - loss: 4.7194 - accuracy: 0.2517 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 4.6917 - accuracy: 0.2560\n",
      "Epoch 47: loss improved from 4.71940 to 4.69174, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1239s 55ms/step - loss: 4.6917 - accuracy: 0.2560 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 4.6448 - accuracy: 0.2586\n",
      "Epoch 48: loss improved from 4.69174 to 4.64485, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1391s 62ms/step - loss: 4.6448 - accuracy: 0.2586 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 4.6214 - accuracy: 0.2613\n",
      "Epoch 49: loss improved from 4.64485 to 4.62145, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 1851s 83ms/step - loss: 4.6214 - accuracy: 0.2613 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "22387/22387 [==============================] - ETA: 0s - loss: 4.5893 - accuracy: 0.2600\n",
      "Epoch 50: loss improved from 4.62145 to 4.58931, saving model to Entertainmentnextword1.h5\n",
      "22387/22387 [==============================] - 15355s 686ms/step - loss: 4.5893 - accuracy: 0.2600 - lr: 0.0010\n",
      "On Pets & Animals\n",
      "Total input sequences:  1769\n",
      "Pets & Animals Max sequence length is: \n",
      "18\n",
      "Epoch 1/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 6.3739 - accuracy: 0.0447\n",
      "Epoch 1: loss improved from inf to 6.37327, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 50s 72ms/step - loss: 6.3733 - accuracy: 0.0447 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 5.9646 - accuracy: 0.0486\n",
      "Epoch 2: loss improved from 6.37327 to 5.96544, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 22s 50ms/step - loss: 5.9654 - accuracy: 0.0486 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "441/443 [============================>.] - ETA: 0s - loss: 5.7761 - accuracy: 0.0488\n",
      "Epoch 3: loss improved from 5.96544 to 5.77549, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 16s 36ms/step - loss: 5.7755 - accuracy: 0.0486 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "441/443 [============================>.] - ETA: 0s - loss: 5.6661 - accuracy: 0.0437\n",
      "Epoch 4: loss improved from 5.77549 to 5.66651, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 14s 31ms/step - loss: 5.6665 - accuracy: 0.0435 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 5.5307 - accuracy: 0.0583\n",
      "Epoch 5: loss improved from 5.66651 to 5.52951, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 25ms/step - loss: 5.5295 - accuracy: 0.0582 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 5.3034 - accuracy: 0.0774\n",
      "Epoch 6: loss improved from 5.52951 to 5.30339, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 13s 28ms/step - loss: 5.3034 - accuracy: 0.0774 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 5.1183 - accuracy: 0.0735\n",
      "Epoch 7: loss improved from 5.30339 to 5.11914, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 14s 32ms/step - loss: 5.1191 - accuracy: 0.0735 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 4.9759 - accuracy: 0.0724\n",
      "Epoch 8: loss improved from 5.11914 to 4.97546, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 27ms/step - loss: 4.9755 - accuracy: 0.0724 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 4.7875 - accuracy: 0.0758\n",
      "Epoch 9: loss improved from 4.97546 to 4.78611, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 13s 29ms/step - loss: 4.7861 - accuracy: 0.0763 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 4.5919 - accuracy: 0.0831\n",
      "Epoch 10: loss improved from 4.78611 to 4.59215, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 26ms/step - loss: 4.5921 - accuracy: 0.0831 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "441/443 [============================>.] - ETA: 0s - loss: 4.4131 - accuracy: 0.0981\n",
      "Epoch 11: loss improved from 4.59215 to 4.41094, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 27ms/step - loss: 4.4109 - accuracy: 0.0984 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 4.2499 - accuracy: 0.1120\n",
      "Epoch 12: loss improved from 4.41094 to 4.24803, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 25ms/step - loss: 4.2480 - accuracy: 0.1125 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "441/443 [============================>.] - ETA: 0s - loss: 4.0771 - accuracy: 0.1213\n",
      "Epoch 13: loss improved from 4.24803 to 4.07774, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 24ms/step - loss: 4.0777 - accuracy: 0.1210 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 3.9152 - accuracy: 0.1334\n",
      "Epoch 14: loss improved from 4.07774 to 3.91516, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 26ms/step - loss: 3.9152 - accuracy: 0.1334 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 3.7645 - accuracy: 0.1357\n",
      "Epoch 15: loss improved from 3.91516 to 3.76494, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 15s 34ms/step - loss: 3.7649 - accuracy: 0.1357 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "441/443 [============================>.] - ETA: 0s - loss: 3.6285 - accuracy: 0.1457\n",
      "Epoch 16: loss improved from 3.76494 to 3.62616, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 25ms/step - loss: 3.6262 - accuracy: 0.1458 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 3.4648 - accuracy: 0.1555\n",
      "Epoch 17: loss improved from 3.62616 to 3.46477, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 25ms/step - loss: 3.4648 - accuracy: 0.1555 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 3.3464 - accuracy: 0.1639\n",
      "Epoch 18: loss improved from 3.46477 to 3.34637, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 10s 24ms/step - loss: 3.3464 - accuracy: 0.1639 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 3.2093 - accuracy: 0.1827\n",
      "Epoch 19: loss improved from 3.34637 to 3.20858, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 27ms/step - loss: 3.2086 - accuracy: 0.1832 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "440/443 [============================>.] - ETA: 0s - loss: 3.0969 - accuracy: 0.1972\n",
      "Epoch 20: loss improved from 3.20858 to 3.09248, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 24ms/step - loss: 3.0925 - accuracy: 0.1979 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 2.9805 - accuracy: 0.2137\n",
      "Epoch 21: loss improved from 3.09248 to 2.98052, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 10s 22ms/step - loss: 2.9805 - accuracy: 0.2137 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 2.8616 - accuracy: 0.2347\n",
      "Epoch 22: loss improved from 2.98052 to 2.86002, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 25ms/step - loss: 2.8600 - accuracy: 0.2352 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 2.7660 - accuracy: 0.2415\n",
      "Epoch 23: loss improved from 2.86002 to 2.76659, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 9s 21ms/step - loss: 2.7666 - accuracy: 0.2414 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 2.6447 - accuracy: 0.2679\n",
      "Epoch 24: loss improved from 2.76659 to 2.64465, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 10s 22ms/step - loss: 2.6447 - accuracy: 0.2679 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 2.5435 - accuracy: 0.2979\n",
      "Epoch 25: loss improved from 2.64465 to 2.54352, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 27ms/step - loss: 2.5435 - accuracy: 0.2979 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "441/443 [============================>.] - ETA: 0s - loss: 2.4384 - accuracy: 0.3135\n",
      "Epoch 26: loss improved from 2.54352 to 2.44145, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 10s 23ms/step - loss: 2.4415 - accuracy: 0.3132 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 2.3333 - accuracy: 0.3403\n",
      "Epoch 27: loss improved from 2.44145 to 2.33327, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 25ms/step - loss: 2.3333 - accuracy: 0.3403 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 2.2266 - accuracy: 0.3810\n",
      "Epoch 28: loss improved from 2.33327 to 2.22663, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 27ms/step - loss: 2.2266 - accuracy: 0.3810 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 2.1537 - accuracy: 0.3829\n",
      "Epoch 29: loss improved from 2.22663 to 2.15374, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 13s 28ms/step - loss: 2.1537 - accuracy: 0.3827 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "441/443 [============================>.] - ETA: 0s - loss: 2.0814 - accuracy: 0.4059\n",
      "Epoch 30: loss improved from 2.15374 to 2.07984, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 13s 29ms/step - loss: 2.0798 - accuracy: 0.4064 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 1.9815 - accuracy: 0.4378\n",
      "Epoch 31: loss improved from 2.07984 to 1.98107, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 26ms/step - loss: 1.9811 - accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 1.9010 - accuracy: 0.4553\n",
      "Epoch 32: loss improved from 1.98107 to 1.90133, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 27ms/step - loss: 1.9013 - accuracy: 0.4551 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 1.8366 - accuracy: 0.4649\n",
      "Epoch 33: loss improved from 1.90133 to 1.83793, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 27ms/step - loss: 1.8379 - accuracy: 0.4647 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 1.7492 - accuracy: 0.4910\n",
      "Epoch 34: loss improved from 1.83793 to 1.74940, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 26ms/step - loss: 1.7494 - accuracy: 0.4907 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 1.6867 - accuracy: 0.5054\n",
      "Epoch 35: loss improved from 1.74940 to 1.68669, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 14s 31ms/step - loss: 1.6867 - accuracy: 0.5054 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 1.6084 - accuracy: 0.5325\n",
      "Epoch 36: loss improved from 1.68669 to 1.60840, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 13s 29ms/step - loss: 1.6084 - accuracy: 0.5325 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 1.5572 - accuracy: 0.5311\n",
      "Epoch 37: loss improved from 1.60840 to 1.55786, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 15s 33ms/step - loss: 1.5579 - accuracy: 0.5308 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 1.5085 - accuracy: 0.5630\n",
      "Epoch 38: loss improved from 1.55786 to 1.50851, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 13s 29ms/step - loss: 1.5085 - accuracy: 0.5630 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 1.4553 - accuracy: 0.5741\n",
      "Epoch 39: loss improved from 1.50851 to 1.45491, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 24ms/step - loss: 1.4549 - accuracy: 0.5743 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "441/443 [============================>.] - ETA: 0s - loss: 1.4181 - accuracy: 0.5867\n",
      "Epoch 40: loss improved from 1.45491 to 1.41705, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 25ms/step - loss: 1.4171 - accuracy: 0.5868 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "440/443 [============================>.] - ETA: 0s - loss: 1.3483 - accuracy: 0.5994\n",
      "Epoch 41: loss improved from 1.41705 to 1.34607, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 10s 24ms/step - loss: 1.3461 - accuracy: 0.5992 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 1.3065 - accuracy: 0.6049\n",
      "Epoch 42: loss improved from 1.34607 to 1.30652, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 10s 22ms/step - loss: 1.3065 - accuracy: 0.6049 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 1.2549 - accuracy: 0.6369\n",
      "Epoch 43: loss improved from 1.30652 to 1.25581, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 13s 29ms/step - loss: 1.2558 - accuracy: 0.6365 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 1.2152 - accuracy: 0.6388\n",
      "Epoch 44: loss improved from 1.25581 to 1.21521, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 24ms/step - loss: 1.2152 - accuracy: 0.6388 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 1.2012 - accuracy: 0.6388\n",
      "Epoch 45: loss improved from 1.21521 to 1.20121, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 27ms/step - loss: 1.2012 - accuracy: 0.6388 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 1.1466 - accuracy: 0.6569\n",
      "Epoch 46: loss improved from 1.20121 to 1.14662, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 26ms/step - loss: 1.1466 - accuracy: 0.6569 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "442/443 [============================>.] - ETA: 0s - loss: 1.0950 - accuracy: 0.6742\n",
      "Epoch 47: loss improved from 1.14662 to 1.09489, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 11s 24ms/step - loss: 1.0949 - accuracy: 0.6744 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 1.0649 - accuracy: 0.6789\n",
      "Epoch 48: loss improved from 1.09489 to 1.06491, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 15s 33ms/step - loss: 1.0649 - accuracy: 0.6789 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "443/443 [==============================] - ETA: 0s - loss: 1.0365 - accuracy: 0.6880\n",
      "Epoch 49: loss improved from 1.06491 to 1.03648, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 13s 29ms/step - loss: 1.0365 - accuracy: 0.6880 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "441/443 [============================>.] - ETA: 0s - loss: 1.0116 - accuracy: 0.7024\n",
      "Epoch 50: loss improved from 1.03648 to 1.01172, saving model to Pets & Animalsnextword1.h5\n",
      "443/443 [==============================] - 12s 27ms/step - loss: 1.0117 - accuracy: 0.7027 - lr: 0.0010\n",
      "On Film & Animation\n",
      "Total input sequences:  9880\n",
      "Film & Animation Max sequence length is: \n",
      "17\n",
      "Epoch 1/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 7.1388 - accuracy: 0.0470\n",
      "Epoch 1: loss improved from inf to 7.13880, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 64s 24ms/step - loss: 7.1388 - accuracy: 0.0470 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2468/2470 [============================>.] - ETA: 0s - loss: 6.6194 - accuracy: 0.0594\n",
      "Epoch 2: loss improved from 7.13880 to 6.62037, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 61s 25ms/step - loss: 6.6204 - accuracy: 0.0593 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 6.3249 - accuracy: 0.0673\n",
      "Epoch 3: loss improved from 6.62037 to 6.32487, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 64s 26ms/step - loss: 6.3249 - accuracy: 0.0673 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 6.0931 - accuracy: 0.0749\n",
      "Epoch 4: loss improved from 6.32487 to 6.09306, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 83s 34ms/step - loss: 6.0931 - accuracy: 0.0749 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 5.9214 - accuracy: 0.0825\n",
      "Epoch 5: loss improved from 6.09306 to 5.92202, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 87s 35ms/step - loss: 5.9220 - accuracy: 0.0825 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 5.7760 - accuracy: 0.0965\n",
      "Epoch 6: loss improved from 5.92202 to 5.77596, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 68s 28ms/step - loss: 5.7760 - accuracy: 0.0965 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 5.6469 - accuracy: 0.1026\n",
      "Epoch 7: loss improved from 5.77596 to 5.64688, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 66s 27ms/step - loss: 5.6469 - accuracy: 0.1026 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 5.5100 - accuracy: 0.1107\n",
      "Epoch 8: loss improved from 5.64688 to 5.51001, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 69s 28ms/step - loss: 5.5100 - accuracy: 0.1107 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 5.3617 - accuracy: 0.1212\n",
      "Epoch 9: loss improved from 5.51001 to 5.36201, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 71s 29ms/step - loss: 5.3620 - accuracy: 0.1212 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 5.2117 - accuracy: 0.1289\n",
      "Epoch 10: loss improved from 5.36201 to 5.21143, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 62s 25ms/step - loss: 5.2114 - accuracy: 0.1288 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 5.0782 - accuracy: 0.1370\n",
      "Epoch 11: loss improved from 5.21143 to 5.07819, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 68s 27ms/step - loss: 5.0782 - accuracy: 0.1370 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 4.9419 - accuracy: 0.1417\n",
      "Epoch 12: loss improved from 5.07819 to 4.94194, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 68s 28ms/step - loss: 4.9419 - accuracy: 0.1417 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 4.8124 - accuracy: 0.1516\n",
      "Epoch 13: loss improved from 4.94194 to 4.81245, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 62s 25ms/step - loss: 4.8124 - accuracy: 0.1516 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 4.6796 - accuracy: 0.1584\n",
      "Epoch 14: loss improved from 4.81245 to 4.67956, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 61s 25ms/step - loss: 4.6796 - accuracy: 0.1584 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 4.5566 - accuracy: 0.1651\n",
      "Epoch 15: loss improved from 4.67956 to 4.55657, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 62s 25ms/step - loss: 4.5566 - accuracy: 0.1651 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 4.4336 - accuracy: 0.1765\n",
      "Epoch 16: loss improved from 4.55657 to 4.43359, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 63s 25ms/step - loss: 4.4336 - accuracy: 0.1765 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 4.3164 - accuracy: 0.1847\n",
      "Epoch 17: loss improved from 4.43359 to 4.31636, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 69s 28ms/step - loss: 4.3164 - accuracy: 0.1847 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 4.1880 - accuracy: 0.1941\n",
      "Epoch 18: loss improved from 4.31636 to 4.18821, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 64s 26ms/step - loss: 4.1882 - accuracy: 0.1941 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 4.0713 - accuracy: 0.2003\n",
      "Epoch 19: loss improved from 4.18821 to 4.07129, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 64s 26ms/step - loss: 4.0713 - accuracy: 0.2003 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 3.9522 - accuracy: 0.2042\n",
      "Epoch 20: loss improved from 4.07129 to 3.95137, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 54s 22ms/step - loss: 3.9514 - accuracy: 0.2045 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 3.8225 - accuracy: 0.2187\n",
      "Epoch 21: loss improved from 3.95137 to 3.82253, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 48s 20ms/step - loss: 3.8225 - accuracy: 0.2187 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 3.6998 - accuracy: 0.2348\n",
      "Epoch 22: loss improved from 3.82253 to 3.69982, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 51s 21ms/step - loss: 3.6998 - accuracy: 0.2348 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "2468/2470 [============================>.] - ETA: 0s - loss: 3.5845 - accuracy: 0.2451\n",
      "Epoch 23: loss improved from 3.69982 to 3.58487, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 57s 23ms/step - loss: 3.5849 - accuracy: 0.2452 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 3.4716 - accuracy: 0.2582\n",
      "Epoch 24: loss improved from 3.58487 to 3.47163, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 70s 29ms/step - loss: 3.4716 - accuracy: 0.2582 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "2468/2470 [============================>.] - ETA: 0s - loss: 3.3661 - accuracy: 0.2642\n",
      "Epoch 25: loss improved from 3.47163 to 3.36486, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 48s 19ms/step - loss: 3.3649 - accuracy: 0.2645 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "2468/2470 [============================>.] - ETA: 0s - loss: 3.2590 - accuracy: 0.2833\n",
      "Epoch 26: loss improved from 3.36486 to 3.25811, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 66s 27ms/step - loss: 3.2581 - accuracy: 0.2833 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 3.1566 - accuracy: 0.2919\n",
      "Epoch 27: loss improved from 3.25811 to 3.15659, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 68s 27ms/step - loss: 3.1566 - accuracy: 0.2919 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 3.0763 - accuracy: 0.3049\n",
      "Epoch 28: loss improved from 3.15659 to 3.07632, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 65s 26ms/step - loss: 3.0763 - accuracy: 0.3048 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 2.9781 - accuracy: 0.3175\n",
      "Epoch 29: loss improved from 3.07632 to 2.97781, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 80s 32ms/step - loss: 2.9778 - accuracy: 0.3176 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 2.8825 - accuracy: 0.3372\n",
      "Epoch 30: loss improved from 2.97781 to 2.88248, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 73s 30ms/step - loss: 2.8825 - accuracy: 0.3372 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 2.7824 - accuracy: 0.3493\n",
      "Epoch 31: loss improved from 2.88248 to 2.78268, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 86s 35ms/step - loss: 2.7827 - accuracy: 0.3493 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 2.7008 - accuracy: 0.3634\n",
      "Epoch 32: loss improved from 2.78268 to 2.70080, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 88s 36ms/step - loss: 2.7008 - accuracy: 0.3634 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 2.6089 - accuracy: 0.3796\n",
      "Epoch 33: loss improved from 2.70080 to 2.60895, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 82s 33ms/step - loss: 2.6089 - accuracy: 0.3796 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 2.5341 - accuracy: 0.3932\n",
      "Epoch 34: loss improved from 2.60895 to 2.53406, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 110s 45ms/step - loss: 2.5341 - accuracy: 0.3932 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 2.4544 - accuracy: 0.4065\n",
      "Epoch 35: loss improved from 2.53406 to 2.45375, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 108s 44ms/step - loss: 2.4538 - accuracy: 0.4066 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 2.3730 - accuracy: 0.4212\n",
      "Epoch 36: loss improved from 2.45375 to 2.37362, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 101s 41ms/step - loss: 2.3736 - accuracy: 0.4211 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 2.3076 - accuracy: 0.4332\n",
      "Epoch 37: loss improved from 2.37362 to 2.30791, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 115s 47ms/step - loss: 2.3079 - accuracy: 0.4331 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 2.2266 - accuracy: 0.4500\n",
      "Epoch 38: loss improved from 2.30791 to 2.22660, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 102s 41ms/step - loss: 2.2266 - accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 2.1608 - accuracy: 0.4633\n",
      "Epoch 39: loss improved from 2.22660 to 2.16050, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 113s 46ms/step - loss: 2.1605 - accuracy: 0.4635 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 2.0827 - accuracy: 0.4756\n",
      "Epoch 40: loss improved from 2.16050 to 2.08278, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 89s 36ms/step - loss: 2.0828 - accuracy: 0.4756 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 2.0141 - accuracy: 0.4928\n",
      "Epoch 41: loss improved from 2.08278 to 2.01409, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 80s 32ms/step - loss: 2.0141 - accuracy: 0.4928 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 1.9479 - accuracy: 0.5038\n",
      "Epoch 42: loss improved from 2.01409 to 1.94792, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 79s 32ms/step - loss: 1.9479 - accuracy: 0.5038 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 1.8766 - accuracy: 0.5176\n",
      "Epoch 43: loss improved from 1.94792 to 1.87652, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 86s 35ms/step - loss: 1.8765 - accuracy: 0.5176 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 1.8121 - accuracy: 0.5313\n",
      "Epoch 44: loss improved from 1.87652 to 1.81156, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 93s 37ms/step - loss: 1.8116 - accuracy: 0.5315 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 1.7366 - accuracy: 0.5452\n",
      "Epoch 45: loss improved from 1.81156 to 1.73671, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 89s 36ms/step - loss: 1.7367 - accuracy: 0.5451 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 1.6877 - accuracy: 0.5566\n",
      "Epoch 46: loss improved from 1.73671 to 1.68781, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 136s 55ms/step - loss: 1.6878 - accuracy: 0.5565 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 1.6111 - accuracy: 0.5781\n",
      "Epoch 47: loss improved from 1.68781 to 1.61112, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 116s 47ms/step - loss: 1.6111 - accuracy: 0.5781 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 1.5691 - accuracy: 0.5842\n",
      "Epoch 48: loss improved from 1.61112 to 1.56963, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 111s 45ms/step - loss: 1.5696 - accuracy: 0.5843 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "2470/2470 [==============================] - ETA: 0s - loss: 1.5085 - accuracy: 0.5968\n",
      "Epoch 49: loss improved from 1.56963 to 1.50849, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 97s 39ms/step - loss: 1.5085 - accuracy: 0.5968 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "2469/2470 [============================>.] - ETA: 0s - loss: 1.4624 - accuracy: 0.6084\n",
      "Epoch 50: loss improved from 1.50849 to 1.46233, saving model to Film & Animationnextword1.h5\n",
      "2470/2470 [==============================] - 88s 36ms/step - loss: 1.4623 - accuracy: 0.6084 - lr: 0.0010\n",
      "On Nonprofits & Activism\n",
      "Total input sequences:  263\n",
      "Nonprofits & Activism Max sequence length is: \n",
      "16\n",
      "Epoch 1/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 5.2907 - accuracy: 0.0346\n",
      "Epoch 1: loss improved from inf to 5.29070, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 18s 35ms/step - loss: 5.2907 - accuracy: 0.0342 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 5.2248 - accuracy: 0.0380\n",
      "Epoch 2: loss improved from 5.29070 to 5.22482, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 32ms/step - loss: 5.2248 - accuracy: 0.0380 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 5.0933 - accuracy: 0.0500\n",
      "Epoch 3: loss improved from 5.22482 to 5.09629, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 5.0963 - accuracy: 0.0494 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 5.0356 - accuracy: 0.0494\n",
      "Epoch 4: loss improved from 5.09629 to 5.03559, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 5.0356 - accuracy: 0.0494 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 4.9703 - accuracy: 0.0494\n",
      "Epoch 5: loss improved from 5.03559 to 4.97027, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 4.9703 - accuracy: 0.0494 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "64/66 [============================>.] - ETA: 0s - loss: 4.9164 - accuracy: 0.0586\n",
      "Epoch 6: loss improved from 4.97027 to 4.92058, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 32ms/step - loss: 4.9206 - accuracy: 0.0570 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 4.8507 - accuracy: 0.0646\n",
      "Epoch 7: loss improved from 4.92058 to 4.85068, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 4.8507 - accuracy: 0.0646 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "64/66 [============================>.] - ETA: 0s - loss: 4.7275 - accuracy: 0.0625\n",
      "Epoch 8: loss improved from 4.85068 to 4.73022, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 24ms/step - loss: 4.7302 - accuracy: 0.0608 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 4.5542 - accuracy: 0.0418\n",
      "Epoch 9: loss improved from 4.73022 to 4.55424, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 4.5542 - accuracy: 0.0418 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 4.4248 - accuracy: 0.0684\n",
      "Epoch 10: loss improved from 4.55424 to 4.42480, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 4.4248 - accuracy: 0.0684 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 4.2917 - accuracy: 0.0654\n",
      "Epoch 11: loss improved from 4.42480 to 4.29024, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 24ms/step - loss: 4.2902 - accuracy: 0.0646 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 4.1758 - accuracy: 0.0798\n",
      "Epoch 12: loss improved from 4.29024 to 4.17582, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 27ms/step - loss: 4.1758 - accuracy: 0.0798 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.9899 - accuracy: 0.0885\n",
      "Epoch 13: loss improved from 4.17582 to 3.98268, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 3.9827 - accuracy: 0.0951 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.8135 - accuracy: 0.0951\n",
      "Epoch 14: loss improved from 3.98268 to 3.81353, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 3.8135 - accuracy: 0.0951 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.6084 - accuracy: 0.1141\n",
      "Epoch 15: loss improved from 3.81353 to 3.60837, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 3.6084 - accuracy: 0.1141 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.3339 - accuracy: 0.1559\n",
      "Epoch 16: loss improved from 3.60837 to 3.33387, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 3.3339 - accuracy: 0.1559 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.0741 - accuracy: 0.1577\n",
      "Epoch 17: loss improved from 3.33387 to 3.07635, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 3.0763 - accuracy: 0.1559 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.7632 - accuracy: 0.2129\n",
      "Epoch 18: loss improved from 3.07635 to 2.76324, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 2.7632 - accuracy: 0.2129 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.4834 - accuracy: 0.2538\n",
      "Epoch 19: loss improved from 2.76324 to 2.47847, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 2.4785 - accuracy: 0.2510 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.1895 - accuracy: 0.3423\n",
      "Epoch 20: loss improved from 2.47847 to 2.19181, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 2.1918 - accuracy: 0.3384 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.9964 - accuracy: 0.4000\n",
      "Epoch 21: loss improved from 2.19181 to 1.99373, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 1.9937 - accuracy: 0.3992 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.7414 - accuracy: 0.4846\n",
      "Epoch 22: loss improved from 1.99373 to 1.74324, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 1.7432 - accuracy: 0.4829 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.5710 - accuracy: 0.5000\n",
      "Epoch 23: loss improved from 1.74324 to 1.56914, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 1.5691 - accuracy: 0.4981 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2644 - accuracy: 0.6077\n",
      "Epoch 24: loss improved from 1.56914 to 1.26201, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 1.2620 - accuracy: 0.6046 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1457 - accuracy: 0.6615\n",
      "Epoch 25: loss improved from 1.26201 to 1.15523, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 1.1552 - accuracy: 0.6616 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9684 - accuracy: 0.7186\n",
      "Epoch 26: loss improved from 1.15523 to 0.96840, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.9684 - accuracy: 0.7186 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8724 - accuracy: 0.7308\n",
      "Epoch 27: loss improved from 0.96840 to 0.86509, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8651 - accuracy: 0.7338 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7702 - accuracy: 0.7871\n",
      "Epoch 28: loss improved from 0.86509 to 0.77017, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.7702 - accuracy: 0.7871 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.8327\n",
      "Epoch 29: loss improved from 0.77017 to 0.62810, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6281 - accuracy: 0.8327 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5822 - accuracy: 0.8538\n",
      "Epoch 30: loss improved from 0.62810 to 0.57641, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.5764 - accuracy: 0.8555 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5579 - accuracy: 0.8403\n",
      "Epoch 31: loss improved from 0.57641 to 0.55790, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.5579 - accuracy: 0.8403 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4623 - accuracy: 0.8669\n",
      "Epoch 32: loss improved from 0.55790 to 0.46229, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.4623 - accuracy: 0.8669 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4134 - accuracy: 0.8808\n",
      "Epoch 33: loss improved from 0.46229 to 0.41008, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.4101 - accuracy: 0.8821 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3855 - accuracy: 0.8923\n",
      "Epoch 34: loss improved from 0.41008 to 0.38210, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.3821 - accuracy: 0.8935 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3438 - accuracy: 0.9011\n",
      "Epoch 35: loss improved from 0.38210 to 0.34380, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.3438 - accuracy: 0.9011 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3173 - accuracy: 0.9087\n",
      "Epoch 36: loss improved from 0.34380 to 0.31730, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3173 - accuracy: 0.9087 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.9278\n",
      "Epoch 37: loss improved from 0.31730 to 0.28616, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 32ms/step - loss: 0.2862 - accuracy: 0.9278 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "64/66 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.9219\n",
      "Epoch 38: loss improved from 0.28616 to 0.26078, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 33ms/step - loss: 0.2608 - accuracy: 0.9240 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.9202\n",
      "Epoch 39: loss did not improve from 0.26078\n",
      "66/66 [==============================] - 2s 28ms/step - loss: 0.2898 - accuracy: 0.9202 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5740 - accuracy: 0.8251\n",
      "Epoch 40: loss did not improve from 0.26078\n",
      "66/66 [==============================] - 2s 28ms/step - loss: 0.5740 - accuracy: 0.8251 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.7654\n",
      "Epoch 41: loss did not improve from 0.26078\n",
      "66/66 [==============================] - 2s 31ms/step - loss: 0.7000 - accuracy: 0.7605 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.8327\n",
      "Epoch 42: loss did not improve from 0.26078\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.4206 - accuracy: 0.8327 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.9240\n",
      "Epoch 43: loss did not improve from 0.26078\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.2791 - accuracy: 0.9240 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2388 - accuracy: 0.9192\n",
      "Epoch 44: loss improved from 0.26078 to 0.23671, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.2367 - accuracy: 0.9202 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.1957 - accuracy: 0.9308\n",
      "Epoch 45: loss improved from 0.23671 to 0.19413, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.1941 - accuracy: 0.9316 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1824 - accuracy: 0.9354\n",
      "Epoch 46: loss improved from 0.19413 to 0.18243, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 27ms/step - loss: 0.1824 - accuracy: 0.9354 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9506\n",
      "Epoch 47: loss improved from 0.18243 to 0.16765, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 32ms/step - loss: 0.1677 - accuracy: 0.9506 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9385\n",
      "Epoch 48: loss did not improve from 0.16765\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.1703 - accuracy: 0.9392 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.1549 - accuracy: 0.9500\n",
      "Epoch 49: loss improved from 0.16765 to 0.16172, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 33ms/step - loss: 0.1617 - accuracy: 0.9468 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1607 - accuracy: 0.9392\n",
      "Epoch 50: loss improved from 0.16172 to 0.16074, saving model to Nonprofits & Activismnextword1.h5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.1607 - accuracy: 0.9392 - lr: 0.0010\n",
      "On People & Blogs\n",
      "Total input sequences:  34687\n",
      "People & Blogs Max sequence length is: \n",
      "20\n",
      "Epoch 1/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 7.7666 - accuracy: 0.0286\n",
      "Epoch 1: loss improved from inf to 7.76658, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 510s 58ms/step - loss: 7.7666 - accuracy: 0.0286 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 7.4666 - accuracy: 0.0315\n",
      "Epoch 2: loss improved from 7.76658 to 7.46664, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 676s 78ms/step - loss: 7.4666 - accuracy: 0.0315 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 7.2805 - accuracy: 0.0325\n",
      "Epoch 3: loss improved from 7.46664 to 7.28050, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 502s 58ms/step - loss: 7.2805 - accuracy: 0.0325 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 7.1508 - accuracy: 0.0349\n",
      "Epoch 4: loss improved from 7.28050 to 7.15083, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 491s 57ms/step - loss: 7.1508 - accuracy: 0.0349 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 7.0614 - accuracy: 0.0378\n",
      "Epoch 5: loss improved from 7.15083 to 7.06136, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 528s 61ms/step - loss: 7.0614 - accuracy: 0.0378 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 6.9899 - accuracy: 0.0396\n",
      "Epoch 6: loss improved from 7.06136 to 6.99002, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 401s 46ms/step - loss: 6.9900 - accuracy: 0.0396 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 6.9190 - accuracy: 0.0417\n",
      "Epoch 7: loss improved from 6.99002 to 6.91904, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 2217s 256ms/step - loss: 6.9190 - accuracy: 0.0417 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 6.8429 - accuracy: 0.0446\n",
      "Epoch 8: loss improved from 6.91904 to 6.84288, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 361s 42ms/step - loss: 6.8429 - accuracy: 0.0446 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 6.7653 - accuracy: 0.0479\n",
      "Epoch 9: loss improved from 6.84288 to 6.76539, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 379s 44ms/step - loss: 6.7654 - accuracy: 0.0479 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 6.6991 - accuracy: 0.0512\n",
      "Epoch 10: loss improved from 6.76539 to 6.69900, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 358s 41ms/step - loss: 6.6990 - accuracy: 0.0513 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 6.6254 - accuracy: 0.0566\n",
      "Epoch 11: loss improved from 6.69900 to 6.62504, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 355s 41ms/step - loss: 6.6250 - accuracy: 0.0566 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 6.5354 - accuracy: 0.0587\n",
      "Epoch 12: loss improved from 6.62504 to 6.53565, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 357s 41ms/step - loss: 6.5357 - accuracy: 0.0587 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 6.4530 - accuracy: 0.0609\n",
      "Epoch 13: loss improved from 6.53565 to 6.45301, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 360s 42ms/step - loss: 6.4530 - accuracy: 0.0609 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 6.3696 - accuracy: 0.0635\n",
      "Epoch 14: loss improved from 6.45301 to 6.36957, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 356s 41ms/step - loss: 6.3696 - accuracy: 0.0635 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 6.2837 - accuracy: 0.0648\n",
      "Epoch 15: loss improved from 6.36957 to 6.28366, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 358s 41ms/step - loss: 6.2837 - accuracy: 0.0648 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 6.1911 - accuracy: 0.0684\n",
      "Epoch 16: loss improved from 6.28366 to 6.19108, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 357s 41ms/step - loss: 6.1911 - accuracy: 0.0684 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 6.0964 - accuracy: 0.0711\n",
      "Epoch 17: loss improved from 6.19108 to 6.09637, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 356s 41ms/step - loss: 6.0964 - accuracy: 0.0711 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 6.0133 - accuracy: 0.0732\n",
      "Epoch 18: loss improved from 6.09637 to 6.01334, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 357s 41ms/step - loss: 6.0133 - accuracy: 0.0732 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 5.9332 - accuracy: 0.0756\n",
      "Epoch 19: loss improved from 6.01334 to 5.93316, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 355s 41ms/step - loss: 5.9332 - accuracy: 0.0756 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 5.8569 - accuracy: 0.0799\n",
      "Epoch 20: loss improved from 5.93316 to 5.85691, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 357s 41ms/step - loss: 5.8569 - accuracy: 0.0799 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 5.7882 - accuracy: 0.0825\n",
      "Epoch 21: loss improved from 5.85691 to 5.78820, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 358s 41ms/step - loss: 5.7882 - accuracy: 0.0825 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 5.7146 - accuracy: 0.0849\n",
      "Epoch 22: loss improved from 5.78820 to 5.71454, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 357s 41ms/step - loss: 5.7145 - accuracy: 0.0848 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 5.6485 - accuracy: 0.0887\n",
      "Epoch 23: loss improved from 5.71454 to 5.64856, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 358s 41ms/step - loss: 5.6486 - accuracy: 0.0887 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 5.5902 - accuracy: 0.0906\n",
      "Epoch 24: loss improved from 5.64856 to 5.59017, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 359s 41ms/step - loss: 5.5902 - accuracy: 0.0906 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 5.5306 - accuracy: 0.0944\n",
      "Epoch 25: loss improved from 5.59017 to 5.53064, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 358s 41ms/step - loss: 5.5306 - accuracy: 0.0944 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 5.4666 - accuracy: 0.0966\n",
      "Epoch 26: loss improved from 5.53064 to 5.46649, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 359s 41ms/step - loss: 5.4665 - accuracy: 0.0966 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 5.4029 - accuracy: 0.0997\n",
      "Epoch 27: loss improved from 5.46649 to 5.40286, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 361s 42ms/step - loss: 5.4029 - accuracy: 0.0997 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 5.3681 - accuracy: 0.1029\n",
      "Epoch 28: loss improved from 5.40286 to 5.36821, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 360s 41ms/step - loss: 5.3682 - accuracy: 0.1029 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 5.2683 - accuracy: 0.1067\n",
      "Epoch 29: loss improved from 5.36821 to 5.26832, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 355s 41ms/step - loss: 5.2683 - accuracy: 0.1067 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 5.2074 - accuracy: 0.1100\n",
      "Epoch 30: loss improved from 5.26832 to 5.20739, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 357s 41ms/step - loss: 5.2074 - accuracy: 0.1100 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 5.1433 - accuracy: 0.1144\n",
      "Epoch 31: loss improved from 5.20739 to 5.14331, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 359s 41ms/step - loss: 5.1433 - accuracy: 0.1144 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 5.0845 - accuracy: 0.1167\n",
      "Epoch 32: loss improved from 5.14331 to 5.08446, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 358s 41ms/step - loss: 5.0845 - accuracy: 0.1167 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 5.0229 - accuracy: 0.1221\n",
      "Epoch 33: loss improved from 5.08446 to 5.02286, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 359s 41ms/step - loss: 5.0229 - accuracy: 0.1221 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 4.9643 - accuracy: 0.1262\n",
      "Epoch 34: loss improved from 5.02286 to 4.96435, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 363s 42ms/step - loss: 4.9644 - accuracy: 0.1262 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 4.9038 - accuracy: 0.1296\n",
      "Epoch 35: loss improved from 4.96435 to 4.90383, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 360s 41ms/step - loss: 4.9038 - accuracy: 0.1296 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 4.8440 - accuracy: 0.1328\n",
      "Epoch 36: loss improved from 4.90383 to 4.84400, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 360s 42ms/step - loss: 4.8440 - accuracy: 0.1328 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 4.7869 - accuracy: 0.1348\n",
      "Epoch 37: loss improved from 4.84400 to 4.78689, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 359s 41ms/step - loss: 4.7869 - accuracy: 0.1348 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 4.7360 - accuracy: 0.1390\n",
      "Epoch 38: loss improved from 4.78689 to 4.73618, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 357s 41ms/step - loss: 4.7362 - accuracy: 0.1390 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 4.6794 - accuracy: 0.1450\n",
      "Epoch 39: loss improved from 4.73618 to 4.67951, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 360s 41ms/step - loss: 4.6795 - accuracy: 0.1450 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 4.6304 - accuracy: 0.1477\n",
      "Epoch 40: loss improved from 4.67951 to 4.63042, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 360s 42ms/step - loss: 4.6304 - accuracy: 0.1477 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 4.5792 - accuracy: 0.1508\n",
      "Epoch 41: loss improved from 4.63042 to 4.57920, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 355s 41ms/step - loss: 4.5792 - accuracy: 0.1508 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 4.5180 - accuracy: 0.1556\n",
      "Epoch 42: loss improved from 4.57920 to 4.51801, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 357s 41ms/step - loss: 4.5180 - accuracy: 0.1556 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 4.4686 - accuracy: 0.1588\n",
      "Epoch 43: loss improved from 4.51801 to 4.46863, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 362s 42ms/step - loss: 4.4686 - accuracy: 0.1588 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 4.4235 - accuracy: 0.1644\n",
      "Epoch 44: loss improved from 4.46863 to 4.42351, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 359s 41ms/step - loss: 4.4235 - accuracy: 0.1644 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 4.3676 - accuracy: 0.1670\n",
      "Epoch 45: loss improved from 4.42351 to 4.36758, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 361s 42ms/step - loss: 4.3676 - accuracy: 0.1670 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 4.3274 - accuracy: 0.1714\n",
      "Epoch 46: loss improved from 4.36758 to 4.32736, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 367s 42ms/step - loss: 4.3274 - accuracy: 0.1714 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 4.2790 - accuracy: 0.1752\n",
      "Epoch 47: loss improved from 4.32736 to 4.27915, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 360s 41ms/step - loss: 4.2792 - accuracy: 0.1751 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 4.2376 - accuracy: 0.1795\n",
      "Epoch 48: loss improved from 4.27915 to 4.23747, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 359s 41ms/step - loss: 4.2375 - accuracy: 0.1795 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "8671/8672 [============================>.] - ETA: 0s - loss: 4.1907 - accuracy: 0.1842\n",
      "Epoch 49: loss improved from 4.23747 to 4.19072, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 374s 43ms/step - loss: 4.1907 - accuracy: 0.1842 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "8672/8672 [==============================] - ETA: 0s - loss: 4.1498 - accuracy: 0.1879\n",
      "Epoch 50: loss improved from 4.19072 to 4.14980, saving model to People & Blogsnextword1.h5\n",
      "8672/8672 [==============================] - 362s 42ms/step - loss: 4.1498 - accuracy: 0.1879 - lr: 0.0010\n",
      "On Howto & Style\n",
      "Total input sequences:  11903\n",
      "Howto & Style Max sequence length is: \n",
      "22\n",
      "Epoch 1/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 7.4297 - accuracy: 0.0187\n",
      "Epoch 1: loss improved from inf to 7.42941, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 116s 37ms/step - loss: 7.4294 - accuracy: 0.0187 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 7.0175 - accuracy: 0.0226\n",
      "Epoch 2: loss improved from 7.42941 to 7.01736, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 113s 38ms/step - loss: 7.0174 - accuracy: 0.0226 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 6.8094 - accuracy: 0.0271\n",
      "Epoch 3: loss improved from 7.01736 to 6.80938, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 111s 37ms/step - loss: 6.8094 - accuracy: 0.0271 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 6.6572 - accuracy: 0.0370\n",
      "Epoch 4: loss improved from 6.80938 to 6.65721, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 110s 37ms/step - loss: 6.6572 - accuracy: 0.0370 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 6.5125 - accuracy: 0.0428\n",
      "Epoch 5: loss improved from 6.65721 to 6.51255, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 107s 36ms/step - loss: 6.5125 - accuracy: 0.0428 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 6.3736 - accuracy: 0.0473\n",
      "Epoch 6: loss improved from 6.51255 to 6.37374, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 108s 36ms/step - loss: 6.3737 - accuracy: 0.0473 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 6.2543 - accuracy: 0.0516\n",
      "Epoch 7: loss improved from 6.37374 to 6.25429, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 125s 42ms/step - loss: 6.2543 - accuracy: 0.0516 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 6.1451 - accuracy: 0.0576\n",
      "Epoch 8: loss improved from 6.25429 to 6.14509, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 112s 38ms/step - loss: 6.1451 - accuracy: 0.0576 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 6.0404 - accuracy: 0.0614\n",
      "Epoch 9: loss improved from 6.14509 to 6.04038, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 108s 36ms/step - loss: 6.0404 - accuracy: 0.0614 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 5.9273 - accuracy: 0.0642\n",
      "Epoch 10: loss improved from 6.04038 to 5.92734, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 112s 38ms/step - loss: 5.9273 - accuracy: 0.0642 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 5.8229 - accuracy: 0.0666\n",
      "Epoch 11: loss improved from 5.92734 to 5.82294, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 111s 37ms/step - loss: 5.8229 - accuracy: 0.0666 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 5.7208 - accuracy: 0.0700\n",
      "Epoch 12: loss improved from 5.82294 to 5.72082, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 108s 36ms/step - loss: 5.7208 - accuracy: 0.0700 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 5.6108 - accuracy: 0.0738\n",
      "Epoch 13: loss improved from 5.72082 to 5.61085, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 105s 35ms/step - loss: 5.6108 - accuracy: 0.0738 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 5.5069 - accuracy: 0.0770\n",
      "Epoch 14: loss improved from 5.61085 to 5.50689, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 108s 36ms/step - loss: 5.5069 - accuracy: 0.0770 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 5.4007 - accuracy: 0.0807\n",
      "Epoch 15: loss improved from 5.50689 to 5.40065, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 107s 36ms/step - loss: 5.4007 - accuracy: 0.0807 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 5.2938 - accuracy: 0.0866\n",
      "Epoch 16: loss improved from 5.40065 to 5.29364, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 107s 36ms/step - loss: 5.2936 - accuracy: 0.0865 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 5.1917 - accuracy: 0.0877\n",
      "Epoch 17: loss improved from 5.29364 to 5.19169, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 108s 36ms/step - loss: 5.1917 - accuracy: 0.0877 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 5.1455 - accuracy: 0.0935\n",
      "Epoch 18: loss improved from 5.19169 to 5.14549, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 105s 35ms/step - loss: 5.1455 - accuracy: 0.0935 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 5.0141 - accuracy: 0.0978\n",
      "Epoch 19: loss improved from 5.14549 to 5.01473, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 109s 37ms/step - loss: 5.0147 - accuracy: 0.0978 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 4.8909 - accuracy: 0.1054\n",
      "Epoch 20: loss improved from 5.01473 to 4.89088, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 104s 35ms/step - loss: 4.8909 - accuracy: 0.1054 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 4.7896 - accuracy: 0.1074\n",
      "Epoch 21: loss improved from 4.89088 to 4.78955, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 109s 37ms/step - loss: 4.7896 - accuracy: 0.1074 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 4.6859 - accuracy: 0.1138\n",
      "Epoch 22: loss improved from 4.78955 to 4.68632, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 109s 37ms/step - loss: 4.6863 - accuracy: 0.1138 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 4.5971 - accuracy: 0.1178\n",
      "Epoch 23: loss improved from 4.68632 to 4.59673, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 107s 36ms/step - loss: 4.5967 - accuracy: 0.1179 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 4.4967 - accuracy: 0.1245\n",
      "Epoch 24: loss improved from 4.59673 to 4.49671, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 104s 35ms/step - loss: 4.4967 - accuracy: 0.1245 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 4.3823 - accuracy: 0.1322\n",
      "Epoch 25: loss improved from 4.49671 to 4.38263, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 104s 35ms/step - loss: 4.3826 - accuracy: 0.1322 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 4.3005 - accuracy: 0.1397\n",
      "Epoch 26: loss improved from 4.38263 to 4.30107, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 107s 36ms/step - loss: 4.3011 - accuracy: 0.1397 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 4.1857 - accuracy: 0.1479\n",
      "Epoch 27: loss improved from 4.30107 to 4.18609, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 104s 35ms/step - loss: 4.1861 - accuracy: 0.1479 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 4.0750 - accuracy: 0.1580\n",
      "Epoch 28: loss improved from 4.18609 to 4.07520, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 108s 36ms/step - loss: 4.0752 - accuracy: 0.1579 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 3.9671 - accuracy: 0.1659\n",
      "Epoch 29: loss improved from 4.07520 to 3.96712, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 103s 35ms/step - loss: 3.9671 - accuracy: 0.1659 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 3.8616 - accuracy: 0.1779\n",
      "Epoch 30: loss improved from 3.96712 to 3.86165, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 105s 35ms/step - loss: 3.8616 - accuracy: 0.1779 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 3.7557 - accuracy: 0.1905\n",
      "Epoch 31: loss improved from 3.86165 to 3.75566, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 107s 36ms/step - loss: 3.7557 - accuracy: 0.1905 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 3.6587 - accuracy: 0.2041\n",
      "Epoch 32: loss improved from 3.75566 to 3.65905, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 104s 35ms/step - loss: 3.6591 - accuracy: 0.2041 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 3.5484 - accuracy: 0.2182\n",
      "Epoch 33: loss improved from 3.65905 to 3.54842, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 105s 35ms/step - loss: 3.5484 - accuracy: 0.2183 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 3.4465 - accuracy: 0.2302\n",
      "Epoch 34: loss improved from 3.54842 to 3.44651, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 104s 35ms/step - loss: 3.4465 - accuracy: 0.2302 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 3.3545 - accuracy: 0.2474\n",
      "Epoch 35: loss improved from 3.44651 to 3.35452, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 103s 35ms/step - loss: 3.3545 - accuracy: 0.2474 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 3.2567 - accuracy: 0.2651\n",
      "Epoch 36: loss improved from 3.35452 to 3.25666, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 107s 36ms/step - loss: 3.2567 - accuracy: 0.2651 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 3.1473 - accuracy: 0.2777\n",
      "Epoch 37: loss improved from 3.25666 to 3.14727, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 103s 35ms/step - loss: 3.1473 - accuracy: 0.2777 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 3.0543 - accuracy: 0.2914\n",
      "Epoch 38: loss improved from 3.14727 to 3.05435, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 105s 35ms/step - loss: 3.0543 - accuracy: 0.2914 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 2.9792 - accuracy: 0.3061\n",
      "Epoch 39: loss improved from 3.05435 to 2.97923, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 105s 35ms/step - loss: 2.9792 - accuracy: 0.3061 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 2.8810 - accuracy: 0.3276\n",
      "Epoch 40: loss improved from 2.97923 to 2.88048, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 103s 35ms/step - loss: 2.8805 - accuracy: 0.3278 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 2.7926 - accuracy: 0.3403\n",
      "Epoch 41: loss improved from 2.88048 to 2.79291, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 103s 35ms/step - loss: 2.7929 - accuracy: 0.3403 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 2.7184 - accuracy: 0.3551\n",
      "Epoch 42: loss improved from 2.79291 to 2.71910, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 106s 36ms/step - loss: 2.7191 - accuracy: 0.3550 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 2.6378 - accuracy: 0.3682\n",
      "Epoch 43: loss improved from 2.71910 to 2.63813, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 103s 34ms/step - loss: 2.6381 - accuracy: 0.3681 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 2.5529 - accuracy: 0.3839\n",
      "Epoch 44: loss improved from 2.63813 to 2.55285, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 103s 35ms/step - loss: 2.5529 - accuracy: 0.3839 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 2.4914 - accuracy: 0.3893\n",
      "Epoch 45: loss improved from 2.55285 to 2.49136, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 105s 35ms/step - loss: 2.4914 - accuracy: 0.3893 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 2.4083 - accuracy: 0.4123\n",
      "Epoch 46: loss improved from 2.49136 to 2.40827, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 102s 34ms/step - loss: 2.4083 - accuracy: 0.4123 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 2.3422 - accuracy: 0.4219\n",
      "Epoch 47: loss improved from 2.40827 to 2.34272, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 103s 35ms/step - loss: 2.3427 - accuracy: 0.4218 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 2.2714 - accuracy: 0.4395\n",
      "Epoch 48: loss improved from 2.34272 to 2.27142, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 105s 35ms/step - loss: 2.2714 - accuracy: 0.4395 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "2975/2976 [============================>.] - ETA: 0s - loss: 2.1999 - accuracy: 0.4546\n",
      "Epoch 49: loss improved from 2.27142 to 2.20005, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 103s 35ms/step - loss: 2.2001 - accuracy: 0.4547 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "2976/2976 [==============================] - ETA: 0s - loss: 2.1362 - accuracy: 0.4646\n",
      "Epoch 50: loss improved from 2.20005 to 2.13620, saving model to Howto & Stylenextword1.h5\n",
      "2976/2976 [==============================] - 103s 35ms/step - loss: 2.1362 - accuracy: 0.4646 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "for category in set(main_data['category_name'].to_list()):\n",
    "    print('On '+category)\n",
    "    catdf = main_data.loc[main_data['category_name'] == category]\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(catdf['title'])\n",
    "    \n",
    "    # saving the tokenizer for predict function.\n",
    "    pickle.dump(tokenizer, open(category+'tokenizer.pkl', 'wb'))\n",
    "    \n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    sequence_data = tokenizer.texts_to_sequences(catdf['title'])\n",
    "    \n",
    "    sequences = []\n",
    "    for line in catdf['title']:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        #print(token_list)\n",
    "        \n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            sequences.append(n_gram_sequence)\n",
    "    \n",
    "    print(\"Total input sequences: \", len(sequences))\n",
    "    \n",
    "    # pad sequences\n",
    "    max_sequence_len = max([len(x) for x in sequences])\n",
    "    sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    print(category + ' Max sequence length is: ')\n",
    "    print(max_sequence_len-1)\n",
    "    \n",
    "    # create features and label\n",
    "    xs, labels = sequences[:,:-1],sequences[:,-1]\n",
    "    ys = to_categorical(labels, num_classes=total_words)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, max_sequence_len+4, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dense(total_words, activation=\"softmax\"))\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(category+\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "        save_best_only=True, mode='auto')\n",
    "    \n",
    "    reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.1, verbose = 1)\n",
    "    \n",
    "    logdir=category+'logsnextword1'\n",
    "    tensorboard_Visualization = TensorBoard(log_dir=logdir)\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(xs, ys, batch_size=4, epochs=50,verbose=1,validation_batch_size=.30,shuffle=True,callbacks=[checkpoint, reduce, tensorboard_Visualization]).history\n",
    "    \n",
    "    model.save(category+'keras_next_word_model.h5')\n",
    "    pickle.dump(history, open(category+\"history.p\", \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
